\documentclass[final]{elsarticle}
\setlength{\marginparwidth}{0.5in}
\usepackage{amsmath,amssymb,amsthm,natbib,mathtools,graphicx}
\input FJHDef.tex

\newcommand{\chin}{\ch_{\text{\rm{in}}}}
\newcommand{\chout}{\ch_{\text{\rm{out}}}}
\newcommand{\pin}{p_{\text{\rm{in}}}}
\newcommand{\pout}{p_{\text{\rm{out}}}}
%\newcommand{\cc}{\mathcal{C}}
\newcommand{\cq}{\mathcal{Q}}
\newcommand{\co}{\mathcal{O}}
\newcommand{\bbW}{\mathbb{W}}
%\newcommand{\tP}{\widetilde{P}}
\newcommand{\bg}{{\bf g}}
\newcommand{\bu}{{\bf u}}
\newcommand{\bbu}{\bar{\bf u}}
\newcommand{\bv}{{\bf v}}
\newcommand{\bbv}{\bar{\bf v}}
\newcommand{\bw}{{\bf w}}
\newcommand{\bbw}{\bar{\bf w}}
%\newcommand{\hv}{\hat{v}}
\newcommand{\bgamma}{\underline{\boldsymbol\gamma}}
\DeclareMathOperator{\MSE}{MSE}
\DeclareMathOperator{\RMSE}{RMSE}
\DeclareMathOperator{\rnd}{rnd}
\DeclareMathOperator{\abso}{abs}
\DeclareMathOperator{\rel}{rel}
\DeclareMathOperator{\nor}{nor}
\DeclareMathOperator{\err}{err}
\DeclareMathOperator{\lin}{lin}
%\DeclareMathOperator{\std}{std}
\DeclareMathOperator{\prob}{prob}
\DeclareMathOperator{\trunc}{trc}
\DeclareMathOperator{\third}{third}
\DeclareMathOperator{\non}{non}
%\DeclareMathOperator{\fourth}{fourth}
\newtheorem{theorem}{Theorem}
\newtheorem{prop}[theorem]{Proposition}
\newtheorem{lem}[theorem]{Lemma}
\theoremstyle{definition}
\newtheorem{algo}{Algorithm}
\newtheorem{condit}{Condition}
%\newtheorem{assump}{Assumption}
\theoremstyle{remark}
\newtheorem{rem}{Remark}
\DeclareMathOperator{\fix}{fix}
\DeclareMathOperator{\up}{up}
\DeclareMathOperator{\lo}{lo}

\journal{Journal of Complexity}

\begin{document}

\begin{frontmatter}

\title{The Complexity of Automatic Algorithms Employing Continuous Linear Functionals}
\author{Fred J. Hickernell}
\address{Room E1-208, Department of Applied Mathematics, Illinois Institute of Technology,\\ 10 W.\ 32$^{\text{nd}}$ St., Chicago, IL 60616}
\author{Llu\'{\i}s Antoni Jim\'{e}nez Rugama}
\address{Room E1-120, Department of Applied Mathematics, Illinois Institute of Technology,\\ 10 W.\ 32$^{\text{nd}}$ St., Chicago, IL 60616}
\begin{abstract}
\end{abstract}

\begin{keyword}
%% keywords here, in the form: keyword \sep keyword

%% MSC codes here, in the form: \MSC code \sep code
%% or \MSC[2008] code \sep code (2000 is the default)

\end{keyword}
\end{frontmatter}


\section{The Basic Problem}
Let $\chin$ be a separable Banach space of input functions with basis $\{u_i\}_{i\in\ci}$, where $\ci$ is a countable index set, and let the norm for this Banach space be defined as the $\ell_{\pin}$-norm terms of the Fourier coefficients as follows:
\begin{equation*}
f = \sum_{i \in \ci} \hf_i u_i \in \chin, \qquad \norm[\chin]{f}=\big\lVert\big(\hf_i\big)_{i\in \ci}\big\rVert_{\pin}, \qquad 1 \le \pin \le \infty.
\end{equation*}
Here the $\hf_i$ denotes the $i^{\text{th}}$ Fourier coefficient of a function $f\in \chin$.  If $\pin=2$, then $\chin$ is a Hilbert space.  Similarly, let $\chout$ be a separable Banach space of outputs  with basis $\{v_i\}_{i \in \ci}$, whose norm may be defined as follows:
\begin{equation*}
f = \sum_{i \in \ci} \tf_i v_i \in \chout, \qquad \norm[\chout]{f}=\big\lVert\big(\tf_i\big)_{i\in \ci}\big\rVert_{\pout}, \qquad 1 \le \pout \le \infty.
\end{equation*}
In this case $\tf_i$ denotes the $i^{\text{th}}$ Fourier coefficient of a function $f\in \chout$.  Define the solution operator $S$ by $S(u_i) = \lambda_i v_i$, and so
\begin{equation} \label{HilbertS}
S(f) = \sum_{i \in \ci}^\infty \lambda_i \hf_i v_i, \qquad \forall f \in \chin.
\end{equation}
Here the $\lambda_i$ are the bounded singular values of the operator $S$.  A generalization of H\"older's inequality is
\begin{multline} \label{HoldIneq}
\norm[r]{(a_ib_i)_{i \in \ci}} \le \norm[p]{(a_i)_{i \in \ci}} \norm[q]{(b_i)_{i \in \ci}}, \\
1 \le r, p \le \infty, \quad  q= \frac{pr}{\max(p-r,0)}.
\end{multline}
Thus, to ensure that the solution operator is bounded, it is assumed that
\begin{equation} \label{lamcond}
\norm[q]{(\lambda_i)_{i \in \ci}}<\infty, \quad  q= \frac{\pin\pout}{\max(\pin-\pout,0)}.
\end{equation}

This problem definition is rather general in some ways, e.g., allowing the inputs and outputs to lie in Banach spaces, rather than Hilbert spaces.  However, a key requirement is that $S(u_i)=\lambda_i v_i$ for the solution operator of interest, which is a serious restriction on the choice of bases.  Moreover, the norms of the Banach spaces cannot be arbitrarily defined, but must be defined in terms of the coefficients of the series expansions of the inputs and outputs.

For example, if $\chin=\chout=\cl_2[0,1]$, and $S: f \mapsto f$ is the embedding operator, then one may choose a trigonometric polynomial basis:
\begin{gather*}
\norm[\chin]{f} = \norm[\chout]{f} = \left[\int_0^1 \abs{f(x)}^2 \, \dif x \right]^{1/2}, \\
u_{i}(x) = v_i(x) = \me^{2 \pi \sqrt{-1} i x}, \quad \lambda_i=1, \qquad i \in \integers,\\
\pin=\pout=2,\quad q=\infty.
\end{gather*}
If $S$ computes the derivative of a function, $S: f \mapsto f'$, then the space $\chin$ must have a stronger inner product to ensure that the $\lambda_i$ are bounded, e.g.,
\begin{gather*}
\norm[\chin]{f} = \left[\int_0^1 \left\{\abs{f(x)}^2 + \abs{f'(x)}^2\right\}\, \dif x\right]^{1/2}, \quad \norm[\chout]{f} = \left[\int_0^1 \abs{f(x)}^2 \, \dif x \right]^{1/2}, \\
u_{i}(x) = \frac{\me^{2 \pi \sqrt{-1} i x}}{\sqrt{1+4 \pi^2 i^2}}, \quad v_i(x) = \me^{2 \pi \sqrt{-1} i x}, \quad \lambda_i=\frac{2 \pi \sqrt{-1} i}{\sqrt{1+4 \pi^2 i^2}}, \qquad i \in \integers,\\
\pin=\pout=2,\quad q=\infty.
\end{gather*}

Suppose that one may choose arbitrary linear functionals to obtain data, and let $i_1, i_2, \ldots$ be an ordering of the elements in $\ci$. Then one may approximate $S(f)$ by the first $n$ terms of its infinite series representation:
\begin{subequations}
\begin{gather} \label{Andef}
A_n(f) = \sum_{j=1}^n \lambda_{i_j} \hf_{i_j} v_{i_j} \qquad \forall f \in \chin, \ n \in \naturals, \\
\label{errsum}
\norm[\chout]{S(f)-A_n(f)} = \norm[\pout]{\left(\lambda_{i_j} \hf_{i_j} \right)_{j\ge n+1}}
\end{gather}
Again by the generalization of H\"older's inequality, \eqref{HoldIneq}, it follows that
\begin{equation}
\label{erreig}
\sup_{0 \ne f \in \chin}\frac{\norm[\chout]{S(f)-A_n(f)}}{\norm[\chin]{f}} = \norm[q]{\left(\lambda_{i_j} \right)_{j\ge n+1}},
\end{equation}
\end{subequations}
where $q$ was defined in \eqref{lamcond}. For the example above, a natural ordering would be $i_j=(-1)^{j-1}\left \lceil (j-1)/2 \right \rceil$.
An automatic algorithm for approximating $S(f)$ must have a way of reliably bounding
\[
\norm[\pout]{\left(\lambda_{i_j} \hf_{i_j} \right)_{j\ge n+1}} \quad \text{or} \quad \norm[\infty]{\left(\lambda_{i_j} \right)_{j\ge n+1}}
\]
in terms of function data.  The next two sections describe two possible ways to do this.

\section{Approximating a Weaker Norm as a Surrogate for a Stronger Norm}

The first method for constructing an automatic algorithm assumes two Banach subspaces of input functions, $\cf \subseteq \cg \subseteq \chin$, whose norms are defined as weighted $\ell_{\pin}$-norms of the series coefficients.  Specifically, let $(\nu_i)_{i \in \ci}$ and $(\omega_i)_{i \in \ci}$ be positive sequences of weights, and
\begin{equation*}
f = \sum_{i \in \ci} \hf_i u_i \in \chin, \qquad \norm[\cg]{f}=\bigg\lVert\bigg(\frac{\hf_i}{\omega_i}\bigg)_{i\in \ci}\bigg\rVert_{\pin}, \qquad \norm[\cf]{f}=\bigg\lVert\bigg(\frac{\hf_i}{\nu_i\omega_i}\bigg)_{i\in \ci}\bigg\rVert_{\pin}.
\end{equation*}
If $\inf_{i \in \ci} \omega_i = 0$, then the $\cg$-norm is stronger than the $\chin$-norm.  If $\inf_{i \in \ci} \nu_i = 0$, then the $\cf$-norm is stronger than the $\cg$-norm.
For these two new norms the worst-case error can be bounded tightly as in \eqref{erreig} as follows:
\begin{gather}
\nonumber \label{erreigweakG}
\tilde{h}(n)=\sup_{0 \ne f \in \cg}\frac{\norm[\chout]{S(f)-A_n(f)}}{\norm[\cg]{f}} = \norm[q]{\left(\omega_{i_j} \lambda_{i_j} \right)_{j\ge n+1}}, \\
\label{erreigweakF}
h(n)=\sup_{0 \ne f \in \cf}\frac{\norm[\chout]{S(f)-A_n(f)}}{\norm[\cf]{f}} = \norm[q]{\left(\nu_{i_j} \omega_{i_j} \lambda_{i_j} \right)_{j\ge n+1}},
\end{gather}
where again $q$ is given in terms of $\pin$ and $\pout$ by \eqref{lamcond}.

The (weaker) $\cg$-semi-norm of a function $f \in \cf$ may be estimated using the function data $\ip[\chin]{u_{i_j}}{f}$ that is also used to approximate the solution $S(f)$ as follows:
\begin{equation} \label{genLGalg}
G_n(f) = \norm[\cg]{\sum_{i=1}^n \hf_{i_j} u_{i_j} } = \bigg\lVert\bigg(\frac{\hf_{i_j}}{\omega_{i_j}}\bigg)_{j=1}^n\bigg\rVert_{\pin}.
\end{equation}
The error of this approximation is given by the two-sided inequality
\begin{align*}
0 \le \norm[\cg]{f}^{\pin} - G_n^{\pin}(f)
&= \bigg\lVert\bigg(\frac{\hf_{i_j}}{\omega_{i_j}}\bigg)_{j\ge n+1}\bigg\rVert_{\pin}^{\pin} \\
& \le \norm[\infty]{\left(\nu_{i_j} \right)_{j\ge n+1}}^{\pin} \norm[\cf]{f}^{\pin}, \quad 1 \le \pin <  \infty.
\end{align*}

We assume $f$ lies in the specified cone of functions, defined by
\begin{equation} \label{FGconedef}
\cc_{\tau} = \{f \in \cf : \tau_{min}\norm[\cg]{f} \le \norm[\cf]{f} \le \tau \norm[\cg]{f} \},
\end{equation}
where $\tau_{min}=\frac{1}{\norm[\infty]{\left(\nu_{i} \right)_{i\in\ci}}}$ is a low bound on $\norm[\cf]{f}$ that arises naturally since,
\begin{equation*}
\norm[\cg]{f}\le\norm[\cf]{f}\norm[\infty]{\left(\nu_{i} \right)_{i\in\ci}}
\end{equation*}

Note that one has to choose $\tau_{min}\le\tau$ for $\cc_{\tau}$ to contain functions with $\norm[\cf]{f}\neq 0$. This means
\begin{equation*}
\frac{1}{\norm[\infty]{\left(\nu_{i_j} \right)_{j\ge1}}}\le\tau<\frac{1}{\norm[\infty]{\left(\nu_{i_j} \right)_{j\ge n_\tau+1}}}
\end{equation*}
where $n_\tau$ is the minimum natural integer satisfying the inequality.

If we consider $\cc_\tau$, the arguments of \cite{HicEtal14b} lead to a two-sided bound on the $\cg$-semi-norm $f$ in terms of $G_n(f)$.  Specifically, if $\norm[\infty]{\left(\nu_{i_j} \right)_{j\ge n+1}} < 1/\tau$, then for all $f \in \cc_{\tau}$,  it follows that
\begin{gather}\label{ineqnormg}
G_n(f)  \le \norm[\cg]{f} \le \fC_n G_n(f)\\
\fC_n=\begin{cases}\displaystyle \left[1 - \tau^{\pin} \norm[\infty]{\left(\nu_{i_j} \right)_{j\ge n+1}}^{\pin}\right]^{-1/\pin}, & 1 \le \pin <  \infty, \\
1, & \pin =  \infty.
\end{cases}
\end{gather}
The case $\pin=\infty$ follows from the $\pin < \infty$ case by taking the limit as $\pin \to \infty$.  Alternatively, the case $\pin=\infty$ may be proven directly by choosing a positive $\delta$ strictly less than $1/\big(\tau \big\lVert\left(\nu_{i_j} \right)_{j\ge n+1}\big\rVert_{\infty}\big)$ and then noting that there exists $j^* \ge n+1$ with
\begin{align*}
\bigg\lVert\bigg(\frac{\hf_{i_j}}{\omega_{i_j}}\bigg)_{j\ge n+1}\bigg\rVert_{\infty}
& \le  \abs{\frac{\hf_{i_{j^*}}}{\omega_{i_{j^*}}}} (1+\delta) \\
& < \frac{1}{\tau \big\lVert\left(\nu_{i_j} \right)_{j\ge n+1}\big\rVert_{\infty}}  \abs{\frac{\hf_{i_{j^*}}}{\omega_{i_{j^*}}}}
\le \frac{1}{\tau } \abs{\frac{\hf_{i_{j^*}}}{\nu_{i_{j^*}} \omega_{i_{j^*}}}}\\
& \le \frac{1}{\tau }\bigg\lVert\bigg(\frac{\hf_{i_j}}{\nu_{i_j}\omega_{i_j}}\bigg)_{j\ge n+1}\bigg\rVert_{\infty} \le \frac{\norm[\cf]{f}}{\tau} \le \norm[\cg]{f}
\end{align*}
This strict inequality implies the existence of some $j \le n$ such that $\lvert\hf_{i_j}/\omega_{i_j} \rvert = \norm[\cg]{f}$, and so $G_n(f)$ has no error.

Combining the upper bound on $\norm[\cg]{f}$ (\ref{ineqnormg}), the cone condition (\ref{FGconedef}), and error bound \eqref{erreigweakF}, implies that
\begin{equation}\label{ineqalgo1}
\norm[\chout]{S(f)-A_n(f)} \le \fC_n G_n(f) \tau h(n). %\norm[\infty]{\left(\nu_{i_j} \omega_{i_j} \lambda_{i_j} \right)_{j\ge n+1}}
\end{equation}
Thus, for $n\ge n_\tau$, one increases $n$ until the right hand side falls below some tolerance, $\varepsilon$.  The drawback of this approach is that one needs to choose the weights $\nu_i$ and $\omega_i$, which define the spaces $\cf$ and $\cg$.

\begin{rem}
It is important to see that a better bound could be
\begin{equation*}
\norm[\chout]{S(f)-A_n(f)} \le \fC_n G_n(f) \min(\tilde{h}(n) , \tau h(n))
\end{equation*}
However, if $n\ge n_\tau$,
\begin{equation*}
\tau h(n) \le \tilde{h}(n) \tau \norm[\infty]{\left(\nu_{i_j} \right)_{j\ge n+1}} < \tilde{h}(n),
\end{equation*}
implying $\min(\tilde{h}(n) , \tau h(n))=\tau h(n)$.
\end{rem}

\subsection{Embedded Non-Adaptive Algorithms}

According to \cite{HicEtal14b}, the upper bound in (\ref{ineqalgo1}) can be used to define an algorithm based on \textit{Embedded Nonadaptive Algorithms}. Thus, with the same notation, we recall that the error of an algorithm $A$ of this kind is:
\begin{equation} \label{errdefworst}
\err(A,\cf,\chout,S)
= \min\{ \delta \ge 0 : \norm[\chout]{S(f) -  A(f)} \le \delta \norm[\cf]{f} \ \forall f \in \cf \},
\end{equation}
while the complexity of a problem for the set of algorithms $\ca_{\non}(\cf,\ch,S,\Lambda)$ becomes,
\begin{multline} \label{fixcostcomplex}
\comp(\varepsilon,\ca_{\non}(\cf,\ch,S,\Lambda)) \\
= \inf\left\{\cos
t(A) : \err(A,\cf,\chout,S) \le \varepsilon, \ A \in \ca_{\non}(\cf,\ch,S,\Lambda) \right \}.
\end{multline}

Indeed, the equalities in (\ref{erreigweakF}) imply
\begin{gather}
\err(A_{n},\cg,\chout,S) = \tilde{h}(n),\\
\err(A_{n},\cf,\chout,S) = h(n).
\end{gather}

\begin{algo}\label{algo1}
 Consider the Banach spaces $\cf$, $\cg$, $\chin$ and $\chout$, the solution operator $S$, $\tau$, $n_\tau$ and the error tolerance $\varepsilon$ as described above. Moreover, let $\{A_n\}_{n\in\naturals}$, $A_n\in\ca_{\non}(\chin,\chout,S,\Lambda)$, be the sequence of non adaptive algorithms defined in (\ref{Andef}) and set $k=n_\tau$. For any input function $f\in\cc_\tau$, do the following:
\begin{description}
\item[Stage 1. Estimate the Bound.] Compute $G_{k}(f)$ and $\fC_{k}$. The $\tau$ chosen guarantees that $\fC_{k}G_{k}(f)$ provides a reliable upper bound on $\norm[\cg]{f}$.
\item[Stage 2. Check for Convergence.] Check whether $k$ is large enough to satisfy the error tolerance, i.e.,
    \begin{equation}
          \tau \fC_{k} G_{k}(f) h(n) \le \varepsilon
    \end{equation}
    If this is true, then return $A_{k}(f)$ and terminate the algorithm.
\item[Stage 3. Compute $k+1$.] Otherwise, increase $k$ by $1$ and return to Stage $1$.
\end{description}
\end{algo}

Following with the notation in \cite[p.\ 20]{HicEtal14b}, we define $f_1$ with $\hat{f_1}_{i_j}=0$ if $j\ge n+2$ and such that for all $\$(\vL)\le n$, $\norm[\ch]{S(f_1)} = 1$ and $\vL(f_1)= \vzero$. Thus,
\begin{align*}
\norm[\cg]{f_1} & \le \norm[\pout]{\left(\lambda_{i_j} \hf_{i_j} \right)_{j = 1}^{n+1}}\norm[\bar{q}]{\left(\frac{1}{\lambda_{i_j} \omega_{i_j}} \right)_{j = 1}^{n+1}},\quad \bar{q}= \frac{\pin\pout}{\max(\pout-\pin,0)} \\
\norm[\cf]{f_1} & \le \norm[\infty]{\left(\frac{1}{\nu_{i_j}} \right)_{j = 1}^{n+1}} \norm[\cg]{f_1}.
\end{align*}
Then,
\begin{gather}
\tg(n) = \norm[\bar{q}]{\left(\frac{1}{\lambda_{i_j} \omega_{i_j}} \right)_{j = 1}^{n+1}}\\
g(n) = \norm[\infty]{\left(\frac{1}{\nu_{i_j}} \right)_{j = 1}^{n+1}}.
\end{gather}

\begin{prop}\label{optalgoseq}
The sequences $g(n)$, $\tg(n)$ and $h(n)$ satisfy
\begin{equation*}
\sup_{m\ge 1} \inf\{j : j \in \naturals, \ g(m)\tg(m)h(jm) \le 1 \}  < \infty
\end{equation*}
\end{prop}
\begin{proof}
Non-increasing subsequence.
\end{proof}

\begin{theorem}
The Algorithm \ref{algo1} satisfies
\begin{equation*}
\sup_{0 < \varepsilon/\sigma \le 1}\frac{\cost(A,\cc_\tau,\varepsilon,\infty,\sigma)} {\comp(\varepsilon/\sigma,\ca_{\non}(\cf,\chout,S,\Lambda))} <\infty
\end{equation*}
\end{theorem}
\begin{proof}
Using proposition \ref{optalgoseq} and the proof of \cite{HicEtal14b}.........
\end{proof}

For the lower bound on the complexity, $\tau_0=\frac{1}{\norm[\infty]{\left(\nu_{i_j} \right)_{j=1}^{n_\tau}}}$. Then we can apply theorem $5$ in \cite{HicEtal14b} to affirm that:

\begin{equation*}
\comp(\varepsilon,\ca(\cc_{\tau},\ch,S,\Lambda),\infty,\sigma) \ge  (g\tg)^{-1}\left(\frac{\sigma\tau(\tau-\tau_0)}{2(2\tau-\tau_0)\varepsilon}\right).
\end{equation*}

\subsection{Example: the embedding operator applied to the parabola $f(x)=x^2-x+\frac{1}{4}$}

Lets consider $\chin=\chout=\cl_2[0,1]$, and $S: f \mapsto f$ the embedding operator with:
\begin{gather*}
\norm[\chin]{f} = \norm[\chout]{f} = \left[\int_0^1 \abs{f(x)}^2 \, \dif x \right]^{1/2}, \\
u_{i}(x) = v_i(x) = \me^{2 \pi \sqrt{-1} i x},\quad \lambda_i=1, \qquad i \in \integers,\\
\pin=\pout=2,\quad q=\infty.
\end{gather*}

In this case, our index set $\ci=\integers=\{0,1,-1,2,-2,\ldots\}$ and we define,
\begin{gather*}
\omega_{i_j} = 1,\quad \nu_{0}=0,\quad \nu_{i_j} = \frac{1}{\abs{i_j}}, \qquad i \in \ci\setminus\{0\},\\
i_j=(-1)^{j}\left\lfloor\frac{j}{2}\right\rfloor,\qquad j\in\naturals.
\end{gather*}
Therefore, one must have $n_\tau=2\lfloor\tau\rfloor+1$. Setting $\tau=2$, $n_\tau=5$ and $\tau_0=1$.

Using integration by parts, Fourier coefficients are as follows,
\begin{gather*}
\hf_{0}=\frac{1}{12},\\
\hf_{i_j}=\int_{[0,1)}x^2e^{2\pi\sqrt{-1}i_jx}dx-\int_{[0,1)}xe^{2\pi\sqrt{-1}i_jx}dx=\frac{1}{2\pi^2{i_j}^2}, \quad i_j\neq 0
\end{gather*}

\begin{center}
\begin{tabular}{|c|c|c|c|}
  \hline
  % after \\: \hline or \cline{col1-col2} \cline{col3-col4} ...
  \multicolumn{3}{|c|}{$f(x)=x^2-x+\frac{1}{4}$} \\
  \hline
  $\varepsilon$ & $n$ & $\operatorname{error}=\norm[\chout]{S(f(x))}-\norm[\chout]{A_{n}(f(x))}$ \\
  \hline
  $1$ & $5$ & $4.5598\cdot10^{-4}$ \\
  $10^{-1}$ & $29$ & $2.5040\cdot10^{-6}$ \\
  $10^{-2}$ & $297$ & $2.3365\cdot10^{-9}$ \\
  $10^{-3}$ & $2961$ & $2.3579\cdot10^{-12}$ \\
  $10^{-4}$ & $29601$ & $5.5511\cdot10^{-15}$ \\
  $10^{-5}$ & $296003$ & $2.5951\cdot10^{-15}$ \\
  $10^{-6}$ & $2960021$ & $2.5951\cdot10^{-15}$ \\
  \hline
\end{tabular}
\end{center}

\begin{rem}
For this weights $\omega_i$ and $\nu_i$, our bound is of $\Theta(\varepsilon^{-1})$ as the complexity so we cannot do better.
\end{rem}

\section{Assuming a Gentle Decay in the Terms of the Series}

Another method for estimating the error of $A_n$ assumes that the decay of the Fourier coefficients of $f$ follows some general rate of decay, which need not be known precisely.  Suppose again that we have some ordering of the linear functionals, as in the previous section, and let $0=n_0 < n_1 < n_2 < \ldots$ be an ordered, unbounded sequence of integers.  Define the sums
\begin{equation} \label{sumdef}
\sigma_k(f) = \norm[\pout]{ \left(\lambda_{i_j} \hf_{i_j} \right)_{j=n_{k-1}+1}^{n_k}}, \qquad k=1, 2, \ldots
\end{equation}
and the cone of functions in $\chin$ as those whose Fourier coefficients decay at a given rate:
\begin{equation} \label{decayconedef}
\cc = \left\{ f \in \chin : \sigma_{k+r}(f) \le \gamma(r) \sigma_k (f), \ \forall k \in \naturals \right\}.
\end{equation}
where the sequence $\gamma(r)$ has to verify $\lim_{n\rightarrow\infty}\gamma(n)=0$, $\gamma(r) > 0$ and that
\begin{equation} \label{constcone}
\norm[\pout]{\bgamma}=\norm[\pout]{ \left(\gamma(r)\right)_{r=1}^\infty}<\infty
\end{equation}

\begin{rem}
The definition of the cone can be rewritten as
\begin{equation}\label{coneredef}
\cc = \left\{ f \in \chin : \sigma_k(f) \le \min_{1 \le i < k}\{\gamma(i)\sigma_{k-i}(f)\}, \ \forall k \in \naturals \right\}.
\end{equation}
\end{rem}

To see some examples on how these conditions might arise naturally, consider the case where $\gamma(r)=s_1s_2^{-r}$ with the terms in the sum defining $\sigma_k(f)$ decaying \emph{algebraically} and the $n_k$ increasing \emph{geometrically}:
\begin{gather*}
C_{\lo} j^{-p} \le \abs{\lambda_{i_j} \ip[\chin]{u_{i_j}}{f}} \le C_{\up} j^{-p}, \quad  p>1, \ j \in \naturals, \\
n_k = a b^k  \quad a, b, k \in \naturals, \ b \ge 2.
\end{gather*}
The sum of positive integers raised to a power can be interpreted as a left or right rectangle rule for approximating an integral.  This leads to upper and lower bounds for the sums:
\begin{align*}
\sum_{j=n_{\lo}}^{n_{\up}} j^{-p}  & \ge \int_{n_{\lo}}^{n_{\up}+1} x^{-p}\, \dif x  = \frac{n_{\lo} ^{1-p} - (n_{\up}+1)^{1-p}}{p-1}, \\
\sum_{j=n_{\lo}}^{n_{\up}} j^{-p}  & \le \int_{n_{\lo}-1}^{n_{\up}} x^{-p}\, \dif x = \frac{(n_{\lo}-1) ^{1-p} - n_{\up}^{1-p}}{p-1}.
\end{align*}
These two bounds can be used to prove that $f$ lies in the cone \eqref{decayconedef} for appropriately chosen $s_1$ and $s_2$:
\begin{multline*}
\sigma_k(f) \ge C_{\lo} \left\{\frac{(n_{k-1}+1) ^{1-p\pout} - (n_{k}+1)^{1-p \pout}}{p \pout-1}\right\}^{1/\pout} \\
= C_{\lo} \left\{\frac{[ab^{k-1}]^{1-p \pout}}{p \pout-1} \left[(1+a^{-1}b^{1-k})^{1-p \pout} - (b+a^{-1}b^{1-k})^{1-p \pout}\right] \right\}^{1/\pout},
\end{multline*}
\begin{multline*}
\sigma_k(f) \le C_{\up}\left\{\frac{n_{k-1} ^{1-p \pout} - n_{k}^{1-p \pout}}{p \pout-1}\right\}^{1/\pout} \\
= C_{\up}\left\{\frac{[ab^{k-1}]^{1-p \pout} (1 - b^{1-p \pout})}{p \pout-1}\right\}^{1/p} ,
\end{multline*}
\begin{align*}
\frac{\sigma_{k+r}(f)}{\sigma_k(f)} &\le \frac{C_{\up}}  {C_{\lo}}
\left\{\frac{ (1 - b^{1-p \pout}) b^{(p \pout -1)(-r)} }  {(1+a^{-1}b^{1-k})^{1-p\pout} - (b+a^{-1}b^{1-k})^{1-p\pout}} \right\}^{1/\pout}\\
& \le \frac{C_{\up}}  {C_{\lo}}
\left\{\frac{(1 - b^{1-p\pout}) b^{(p \pout -1)(-r)} }  {(1+a^{-1})^{1-p\pout} - (b+a^{-1})^{1-p\pout}} \right\}^{1/\pout}   = s_1 s_2^{-r},
\end{align*}
where
\[
s_1 = \frac{ C_{\up}}  {C_{\lo}}\left\{\frac{(1 - b^{1-p\pout})}  {(1+a^{-1})^{1-p\pout} - (b+a^{-1})^{1-p\pout} } \right\}^{1/\pout}, \qquad s_2 = b^{p-1/\pout}.
\]

One may also consider the case where the terms in the sum defining $\sigma_k(f)$ decay \emph{exponentially} and the $n_k$ increase \emph{linearly}:
\begin{gather*}
C_{\lo} p^{-j} \le \abs{\lambda_{i_j} \ip[\chin]{u_{i_j}}{f}} \le C_{\up} p^{-j}, \quad  p>1, \ j \in \naturals, \\
n_k = a +kb  \quad a, b, k \in \naturals.
\end{gather*}
The geometric sum that now arises in a bound on the  definition of $\sigma_k(f)$ takes the form
\begin{equation*}
\sum_{j=n_{\lo}}^{n_{\up}} p^{-j} = \frac{p^{-n_{\lo}} - p^{-n_{\up}-1}}{1-p^{-1}}, \\
\end{equation*}
which implies that
\begin{align*}
\frac{\sigma_{k+r}(f)}{\sigma_k(f)} & \le
\frac{C_{\up}}  {C_{\lo}} \left\{\frac{p^{-\pout( n_{k+r-1}-1} - p^{- \pout(n_{k+r}-1)}]}  {C_{\lo}^2[p^{-\pout(n_{k-1}-1)} - p^{-\pout(n_{k}-1)}]}\right\}^{1/\pout} \\
& =
\frac{C_{\up} p^{n_{k-1}-n_{k+r-1}}} {C_{\lo}}  \left\{\frac{1 - p^{\pout(n_{k+r-1}-n_{k+r})}} {1 - p^{\pout(n_{k-1}-n_{k})}]} \right\}^{1/p}\\
& = \frac{ C_{\up} p^{b(-r)}}  {C_{\lo}}  = s_1 s^{-r},
\end{align*}
where
\[
s_1 = \frac{ C_{\up}}  {C_{\lo}} , \qquad s_2 = p^b.
\]

From the definition of the cone and \eqref{errsum}, one can show that
\begin{equation}\label{algoineq}
\begin{aligned}
\norm[\chout]{S(f)-A_{n_k}(f)} &= \norm[\pout]{\left(\lambda_{i_j} \hf_{i_j} \right)_{j\ge n_k+1}} \\
&= \left\{\sum_{r=1}^\infty \sum_{j=n_{k+r-1}+1}^{n_{k+r}}  \abs{\lambda_{i_j}\tf_{i_j} }^{\pout}  \right\}^{1/\pout}\\
&= \norm[\pout]{ \left(\sigma_{k+r}(f)\right)_{r=1}^{\infty}} \\
&\le \norm[\pout]{ \left(\gamma(r)\sigma_{k}(f)\right)_{r=1}^{\infty}} \\
&= \sigma_{k}(f)\norm[\pout]{\bgamma}
\end{aligned}
\end{equation}

Since the right hand side depends on the data, we have a data-driven error bound.  Note also that $\sigma_k(f)$ decays as quickly with respect to $n$ as the true error.  Unlike the earlier method, one does not need to know the decay rate.  In practice, one increases $k$ until the right hand side is smaller than the error tolerance.

\subsection{Adaptive algorithm}

The approximate solution to the problem $S:\cc\rightarrow\chout$ depends on
Given the error tolerance $\varepsilon$, we have to find $k^*$ such that
\begin{equation*}
         \sigma_{k^*}(f) \le \frac{\varepsilon}{\norm[\pout]{\bgamma}} < \sigma_{k^*-1}(f)
\end{equation*}

According to the inequality (\ref{algoineq}) above, this $k^*$ is ensuring
\begin{equation*}
\norm[\chout]{S(f)-A_{n_{k^*}}(f)} \le \varepsilon
\end{equation*}

Remember that in order to build our algorithm, we need to define a countable, non-negative-valued index set such that,
\begin{equation} \label{indexdef}
\cn=\{n_1, n_2, \ldots\} \quad \text{with } n_k < n_{k+1}, \quad \text{satisfying } \sup_k \frac{n_{k+1}}{n_k} <\infty.
\end{equation}

\begin{algo}\label{algo2}
 Consider the set of functions $\cc$, the error tolerance $\varepsilon$ and the sequence $\{A_n\}_{n \in \ci}$ as described before.
The algorithm starts by setting $k=1$.
\begin{description}
\item[Stage 1. Computing $\sigma(f)$.] First we compute $\sigma_{k}(f)$.
\item[Stage 2. Check for Convergence.] Check whether $k$ is large enough to satisfy the error tolerance, i.e.,
    \begin{equation}
          \sigma_{k}(f) \le \frac{\varepsilon}{\norm[\pout]{\bgamma}}
    \end{equation}
    If this is true, then we return $A_{n_{k}}(f)$ and terminate the algorithm.
\item[Stage 3. Increasing $k$.] Otherwise, we increase $k$ by $1$ and return to Stage $1$.
\end{description}
\end{algo}

 By construction, the cost of this Algorithm \ref{algo2} is
\begin{equation}\label{costalgo}
\cost(A,f;\varepsilon)=n_{k^*}
\end{equation}

\subsection{Optimality of the Algorithm \ref{algo2}}

The sequence $\cn$ is used to gather the Fourier coefficients into groups to smooth the peaks we could encounter if we take these coefficients one by one. Flattening the peaks is a must to fit functions into the cone. Nevertheless, the solution of our algorithm is in this set $\cn$ which means that the sequence does not have to distance the optimal number of data needed in our algorithm (defined below) more than a constant.

We define the optimal number of data needed in our algorithm as
\begin{equation*}
N_{\opt}(f,\varepsilon)=\min\left\{n\in\naturals:\,\norm[\pout]{\left(\lambda_{i_j} \hf_{i_j} \right)_{j\ge n+1}} \le \varepsilon \right\}
\end{equation*}

The index set for our algorithm defined in (\ref{indexdef}) is called \emph{optimal} for the Algorithm \ref{algo2} and the problem $(\cc,\chout,S,\Lambda)$ if it essentially tracks the optimal number of data needed,
\begin{equation} \label{nearoptdef}
\sup_{0 < \varepsilon \le 1} \frac{\cost(A,f;\varepsilon)} {N_{\opt}(f,\varepsilon)} <\infty.
\end{equation}

\begin{theorem}\label{nopt}
The index set defined in (\ref{indexdef}) is optimal for Algorithm \ref{algo2}.
\end{theorem}
\begin{proof}
For a given $f$ and $\varepsilon$, consider $k^*$ depending on $\varepsilon$ such that
\begin{equation*}
\sigma_{k^*}(f) \le \frac{\varepsilon}{\norm[\pout]{\bgamma}} < \sigma_{k^*-1}(f)
\end{equation*}

From the definition of $N_{\opt}(f,\varepsilon)$, it is clear that $N_{\opt}(f,\varepsilon)=n^*\le n_{k^*}$.

Because $\lim_{n\rightarrow\infty}\gamma(n)=0$, we can take $r=\min\{n: \gamma(n)\norm[\pout]{\bgamma} \le 1\}$. Define also $\sup_k\frac{n_{k+1}}{n_k}=C$ according to the properties in (\ref{indexdef}).

Then, if $k^*>r+1$,
\begin{align*}
& \varepsilon \le \frac{\varepsilon}{\gamma(r)\norm[\pout]{\bgamma}} < \sigma_{k^*-1-r}(f) \le \norm[\pout]{S(f)-A_{n_{k^*-1-r}}(f)} \\
\norm[\pout]{S(f)-A_{n^*}(f)} \le & \varepsilon
\end{align*}
what means that $n_{k^*-1-r} < n^* \le n_{k^*}$. Therefore,
\begin{equation*}
\frac{n_{k^*}}{n^*} < \frac{n_{k^*}}{n_{k^*-1}}\times\cdots\times\frac{n_{k^*-r}}{n_{k^*-1-r}} \le C^{r+1}
\end{equation*}

If $k^* \le r+1$,
\begin{equation*}
\frac{n_{k^*}}{n^*} \le \frac{n_{k^*}}{n_{k^*-1}}\times\cdots\times\frac{n_{2}}{n_{1}}\times n_{1} \le n_{1}C^{k^*-1}
\end{equation*}

Thus,
\begin{equation*}
\sup_{0 < \varepsilon \le 1} \frac{\cost(A,f;\varepsilon)} {N_{\opt}(f,\varepsilon)} \le n_1C^{r+1}.
\end{equation*}
\end{proof}

The cost of any algorithm $A\in\ca(\chin,\chout,S,\Lambda)$ may also depend not only on a function, but on a set of functions $\cb\subseteq\chin$. To represent this idea one defines
\begin{equation*}
\cost(A,\cb,\varepsilon)=\sup_{f\in\cb}\{\cost(A,f;\varepsilon)\}.
\end{equation*}

The next step is comparing our Algorithm \ref{algo2} to the whole set of possible algorithms in $\ca(\cc,\chout,S,\Lambda)$. If the conditions describing the cone $\cc$ are well stated, then our algorithm will not have a greater cost than a constant times the best algorithm in $\ca(\cc,\chout,S,\Lambda)$.

Lets define now the following set of functions,
\begin{equation*}
\cj_{n,\varepsilon}=\left\{f\in\chin:\norm[\pout]{\left(\lambda_{i_j} \hf_{i_j} \right)_{j\ge n+1}} \le \varepsilon \right\}
\end{equation*}
One can see that $\cj_{n,\varepsilon}\subseteq\cj_{m,\varepsilon}$ if $n\le m$, and $\lim_{n\rightarrow\infty}\cj_{n,\varepsilon}=\chin$.
\begin{theorem}\label{supcost}
   Given $A\in\ca(\cj_{n,\varepsilon},\chout,S,\Lambda)$, if $\,$ $\norm[\chout]{S(f)-A(f)}\le\varepsilon$ $\forall f\in\cj_{n,\varepsilon}$, then
 \begin{equation*}
 \cost(A,\cj_{n,\varepsilon},\varepsilon)\ge n
 \end{equation*}
\end{theorem}
\begin{proof}
Consider the following subsets of $\cj_{n,\varepsilon}$,
\begin{gather*}
\cf_n=\spn\{u_{i_1}(x),\ldots,u_{i_n}(x)\}=\{\hf_{i_1}u_{i_1}(x)+\ldots+\hf_{i_n}u_{i_n}(x),\, \hf_{i_j}\in\complex\},\\
\cf_{n,A}=\{f\in\cf_n:\, L_1(f)=\ldots=L_m(f)=0,\, m<n\}.
\end{gather*}
where $m$ is the cost that we suppose lower than $n$. Remark that,
\begin{gather*}
 \forall\varepsilon,\;\cf_n\subseteq\cj_{n,\varepsilon},\\
 \forall f\in\cf_{n,A},\;A(f)=A(0)
\end{gather*}

Therefore, there is the following contradiction
\begin{align*}
\sup_{f\in\cf_{n,A}}\norm[\chout]{S(f)-A(f)}&=\sup_{f\in\cf_{n,A}}\norm[\chout]{S(f)-A(0)}\\
&\ge\sup_{f\in\cf_{n,A}}\norm[\chout]{S(f)}-\norm[\chout]{A(0)}\\
&>\varepsilon
\end{align*}
For the last last inequality we are using
\begin{equation*}
\sup_{f\in\cf_{n,A}}\norm[\chout]{S(f)}\ge\lim_{\substack{\lambda_{i_l}\neq 0 \\ |\hf_{i_l}|\rightarrow\infty}}\norm[\chout]{\left(\lambda_{i_j} \hf_{i_j} \right)_{j=1}^n}
\end{equation*}
\end{proof}

The complexity of a problem for the set of algorithms $\ca(\cb,\chout,S,\Lambda)$ is defined as the cost of the cheapest algorithm that satisfies the specified error tolerance, $\varepsilon$:
\begin{equation}\label{complexity2}
\begin{aligned}
\comp(\varepsilon,&\ca(\cb,\chout,S,\Lambda))\\
&=\inf_{A\in\ca(\cb,\chout,S,\Lambda)}\left\{\cost(A,\cb,\varepsilon):\norm[\chout]{S(f)-A(f)}\le\varepsilon,\; \forall f\in\cb\right\}
\end{aligned}
\end{equation}

According to notation in equation (\ref{indexdef}), if $n\in\cn$, $\forall f\in\cc\cap\cj_{n,\varepsilon}$, for Algorithm \ref{algo2} we have that $n\ge\cost(A;f,\varepsilon)\ge N_{\opt}(f,\varepsilon)$. Thus, using Theorem \ref{supcost} we know that
\begin{equation*}
\comp(\varepsilon,\ca(\cc\cap\cj_{n,\varepsilon},\chout,S,\Lambda))=n
\end{equation*}

\begin{prop}[for Theorem \ref{optimality}]\label{optimcost}
The cost of our second algorithm tracks the optimal cost for all $f\in\cc$
\begin{equation*}
\sup_{0 < \varepsilon \le 1}\sup_{f\in\cc} \frac{\cost(A,f;\varepsilon)} {\min\{n:f\in\cj_{n,\varepsilon}\}} <\infty
\end{equation*}
\end{prop}
\begin{proof}
First of all
\begin{equation*}
\min\{n:f\in\cj_{n,\varepsilon}\}=N_{\opt}(f,\varepsilon)
\end{equation*}
Recalling the proof of Theorem \ref{nopt}, remark that $r=\min\{n: \gamma(n)\norm[\pout]{\bgamma} \le 1\}$ does not depend on $k^*$ which means that $r$ does not depend on $f$
\begin{equation*}
\sup_{0 < \varepsilon \le 1} \frac{\cost(A,f;\varepsilon)} {N_{\opt}(f,\varepsilon)} \le n_1C^{r+1},\quad\forall f\in\cc
\end{equation*}
\end{proof}

\begin{theorem}\label{optimality}
Our Algorithm $\ref{algo2}$ is optimal for the problem $(\cc,\chout,S,\Lambda)$,
\begin{equation*}
\sup_{0 < \varepsilon \le 1}\frac{\cost(A,\cc,\varepsilon)} {\comp(\varepsilon,\ca(\cc,\chout,S,\Lambda))} <\infty
\end{equation*}
\end{theorem}
\begin{proof}
Because $\lim_{n\rightarrow\infty}\cj_{n,\varepsilon}=\chin$, there is $m$ such that $\cc\cap\cj_{m,\varepsilon}\neq\emptyset$. Then,
\begin{align*}
\comp(\varepsilon,\ca(\cc,\chout,S,\Lambda))&\ge\comp(\varepsilon,\ca(\cc\cap\cj_{m,\varepsilon},\chout,S,\Lambda))\\
&\ge\inf_{f\in\cc}\min\{j\in\naturals:f\in\cj_{j,\varepsilon}\}
\end{align*}
The first inequality comes from the fact that $\cc\supseteq\cc\cap\cj_{m,\varepsilon}$ and for the second one, $m\ge\inf_{f\in\cc}\min\{j\in\naturals:f\in\cj_{j,\varepsilon}\}$ as there exists $f\in\cc\cap\cj_{m,\varepsilon}$.
.........................................
%Then we can apply Proposition \ref{optimcost} to obtain the proof:
%\begin{equation*}
%\sup_{0 < \varepsilon \le 1}\frac{\cost(A,\cc,\varepsilon)} {\comp(\varepsilon,\ca(\cc,\chout,S,\Lambda))} <\infty
%\end{equation*}
\end{proof}

\subsection{Lower Complexity Bounds for Algorithms \ref{algo1} and \ref{algo2}}



\subsection{Examples}
\subsubsection{The embedding of $f(x)=\frac{(b^2-1)}{b^2+1-2b\cos(2\pi x)}$}

In our example, consider $\chin=\chout=\cl_2[0,1]$, and $S: f \mapsto f$ the embedding operator with:
\begin{gather*}
\norm[\chin]{f} = \norm[\chout]{f} = \left[\int_0^1 \abs{f(x)}^2 \, \dif x \right]^{1/2}, \\
u_{i}(x) = v_i(x) = \me^{2 \pi \sqrt{-1} i x},\quad \lambda_i=1, \qquad i \in \integers,\\
\pin=\pout=2,\quad q=\infty.
\end{gather*}

For our index set $\ci=\integers=\{0,1,-1,2,-2,\ldots\}$,
\begin{gather*}
i_j=(-1)^{j}\left\lfloor\frac{j}{2}\right\rfloor,\qquad j\in\naturals,\\
n_0=0,\quad n_k=2k+1, \qquad k\in\naturals.
\end{gather*}

The Fourier coefficients of this function can be easily found:
\begin{align*}
f(x)&=\frac{b^2-1}{b^2+1-2b\cos(2\pi x)}\\
&=\frac{1-b^{-2}+b^{-1}(\me^{-2\pi\sqrt{-1}x}-\me^{-2\pi\sqrt{-1}x})}{1+b^{-2}-b^{-1}(\me^{2\pi\sqrt{-1}x}+\me^{-2\pi\sqrt{-1}x})}\\
&=\frac{(1-b^{-1}\me^{-2\pi\sqrt{-1}x})+(1-b^{-1}\me^{2\pi\sqrt{-1}x})b^{-1}\me^{-2\pi\sqrt{-1}x}}{(1-b^{-1}\me^{2\pi\sqrt{-1}x})(1-b^{-1}\me^{-2\pi\sqrt{-1}x})}\\
&=\frac{1}{1-b^{-1}\me^{2\pi\sqrt{-1}x}}+\frac{b^{-1}\me^{-2\pi\sqrt{-1}x}}{1-b^{-1}\me^{-2\pi\sqrt{-1}x}}\\
&=\sum_{k\in\naturals_0}\left(b^{-1}\me^{2\pi\sqrt{-1}x}\right)^k+\sum_{k\in\naturals}\left(b^{-1}\me^{-2\pi\sqrt{-1}x}\right)^k\\
&=\sum_{k\in\integers}b^{-\abs{k}}\me^{2\pi\sqrt{-1}kx}\Longrightarrow\hf_{i_j}=b^{-\abs{i_j}}
\end{align*}

Because we are interested in seeing how far the approximation is from the real solution, the norm of the exact solution has to be found. Knowing the Fourier coefficients leads us to an easy way to calculate it:
\begin{align*}
\norm[\chout]{S(f)}&=\left(\sum_{j\in\integers}\lambda_{i_j}^2\hf_{i_j}^2\right)^{\frac{1}{2}} =\left(1+2\sum_{k\in\naturals}b^{-2k}\right)^{\frac{1}{2}}=\left(\frac{b^2+1}{b^2-1}\right)^{\frac{1}{2}}
\end{align*}

We can also give the sums of coefficients,
\begin{align*}
&\sigma_1(f)=\sqrt{1+2b^{-2}}\\
&\sigma_k(f)=\sqrt{2}b^{-k}, \qquad k \in\naturals\setminus\{1\}.
\end{align*}
Recalling the alternative definition of the cone in (\ref{coneredef}), with these sums, $f\in\cc$ for $\gamma(r)=b^{-r}$ since
\begin{equation*}
\sigma_k(f) \le \min_{1 \le i <k}\{\gamma(i)\sigma_{k-i}(f)\}=\min\left\{1 \; , \; \sqrt{1+\frac{b^2}{2}} \right\}\sigma_k(f)
\end{equation*}

In this case, $\norm[\pout]{\bgamma}=\frac{1}{\sqrt{b^2-1}}$. If we use our Algorithm \ref{algo2}, we have to check first whether $\sqrt{\frac{b^2+2}{b^4-b^2}}\le\varepsilon$. If this inequality states, then $k^*=1$. If not,
\begin{equation*}
k^*=\min\left\{k\in\naturals\setminus\{1\}:\frac{1}{2\log(b)}\log\left(\frac{2}{(b^2-1)\varepsilon^2}\right) \le k \right\}
\end{equation*}

For a numerical application, set $b=3$ and find below a table of results for this example,

\begin{center}
\begin{tabular}{|c|c|c|c|}
  \hline
  % after \\: \hline or \cline{col1-col2} \cline{col3-col4} ...
  $\varepsilon$ & $k^*$ & $n_{k^*}$ & $\operatorname{error}=\norm[\chout]{S(f)}-\norm[\chout]{A_{n_{k^*}}(f)}$ \\
  \hline
  $1$ & $1$ & $3$ & $1.2492\cdot10^{-2}$ \\
  $10^{-1}$ & $2$ & $5$ & $1.3811\cdot10^{-3}$ \\
  $10^{-2}$ & $4$ & $9$ & $1.7041\cdot10^{-5}$ \\
  $10^{-3}$ & $6$ & $13$ & $2.1038\cdot10^{-7}$ \\
  $10^{-4}$ & $8$ & $17$ & $2.5973\cdot10^{-9}$ \\
  $10^{-5}$ & $10$ & $21$ & $3.2065\cdot10^{-11}$ \\
  $10^{-6}$ & $12$ & $25$ & $3.9591\cdot10^{-13}$ \\
  $10^{-7}$ & $15$ & $31$ & $4.4409\cdot10^{-16}$ \\
  \hline
\end{tabular}
\end{center}

\subsubsection{Derivation of the Bernoulli polynomials}

The properties we will use for these particular polynomials can be found in \cite{AbrSte64}: derivatives, explicit Fourier coefficients, etc. To deal with this example, now $\chin=\chout=\cl_2[0,1]$, and $S: f \mapsto f'$ with,
\begin{gather*}
\norm[\chin]{f} = \left[\int_0^1 \left\{\abs{f(x)}^2 + \abs{f'(x)}^2\right\}\, \dif x\right]^{1/2}, \quad \norm[\chout]{f} = \left[\int_0^1 \abs{f(x)}^2 \, \dif x \right]^{1/2}, \\
u_{i}(x) = \frac{\me^{2 \pi \sqrt{-1} i x}}{\sqrt{1+4 \pi^2 i^2}}, \quad v_i(x) = \me^{2 \pi \sqrt{-1} i x}, \quad \lambda_i=\frac{2 \pi \sqrt{-1} i}{\sqrt{1+4 \pi^2 i^2}}, \qquad i \in \integers,\\
\pin=\pout=2,\quad q=\infty.
\end{gather*}

Like in the previous example, the index set $\ci=\integers=\{0,1,-1,2,-2,\ldots\}$ has the following ordering
\begin{gather*}
i_j=(-1)^{j}\left\lfloor\frac{j}{2}\right\rfloor,\qquad j\in\naturals,\\
n_0=0,\quad n_k=2^k, \qquad k\in\naturals.
\end{gather*}

For this case,
\begin{equation*}
B_n(x)=-\frac{n!}{(2\pi\sqrt{-1})^n}\sum_{k\in\integers\setminus\{0\}}\frac{\me^{2\pi\sqrt{-1}kx}}{k^n}\Longrightarrow
\left\{\begin{array}{l}
\widehat{B_n}_0=0 \\
\widehat{B_n}_{i_j}=-\frac{n!\sqrt{1+4 \pi^2 i_j^2}}{(2\pi\sqrt{-1}i_j)^n},\quad i_j\in\integers\setminus\{0\}
\end{array}\right.
\end{equation*}

Using that $B'_n(x)=nB_{n-1}(x)$ and $\int_0^1B_n(t)B_m(t)\dif t=(-1)^{n-1}\frac{m!n!}{(m+n)!}B_{n+m}$, the real solution comes automatically,
\begin{equation*}
\norm[\chout]{S(B_n(x))}=n\norm[\chout]{B_{n-1}(x)}=n!\sqrt{\frac{\abs{B_{2(n-1)}}}{[2(n-1)]!}}
\end{equation*}
where $B_n=B_n(0)$ are the Bernoulli numbers.

If we consider $\gamma(r)=s_1s_2^{-r}$ and that
\begin{equation}\label{bound}
\frac{n!}{\sqrt{2}\pi^n}j^{-n} \le \abs{\lambda_{i_j}\widehat{B_n}_{i_j}} \le \frac{n!2^n}{\pi^n}j^{-n},\quad j\in\naturals\setminus\{1\}
\end{equation}
then $C_{\up}=2^{n+\frac{1}{2}}C_{\lo}=\frac{n!2^n}{\pi^n}$. Thus, as shown in the example example at the beginning of this section we can take
\begin{equation*}
s_1=2\times11^{n-\frac{1}{2}},\quad s_2=2^{n-\frac{1}{2}}
\end{equation*}
There is still one thing to check because $j=1$ was not taken into account in the inequality (\ref{bound}). However, see that $\sigma_1(f) \ge C_{\lo}\left(\frac{5}{36}\right)^{n-\frac{1}{2}}$ what means that the $\gamma(r)=s_1s_2^{-r}$ found satisfies our needs.

For a numerical application, find below the results for the case $n=5$ and $n=10$,

\begin{center}
\begin{tabular}{|c|c|c|c|}
  \hline
  % after \\: \hline or \cline{col1-col2} \cline{col3-col4} ...
  \multicolumn{4}{|c|}{$B_5(x)=x^5-\frac{5}{2}x^4+\frac{5}{3}x^3-\frac{1}{6}x$} \\
  \hline
  $\varepsilon$ & $k^*$ & $n_{k^*}$ & $\operatorname{error}=\norm[\chout]{S(B_5(x))}-\norm[\chout]{A_{n_{k^*}}(B_5(x))}$ \\
  \hline
  $1$ & $5$ & $32$ & $2.9439\cdot10^{-11}$ \\
  $10^{-1}$ & $6$ & $64$ & $2.2703\cdot10^{-13}$ \\
  $10^{-2}$ & $7$ & $128$ & $1.8735\cdot10^{-15}$ \\
  $10^{-3}$ & $7$ & $128$ & $1.8735\cdot10^{-15}$ \\
  $10^{-4}$ & $8$ & $256$ & $1.1102\cdot10^{-16}$ \\
  $10^{-5}$ & $9$ & $512$ & $9.7145\cdot10^{-17}$ \\
  \hline
\end{tabular}
\end{center}

\begin{center}
\begin{tabular}{|c|c|c|c|}
  \hline
  % after \\: \hline or \cline{col1-col2} \cline{col3-col4} ...
  \multicolumn{4}{|c|}{$B_{10}(x)=x^{10}-5x^9+\frac{15}{2}x^8-7x^6+5x^4-\frac{3}{2}x^2+\frac{5}{66}$} \\
  \hline
  $\varepsilon$ & $k^*$ & $n_{k^*}$ & $\operatorname{error}=\norm[\chout]{S(B_{10}(x))}-\norm[\chout]{A_{n_{k^*}}(B_{10}(x))}$ \\
  \hline
  $1$ & $5$ & $32$ & $7.7716\cdot10^{-16}$ \\
  $10^{-1}$ & $5$ & $32$ & $7.7716\cdot10^{-16}$ \\
  $10^{-2}$ & $6$ & $64$ & $7.7716\cdot10^{-16}$ \\
  $10^{-3}$ & $6$ & $64$ & $7.7716\cdot10^{-16}$ \\
  $10^{-4}$ & $7$ & $128$ & $7.7716\cdot10^{-16}$ \\
  $10^{-5}$ & $7$ & $128$ & $7.7716\cdot10^{-16}$ \\
  \hline
\end{tabular}
\end{center}

\begin{rem}
MATLAB's machine epsilon is $2.2204\cdot10^{-16}$.
\end{rem}

%\subsection{Examples: Approximating the divergence of a vector field}
%
%In this case, $\chin=\{f:\reals^n\mapsto\reals^m,\, f\in C^1(\Omega)\}$ and $\chout=\{g:\reals^n\mapsto\reals,\, g\in C^0(\Omega)\}$ and $S: f\mapsto \nabla\cdot f=\sum_{j=1}^{m}\partial_{x_j}f_j$.
%
%\begin{gather*}
%\norm[\chin]{f} =\left[\int_\Omega \sum_{j=1}^{m}\abs{f_j(x)}^2+\sum_{j=1}^{m}\abs{f'_j(x)}^2 \, \dif x \right]^{1/2},\\
%\norm[\chout]{g} = \left[\int_\Omega \abs{g(x)}^2 \, \dif x \right]^{1/2}, \\
%u_{i}(x) = \frac{\me^{2 \pi \sqrt{-1} <i,x>}}{\sqrt{mV(\omega)+4\pi<i,i>}}, \\
%v_i(x) = \me^{2 \pi \sqrt{-1} <i,x>},\\
% \lambda_i=\frac{2\pi\sqrt{-1}\sum_{j=1}^{m}i_j}{\sqrt{mV(\omega)+4\pi<i,i>}}, \qquad i \in \integers^m,\\
%\pin=\pout=2,\quad q=\infty.
%\end{gather*}

\section*{References}
%\nocite{*}
\bibliographystyle{elsarticle-num}
\bibliography{FJH22,FJHown22}


\end{document}
