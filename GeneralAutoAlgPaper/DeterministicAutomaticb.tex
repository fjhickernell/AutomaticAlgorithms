\documentclass[final]{elsarticle}
\setlength{\marginparwidth}{0.5in}
\usepackage{amsmath,amssymb,amsthm,natbib,mathtools,graphicx}
\input FJHDef.tex

\newcommand{\sphere}{\mathbb{S}}
%\newcommand{\cc}{\mathcal{C}}
\newcommand{\cq}{\mathcal{Q}}
\newcommand{\bbW}{\mathbb{W}}
%\newcommand{\tP}{\widetilde{P}}
\newcommand{\bg}{{\bf g}}
\newcommand{\bu}{{\bf u}}
\newcommand{\bbu}{\bar{\bf u}}
\newcommand{\bv}{{\bf v}}
\newcommand{\bbv}{\bar{\bf v}}
\newcommand{\bw}{{\bf w}}
\newcommand{\bbw}{\bar{\bf w}}
\newcommand{\hv}{\hat{v}}
\DeclareMathOperator{\MSE}{MSE}
\DeclareMathOperator{\RMSE}{RMSE}
\DeclareMathOperator{\rnd}{rnd}
\DeclareMathOperator{\abso}{abs}
\DeclareMathOperator{\rel}{rel}
\DeclareMathOperator{\nor}{nor}
\DeclareMathOperator{\err}{err}
\DeclareMathOperator{\lin}{lin}
%\DeclareMathOperator{\std}{std}
\DeclareMathOperator{\prob}{prob}
\DeclareMathOperator{\trunc}{trc}
\DeclareMathOperator{\third}{third}
%\DeclareMathOperator{\fourth}{fourth}
\newtheorem{theorem}{Theorem}
\newtheorem{prop}[theorem]{Proposition}
\newtheorem{lem}{Lemma}
\theoremstyle{definition}
\newtheorem{algo}{Algorithm}
\newtheorem{condit}{Condition}
%\newtheorem{assump}{Assumption}
\theoremstyle{remark}
\newtheorem{rem}{Remark}
\DeclareMathOperator{\sMC}{sMC}
\DeclareMathOperator{\aMC}{aMC}
\DeclareMathOperator{\size}{size}
\DeclareMathOperator{\guile}{guile}
\DeclareMathOperator{\scale}{scale}
\DeclareMathOperator{\fix}{non}
\DeclareMathOperator{\up}{up}
\DeclareMathOperator{\lo}{lo}

\journal{Journal of Complexity}

\begin{document}

\begin{frontmatter}

\title{The Complexity of Deterministic Guaranteed Automatic Algorithms}
\author{Yuhan Ding}
\author{Nicholas Clancy}
\author{Caleb Hamilton}
\author{Fred J. Hickernell}
\author{Yizhi Zhang}
\address{Room E1-208, Department of Applied Mathematics, Illinois Institute of Technology,\\ 10 W.\ 32$^{\text{nd}}$ St., Chicago, IL 60616}
\begin{abstract} There are a quite a few automatic numerical algorithms that are widely used in practice.  By automatic, it is meant that the user provides an error tolerance, $\varepsilon$, and the algorithm attempts to provide an approximate solution that is within a distance of $\varepsilon$ of the true solution.  Furthermore, this solution is provided at a reasonable computational cost.  Unfortunately, many of these automatic algorithms lack rigorous guarantees, i.e., sufficient conditions on the input function that ensure that the algorithm is successful. 

This article establishes a framework for providing rigorous guarantees for automatic algorithms.  These algorithms adaptively determine the sample size necessary to guarantee that the error tolerance is met.  Upper bounds on their cost are provided (Theorem \ref{TwoStageDetermThm} and \ref{MultiStageThm}), and lower bounds on the complexity of the problem are derived (Theorem \ref{complowbd}). Examples of these general theorems are illustrated for univariate numerical integration and function recovery.  The key observation is that the error analysis should be done for \emph{cones} of input functions rather than balls. The existing literature contains certain cautions about the usefulness and reliability of automatic algorithms.  The theory presented here does not share the assumptions on which those concerns are based, rendering them moot.
\end{abstract}

\begin{keyword}
adaptive \sep cones \sep function recovery \sep integration \sep quadrature
%% keywords here, in the form: keyword \sep keyword

\MSC[2010] 65D05 \sep 65D30 \sep 65G20
%% MSC codes here, in the form: \MSC code \sep code
%% or \MSC[2008] code \sep code (2000 is the default)

\end{keyword}
\end{frontmatter}

\section{Introduction}
Function recovery and integration are two fundamental examples of numerical problems that arise often in practice.  It is desirable to have \emph{automatic} algorithms for solving these problems, i.e., the algorithm decides adaptively how many and which pieces of function data are needed and then uses those data to construct an approximate solution.  The error of this approximation should be guaranteed not to exceed the prescribed tolerance, $\varepsilon$, and the number of data required by the algorithm should be reasonable, given the assumptions made about the problem and the error tolerance required.

Most existing theory starts with a Banach space, $\cg$, of input functions defined on some set $\cx$, and having a semi-norm, $\norm[\cg]{\cdot}$.  The definition of $(\cg,\norm[\cg]{\cdot})$ contains assumptions about smoothness, periodicity or other qualities of the input functions.  The mathematical problem of interest is defined by a solution operator $S:\cg \to \ch$, where $\ch$ is some other Banach space with its norm $\norm[\ch]{\cdot}$.  For integration, $\ch=\reals$, and for function approximation, $\ch$ is some superset of $\cg$, for example, $\cl_{\infty}(\cx)$. One then shows that there exists an algorithm, $A$, that provides an approximate solution differing from the true solution by no  more than $\varepsilon$.  Furthermore, the number of function data needed, $\cost(A)$, is bounded in terms of $\varepsilon$, typically as follows:
\begin{equation} \label{traditionerr}
\sup_{\substack{f \in \cg\\ \norm[\cg]{f} \le \sigma}} \norm[\ch]{S(f)-A(f)} \le \varepsilon, \qquad \cost(A)\le C_{\up}\left(\frac{\sigma}{\varepsilon}\right)^{p}.
\end{equation}
Here it is necessarily assumed that the algorithm is exact if the semi-norm of the input function vanishes, i.e., $S(f)=A(f)$ if $\norm[\cg]{f}=0$. Algorithm $A$ is said to be optimal if any algorithm satisfying the error criterion above must have a cost of at least $\Order((\sigma/\varepsilon)^{p})$.

A practical drawback of the analysis summarized above is that the definition of the algorithm, at least as far as its cost is concerned, depends a priori on an upper bound on $\norm[\cg]{f}$. Specifically, this theory applies to functions in a \emph{ball}, $\cb_\sigma=\{ f \in \cg : \norm[\cg]{f} \le \sigma\}$, and the algorithm needs the radius, $\sigma$, as an input. Automatic algorithms try to estimate $\sigma$ so that the number of function data needed can be determined adaptively.  This is the approach taken here.  Unfortunately, there is a lack of theory to rigorously justify the estimate of $\sigma$ and the resulting adaptive algorithm.  This article develops a general framework for doing, and also provides two illustrative examples. The key is to look at functions in a \emph{cone} instead of a ball.

A simple example of the kind of practical problem addressed here is computing $\int_0^1 f(x) \, \dif x$ for all integrands whose second derivatives are absolutely integrable, i.e., $\norm[1]{f''}$.  The trapezoidal rule computes an approximation with error no greater than $\varepsilon$ using $\Order(\sqrt{\norm[1]{f''}/\varepsilon})$ function values.  However, except for relatively simple integrands this norm of the second derivative, $\norm[1]{f''}$, is not available.  After evaluating $f$, at a number of points in $[0,1]$, one may estimate the norm of the first derivative, $\norm[1]{f'}$, by the corresponding norm of the piecewise linear spline for $f$, and the error can be bounded rigorously in terms of $\norm[1]{f''}$.  If one assumes that $\norm[1]{f''}$ is not arbitrarily larger than $\norm[1]{f'}$ (a cone condition), then the reliable numerical estimate of $\norm[1]{f'}$ can be used as a surrogate for $\norm[1]{f''}$, and one can then reliable estimate the integral to the desired precision.  This example is discussed in detail in Section \ref{integsec}.

This article proceeds from the abstract to the concrete.  Section \ref{probdefsec} lays out the problems to be solved, including how to define the cost of an adaptive, automatic algorithm and the computational complexity of a problem.  The problem definition here allows for a maximum computational cost budget to be imposed.  Section \ref{genthmsec} describes the automatic algorithms in detail and provides proofs of their success for cones of input functions.  Their cost is derived in terms of an unknown, but estimated semi-norm of the input function.  Lower bounds on the complexity of the problem are derived using fooling functions.  Section \ref{integsec} illustrates these theorems for the univariate integration problem.  Section \ref{approxsec}  considers function approximation.  Common concerns about automatic and adaptive algorithms are answered in Section \ref{overcomesec}. The article ends with several suggestions for future work.

\section{General Problem Definition} \label{probdefsec}

\subsection{Problems and Algorithms} The function approximation, integration, or other problem to be solved is defined by a \emph{solution operator} $S:\cg \to \ch$, where $\cg$ is a Banach space of possible input functions defined on $\cx$ with semi-norm $\norm[\cg]{\cdot}$, and $\ch$ is some other Banach of possible outputs or solutions with norm $\norm[\ch]{\cdot}$. It is assumed that the $\cg_0 = \{f \in \cg :  \norm[\cg]{f}=0\}$, the subspace of $\cg$ containing functions with vanishing $\cg$-semi-norm, has finite dimension. The solution operator is assumed to have a scale property, i.e., 
\[
S(cf) = cS(f) \qquad \forall c\ge 0.
\]
Examples include the following:
\begin{align*}
\text{Integration:} \quad & S(f) = \int_{\cx} f(\vx) \, \rho(\vx) \, \dif \vx, \quad \rho \text{ is fixed,}\\
\text{Function Recovery:} \quad & S(f) = f, \\
\text{Poisson's Equation:} \quad & S(f) = u, \quad \text{where } \begin{array}{c} -\Delta u(\vx) = f(\vx), \ \vx \in \cx, \\ u(\vx)=0 \ \forall \vx \in \partial \cx, \text{ and}\end{array} \\
\text{Optimization:} \quad & S(f) = \min_{\vx \in \cx} f(\vx).
\end{align*}
The first three examples above are linear problems, which automatically satisfy the scale property for $S$, but so does the last example, which is a nonlinear problem.

The goal is to find an algorithm $A:\cg \to \ch$ for which $S(f) \approx A(f)$. Following the definition of algorithms described in \cite[Section 3.2]{TraWasWoz88}, the algorithm takes the form of some function of data derived from the input function:
\begin{equation}
\label{algoform}
A(f) =  \phi(\vL(f)), \quad \vL(f) = \left(L_1(f), \ldots, L_m(f)\right) \qquad \forall f \in \cg.
\end{equation}
Here the $L_i \in \Lambda$ are real-valued functions defined on $\cg$ with the following scale property:
\begin{equation}
\label{dataassump}
L(cf) = cL(f) \qquad \forall f \in \cg, \ c \in \reals, \ L \in \Lambda.
\end{equation}
One popular choice for $\Lambda$ is the set of all function values, $\Lambda^{\std}$, i.e., $L_i(f) = f(\vx_i)$ for some $\vx_i \in \cx$.  Another common choice is the set of all bounded linear functionals, $\Lambda^{\lin}$.  In general, $m$ may depend on $f$ and the choice of $L_i$ may depend on $L_1(f), \ldots, L_{i-1}(f)$.  In this article, all algorithms are assumed to be deterministic.  There is no random element.

\subsection{Non-Adaptive Algorithms}

The set $\ca_{\fix}(\cg,\ch,S,\Lambda)$ contains algorithms as just described for which the choice of the $L_i$ and the number of function data used by the algorithm, $m$, are both assumed to be independent of the input function, i.e., these algorithms are non-adaptive.  Furthermore, any $A \in \ca_{\fix}(\cg,\ch,S,\Lambda)$ is assumed to satisfy the following scale properties
\begin{equation}
\label{algoscale}
\vL(cf) = c \vL(f), \quad 
\phi(c\vy) = c\phi(\vy), \quad A(cf) = cA(f) \qquad \forall c \ge 0, \ \vy \in \reals^m.
\end{equation}

The cost of a non-adaptive algorithm, $A \in  \ca_{\fix}(\cg,\ch,S,\Lambda)$, is fixed and is defined as the sum of the costs of all the function data:
\begin{equation} \label{costfix}
\cost(A) = \$(\vL) = \$(L_1) + \cdots +\$(L_m),
\end{equation}
where $\$:\Lambda \to (0,\infty)$, and $\$(L)$ is the cost of acquiring the datum $L(f)$. The cost of $L$ may be the same for all $L \in \Lambda$.  Alternatively, it might be a vary with the choice of $L$.  E.g., if $f$ is a function of the infinite sequence real numbers, $(x_1, x_2, \ldots)$, the cost of evaluating the function with arbitrary values of the first $d$ coordinates, $L(f)=f(x_1, \ldots, x_d, 0, \ldots)$, might be $d$.  This cost model has been used by \cite{HicMGRitNiu09a,KuoEtal10a,NiuHic09a,NiuHic09b,PlaWas11a} for integration problems and \cite{Was13a,WasWoz11a,WasWoz11b} for function approximation problems.

The error of a non-adaptive algorithm $A  \in \ca_{\fix}(\cg,\ch,S,\Lambda)$ is defined  as
\begin{equation} \label{errdefworst}
\err(A,\cg,\ch,S)
= \min\{ \delta \ge 0 : \norm[\ch]{S(f) -  A(f)} \le \delta \norm[\cg]{f} \ \forall f \in \cg \},
\end{equation}
When the problem has real-valued solutions, i.e., $\ch=\reals$, one may also define a one sided error criterion:
\begin{equation}\label{errpmdefworst}
\err_{\pm}(A,\cg,\reals,S) = 
\min \{ \delta \ge 0 : \pm[S(f) -  A(f)] \le \delta \norm[\cg]{f} \ \forall f \in \cg \} 
\end{equation}
Since $\norm[\cg]{\cdot}$ may be a semi-norm, but not a norm, a finite error in any of the above definitions assumes that the algorithm is exact, i.e., $S(f)=A(f)$, for all $f$ with $\norm[\cg]{f}=0$.

The above error criteria are normalized, meaning that the absolute error, $\norm[\ch]{S(f) -  A(f)}$ is measured with respect to the $\cg$-semi-norm of the input function. The complexity of a problem for this set of algorithms, $\ca_{\fix}(\cg,\ch,S,\Lambda)$, is defined as the cost of the cheapest algorithm that satisfies the specified error tolerance, $\varepsilon$:
\begin{multline} \label{fixcostcomplex}
\comp(\varepsilon,\ca_{\fix}(\cg,\ch,S,\Lambda)) \\
= \inf\left\{\cost(A) : \err(A,\cg,\ch,S) \le \varepsilon, \ A \in \ca_{\fix}(\cg,\ch,S,\Lambda) \right \}.
\end{multline}
Here the infimum of an empty set is defined to be $\infty$.  This means that to guarantee that $\norm[\ch]{S(f) -  A(f)} \le \varepsilon$, one needs an algorithm with a cost of at least 
\[
\comp(\varepsilon/\norm[\cg]{f},\ca_{\fix}(\cg,\ch,S,\Lambda)).
\]
This cost does not decrease as either $\varepsilon$ decreases or $\norm[\cg]{f}$ increases.

Suppose that there is a sequence of nonadaptive algorithms indexed by their cost, and which converge to the true answer:
\begin{subequations} \label{algseqdef}
\begin{gather} 
\{A_n\}_{n \in \ci}, \qquad A_n  \in \ca_{\fix}(\cg,\ch,S,\Lambda), \\
\lim_{\substack{n \to \infty\\ n \in \ci}} \err(A_n,\cg,\ch,S) = 0, \qquad \cost(A_n) = n,  
\end{gather}
\end{subequations}
where the countable, non-negative-valued index set, 
\begin{equation} \label{indexdef}
\ci=\{n_1, n_2, \ldots\} \quad \text{with } n_i < n_{i+1}, \quad \text{satisfies } \sup_i \frac{n_{i+1}}{n_i} <\infty. 
\end{equation} 
This sequence of algorithms is called \emph{nearly optimal} for the problem $(\cg,\ch,S,\Lambda)$ if it essentially tracks the minimum cost algorithms, namely,
\begin{equation} \label{nearoptdef}
\sup_{0 < \varepsilon \le 1} \frac{\min\{n \in \ci : \err(A_n,\cg,\ch,S) \le \varepsilon\} \varepsilon^p}{\comp(\varepsilon,\ca_{\fix}(\cg,\ch,S,\Lambda))} <\infty, \qquad \forall p>0.
\end{equation}
The sequence is called optimal if above inequality holds fo $p=0$.  A nearly optimal sequence of algorithms may differ in its cost from an optimal algorithm by powers of $\log(\varepsilon^{-1})$, for example.


\subsection{Automatic, Adaptive Algorithms}

Non-adaptive algorithms, $A \in \ca_{\fix}(\cg,\ch,S,\Lambda)$ need an upper bound on $\norm[\cg]{f}$ to guarantee that they meet the prescribed error tolerance for the input function $f$.  Automatic algorithms attempt to estimate $\norm[\cg]{f}$ and then determine the number of function data needed to meet the error tolerance.  Such automatic, adaptive algorithms are now defined, somewhat differently from the non-adaptive algorithms above.  However, in practice automatic algorithms use non-adaptive algorithms as building blocks.

Practical automatic algorithms in $\ca(\cg,\ch,S,\Lambda)$ take the form of ordered pairs of functions
\[
(A,W): \cg \times (0,\infty)\times (0,\infty] \to \ch \times \{\text{false},\text{true}\},
\]
for which $S(f) \approx A(f;\varepsilon,N_{\max})$.  Here $\varepsilon \in (0,\infty)$ is a user-supplied error tolerance, $N_{\max} \in (0,\infty]$ is a user-supplied maximum cost budget, and $W(f;\varepsilon,N_{\max})$ is a Boolean warning flag that is false if the algorithm completed its calculations without attempting to exceed the cost budget, and is true otherwise.  

As in \eqref{algoform}, the algorithm takes the form of some function of function data: $A(f;\varepsilon,N_{\max}) = \phi\left(\vL(f);\varepsilon,N_{\max}\right)$.
Now, however, the algorithm is allowed to be adaptive. The choice of $L_2$ may depend on the value of $L_1(f)$, the choice of $L_3$ may depend on $L_1(f)$ and $L_2(f)$, etc.  The number of function data used by the algorithm, $m$, may also be determined adaptively. The choice of how many and which function data to use depends on $\varepsilon$ and $N_{\max}$.  Thus, $\vL(c\vy)$ might not equal $c\vL(\vy)$ since the length of the information vector depends on the data recorded.  The goal of the algorithm is to make $\norm[\ch]{S(f) - A(f;\varepsilon,N_{\max})} \le \varepsilon$, but this is not a requirement of the definition.

The cost of the algorithm for a specified input function is defined analogously to \eqref{costfix} as the sum of the costs of all function data.
\[
\cost(A,f;\varepsilon,N_{\max}) = \$(\vL) = \$(L_1) + \cdots +\$(L_m).
\]
Because of the potentially adaptive nature of the algorithm, namely, that $m$ may depend on $f$, it follows that the cost may depend on $f$ as well as $A$. The input parameter $N_{\max}$ tells the algorithm to ensure that $\cost(A,f;\varepsilon,N_{\max}) \le N_{\max}$ for all $f$ and $\varepsilon$.  This is a practical consideration since the user does not want to wait indefinitely for an answer.  

The cost of the algorithm is expected to scale with the $\cg$-semi-norm of the integrand.  This means that the cost of an algorithm generally increases as $\norm[\cg]{f}$ increases.  The cost of the algorithm may also depend on $\cn$, the subset of $\cg$ where the functions of interest lie.  To represent this idea one defines
\begin{equation*}
\cost(A,\cn,\varepsilon,N_{\max},\sigma)
= \sup \{ \cost(A,f;\varepsilon,N_{\max}) : f \in \cn, \ \norm[\cg]{f} \le \sigma \} .
\end{equation*}
Here the set $\cn$ is allowed to depend on the algorithm inputs, $\varepsilon$ and $N_{\max}$, but not on the unknown $\sigma$.

For automatic algorithms, returning an approximation with the desired error is not enough.  One also wants the algorithm to be confident that the answer is correct.  A successful algorithm for $\cn \subseteq \cg$, denoted $(A,W) \in \ca(\cn,\ch,S,\Lambda)$, is one that meets the prescribed error tolerance and does not raise the warning flag.  Specifically, success is defined as
\begin{multline*}
\success(A,W,\cn,\varepsilon,N_{\max}) \\
= \begin{cases} \text{true} & \text{if } \displaystyle \norm[\ch]{S(f)-A(f;\varepsilon,N_{\max})} \le \varepsilon \ \& \ W(f;\varepsilon,N_{\max})=\text{false} \quad \forall  f \in \cn, \\
\displaystyle \text{false} & \text{otherwise}.
\end{cases}
\end{multline*}
The above are absolute error criteria for success.  One might also define relative error criteria instead, but finding successful algorithms for relative error is a non-trivial exercise and will be considered in future work.

The complexity of a problem is defined as the cost of the cheapest successful algorithm with $\cg$-semi-norm no greater than $\sigma$:
\begin{multline} \label{complexdef}
\comp(\varepsilon,\ca(\cn,\ch,S),\Lambda,N_{\max},\sigma) \\
 = \inf\left\{\cost(A,\cn,\varepsilon,N_{\max},\sigma) : \success(A,W,\cn,\varepsilon,N_{\max}) = \text{true}, \right .\\
\left.  (A,W) \in \ca(\cn,\ch,S,\Lambda) \right \}.
\end{multline}
Here the infimum of an empty set is defined to be $\infty$.

The set of non-adaptive algorithms, $\ca_{\fix}(\cg,\ch,S,\Lambda)$, defined in the previous subsection is a subset of the automatic algorithms $\ca(\cg,\ch,S,\Lambda)$.  Algorithms in $\ca_{\fix}(\cg,\ch,S,\Lambda)$ are not affected by the error tolerance $\varepsilon$ and do not recognize a cost budget $N_{\max}$.  Moreover, the warning flag for an algorithm in $\ca_{\fix}(\cg,\ch,S,\Lambda)$ is always returned as false.  Whereas the non-adaptive algorithms are inherently impractical by themselves, they are vital components of automatic, adaptive algorithms.

\subsection{Cones of Functions} \label{conesubsec} All algorithms can be fooled by some input functions, even if these functions are sufficiently smooth.  Error analysis such as that outlined in \eqref{traditionerr} rules out fooling functions with large error by restricting the size of  $\norm[\cg]{f}$.  

As mentioned above, it is often difficult to know how large $\norm[\cg]{f}$ is a priori and so practical automatic algorithms try to estimate it.  The framework described here rules out fooling functions whose $\cg$-semi-norms cannot be estimated reliably.  This is done by considering $\cf$, a subspace of $\cg$, with its own semi-norm $\norm[\cf]{\cdot}$.   The semi-norm $\norm[\cf]{\cdot}$ is considered to be stronger than $\norm[\cg]{\cdot}$ in the following sense:
\begin{subequations} \label{Fspacecond}
\begin{equation} \label{Fspacecondstrong}
\min_{f_0 \in \cf_0} \norm[\cg]{f - f_0} \le C_{\cf} \norm[\cf]{f} \qquad \forall f \in \cf,
\end{equation}
where $\cf_0=\{f \in \cf : \norm[\cf]{f}=0\}$ is a finite dimensional subspace of $\cf$.  Moreover, it is assumed that any $f \in \cg$ with zero $\cg$-semi-norm must also lie in $\cf$ and have zero $\cf$-semi-norm:
\begin{equation} \label{Fspacecondzero}
\cg_0 \subseteq \cf_0.
\end{equation}
\end{subequations}

Given $\tau>0$, let $\cc_{\tau} \subset \cf$ denote a \emph{cone} of functions whose $\cf$-semi-norms are no greater than $\tau$ times their $\cg$-semi-norms:
\begin{equation} \label{conedef}
\cc_{\tau}=\{f \in \cf : \norm[\cf]{f} \le \tau \norm[\cg]{f} \}.
\end{equation}
For any $f \in \cc_{\tau}$ both $\norm[\cf]{f}$ and $\norm[\cg]{f}$ must be finite, but they can be arbitrarily large.  There is no need to assume an upper bound on their sizes, but it is possible to obtain reliable upper bounds for both $\norm[\cf]{f}$ and $\norm[\cg]{f}$ by sampling $f$.  An upper bound on $\norm[\cg]{f}$ for $f \in \cc_{\tau}$ can be computed in terms of the data $\vL(f)=(L_1(f), \ldots, L_m(f))$ because $\norm[\cf]{\cdot}$ is a stronger semi-norm than $\norm[\cg]{\cdot}$ in the sense of \eqref{Fspacecondstrong}, and because $\norm[\cf]{f}$ is no larger than a multiple of $\norm[\cg]{f}$ (see Lemma \ref{Gnormlem} below). This upper bound on $\norm[\cg]{f}$ then automatically implies an upper bound on $\norm[\cf]{f}$ from the definition of the cone. These reliable bounds on both $\norm[\cf]{f}$ and $\norm[\cg]{f}$ may be used to obtain a bound on the error of the algorithm for estimating $S(f)$  (see Theorem \ref{TwoStageDetermThm} below).

\subsection{Results that One Wishes to Prove}  The previous subsections define the problem to be approximated and the notation describing the difficulty of the problem and the efficiency of the algorithms.  This subsection summarizes the results that are proved in general in the next section and illustrated for specific cases in the following sections.

\begin{enumerate}

\renewcommand{\labelenumi}{\roman{enumi}.}

\item \emph{Upper bound on the complexity.}
One wishes to bound the complexity of solving the problem successfully, $\comp(\cn,\varepsilon,N_{\max},\sigma,\Lambda)$, in terms of some power of $\varepsilon/\sigma$ for $\cn$ suitably defined as a subset of the cone $\cc_{\tau}$.  This is done in Theorem \ref{TwoStageDetermThm}.

\item \emph{An algorithm that achieves the upper bound.}  Upper bounds on the complexity are sometimes found in a non-constructive way.  However, it is desirable to identify an explicit successful algorithm, $(A,W) \in \ca(\cn,\ch,S,\Lambda)$, that achieves these upper bounds.  This is also done in Theorem \ref{TwoStageDetermThm}.

\item \emph{Penalty for not knowing the $\cf$- and $\cg$-semi-norms of $f$.} The optimal successful algorithm must find an upper bound on $\norm[\cf]{f}$ or $\norm[\cg]{f}$ rather than assuming such an upper bound.  One hopes that the extra cost relative to the situation of knowing a priori bounds on these semi-norms is not too great.  A positive result is shown in Theorem \ref{TwoStageDetermThm}.

\item \emph{Lower bound on the complexity.}  The difficulty of the problem is provided by lower bounds on the complexity.  These are given in Theorem \ref{complowbd}.

\end{enumerate}

\section{General Theorems} \label{genthmsec}

This section provides rather general theorems about the complexity of automatic algorithms.  In some sense, these theorems are roadmap or an outline because their assumptions are non-trivial and take effort to be be verified for specific problems of interest.  On the other hand, the assumptions are reasonable as is demonstrated in the later sections where concrete cases are are discussed.  

\subsection{Bounding the $\cg$-Semi-Norm}

As mentioned in Section \ref{conesubsec} automatic algorithms require a reliable upper bound on $\norm[\cg]{f}$ for all $f$ in the cone $\cc_{\tau}$. This can be obtained using a non-adaptive algorithm $G \in \ca_{\fix}(\cf,\reals_+,\norm[\cg]{\cdot},\Lambda)$, provided that one has error bounds, $\err_{\pm}(G,\cf,\reals_+,\norm[\cg]{\cdot})$, as defined in $\eqref{errpmdefworst}$.  These upper and lower error bounds imply 
\begin{multline*}
-\err_{-}(G,\cf,\reals_{+},\norm[\cg]{\cdot}) \norm[\cf]{f} \le \norm[\cg]{f}-G(f) \le \err_{+}(G,\cf,\reals_{+},\norm[\cg]{\cdot}) \norm[\cf]{f} \\ \forall f \in \cf.
\end{multline*}
Noting that $\norm[\cf]{f} \le \tau \norm[\cg]{f}$ for all $f$ in the cone $\cc_{\tau}$ implies the lemma below. 

\begin{lem} \label{Gnormlem} Any nonadaptive algorithm $G_n \in \ca_{\fix}(\cf,\reals_+,\norm[\cg]{\cdot},\Lambda)$ with cost $n=\cost(G_n)$ yields an approximation to the $\cg$-semi-norm of functions in the cone $\cc_{\tau}$ with the following upper and lower error bounds:
\begin{equation} \label{twosidedGineq}
\frac{G_n(f)}{\fc_n} \le \norm[\cg]{f} \le \fC_n G_n(f) \qquad \forall f \in \cc_{\tau}.
\end{equation}
where the deflation factor, $\fc_n$, and the inflation factor, $\fC_n$ are defined as follows:
\begin{gather} \label{normdeflate}
\fc_n =1 + \tau \err_{-}(G_n,\cf,\reals_+,\norm[\cg]{\cdot})  \ge 1 \\
\label{norminflate}
\fC_n =\frac{1}{1 - \tau \err_{+}(G_n,\cf,\reals_+,\norm[\cg]{\cdot})} \ge 1.
\end{gather}
The upper bound assumes that  
\begin{equation} \label{Gtaucond}
\err_{+}(G_n,\cf,\reals,\norm[\cg]{\cdot}) < 1/\tau.
\end{equation}
\end{lem}

\subsection{Two-Stage, Automatic Algorithms}

Computing an approximate solution to the problem $S: \cc_{\tau} \to \ch$, e.g., integration or function approximation, depends on non-adaptive algorithms. Suppose that there is a sequence of such of algorithms, $\{A_n\}_{n \in \ci}$, with $A_n  \in \ca_{\fix}(\cg,\ch,S,\Lambda)$, indexed by their cost as defined in \eqref{algseqdef}, and for which upper error bounds are known for both the spaces $\cg$ and $\cf$:
\begin{equation}\label{algseqerrbd}
%\err(A_n,\cg,\ch,S) \le C_{1} n^{-p_1}, \qquad \err(A_n,\cf,\ch,S) \le C_{2} n^{-p_2}, 
\err(A_n,\cg,\ch,S) \le h(n), \qquad \err(A_n,\cf,\ch,S) \le \tildeh(n), 
\end{equation}
for some \emph{non-increasing} functions $h$ and $\tildeh$.  The definitions of these errors in \eqref{errdefworst} then implies upper bounds on the error of $A_n(f)$ in terms of the $\cg$-semi-norm of $f$:
\begin{align} \nonumber
\norm[\ch]{S(f) -  A_n(f)} &\le \min(\err(A_n,\cg,\ch,S)\norm[\cg]{f},\err(A_n,\cf,\ch,S)\norm[\cf]{f}) \\
\label{Anerrbound}
&\le \min(h(n),\tau \tildeh(n))\norm[\cg]{f} \qquad \forall f \in \cc_{\tau}.
\end{align}

\begin{algo} \label{twostagedetalgo} {\bf (Automatic, Adaptive, Two-Stage).} Let $\cf$, $\cg$, and $\ch$ be Banach spaces as described above, let $S$ be the solution operator, let $\varepsilon$ be a positive error tolerance, and let $N_{\max}$ be the maximum cost allowed.  Let $\tau$ be a fixed positive number, and let $G_{n_G} \in \ca_{\fix}(\cf,\reals_+,\norm[\cg]{\cdot},\Lambda)$ be an algorithm as described in Lemma \ref{Gnormlem} with cost $n_G$ and satisfying  \eqref{Gtaucond} for $n=n_G$.
Moreover, let  $\{A_n\}_{n \in \ci}$, $A_n  \in \ca_{\fix}(\cg,\ch,S,\Lambda)$, be a sequence of algorithms as described in  \eqref{algseqdef} and \eqref{algseqerrbd}.  Given an input function $f$, do the following:

\begin{description} 

\item[Stage 1.\ Estimate {$\norm[\cg]{f}$}.] First compute $G_{n_G}(f)$.  Define the inflation factor $\fC=\fC_{n_G}$ according to \eqref{norminflate}.
Then $\fC G_{n_G}(f)$ provides a reliable upper bound on $\norm[\cg]{f}$.  

\item [Stage 2.\ Estimate {$S(f)$}.] Choose the sample size need to approximate $S(f)$, namely, $n_A=N_{A}(\varepsilon/(\fC G_{n_G}(f)))$, where 
\begin{equation} \label{Nmindef}
N_{A}(a)= \min\left\{ n \in \ci : \min(h(n),\tau \tildeh(n)) \le a \right\}, \quad a \in (0,\infty).
\end{equation}
If $n_A \le N_{\max}-n_G$, then $S(f)$ may be approximated within the desired error tolerance and within the cost budget.  Set the warning flag, $W$, to false. Otherwise, recompute $n_A$ to be within budget, $n_A = \tN_{\max} := \max\{n \in \ci : n\le N_{\max} -  n_G\}$, and set the warning flag, $W$ to true.  Compute $A_{n_A}(f)$ as the approximation to $S(f)$.
\end{description}

Return the result $(A_{n_A}(f),W)$, at a total cost of $n_G+n_A$.  
\end{algo}



\begin{theorem}  \label{TwoStageDetermThm}  Let  $\cf$, $\cg$, $\ch$, $\varepsilon$, $N_{\max}$, $\tN_{\max}$, $\fC$, and $\tau$ be given as described in Algorithm \ref{twostagedetalgo}, and assume that $\cf$ satisfies \eqref{Fspacecond}.  Let $\fc=\fc_{n_G}$ be defined as in \eqref{normdeflate}.
Let $\cc_\tau$ be the cone of functions defined in \eqref{conedef} whose $\cf$-semi-norms are no larger than $\tau$ times their $\cg$-semi-norms.  Let
\begin{align} 
\nonumber
\cn &= \left \{ f \in \cc_\tau : N_{A}\left(\frac{\varepsilon}{\fC \fc \norm[\cg]{f}} \right) \le \tN_{\max} \right\} \\
\label{nicefdef}
&= \left \{ f \in \cc_\tau : \norm[\cg]{f} \le \frac{\varepsilon}{\fC \fc \min(h(\tN_{\max}),\tau \tildeh(\tN_{\max}))} \right\}
\end{align}
be a subset of the cone $\cc_\tau$ that lies inside a $\cg$-semi-norm ball of rather large radius (since $\min(h(\tN_{\max}),\tau \tildeh(\tN_{\max}))$ is assumed to be tiny).  Then it follows that Algorithm \ref{twostagedetalgo} is successful for all functions in this set of \emph{nice} functions $\cn$,  i.e.,  $\success(A,W,\cn,\varepsilon,N_{\max}) = 1$.  Moreover, the cost of this algorithm is bounded above in terms of the $\cg$-semi-norm of the input function as follows:
\begin{equation} \label{auto2stagedetcost}
\cost(A,\cn,\varepsilon,N_{\max},\sigma)
\le n_G+ N_{A}\left(\frac{\varepsilon}{\fC \fc \sigma} \right).
\end{equation}
The upper bound on the cost of this specific algorithm provides an upper bound on the complexity of the problem, $\comp(\varepsilon,\ca(\cn,\ch,S,\Lambda),N_{\max},\sigma)$.  

Consider the limit of infinite cost budget, i.e., $N_{\max} \to \infty$.  If the sequence of algorithms $\{A_n\}_{n \in \ci}$, $A_n \in\ca_{\fix}(\cg,\ch,S,\Lambda)$  is nearly optimal for the problems $(\cg,\ch,S,\Lambda)$ and $(\cf,\ch,S,\Lambda)$ as defined in \eqref{nearoptdef}, then Algorithm \ref{twostagedetalgo} does not incur a significant penalty for not knowing $\norm[\cg]{f}$ a priori, i.e., for all $p>0$,
\begin{equation} \label{penalty}
\sup_{0 < \varepsilon/\sigma \le 1} \frac{\cost(A,\cc_{\tau},\varepsilon,\infty,\sigma)} {\comp(\varepsilon/\sigma,\ca_{\fix}(\cj,\ch,S,\Lambda))} \left(\frac{\varepsilon}{\sigma}\right)^p <\infty, \qquad \cj \in \{\cf,\cg\}.
\end{equation}
\end{theorem}

\begin{proof} The definition of $\fC$ in \eqref{norminflate} implies that the true $\cg$-semi-norm of $f$ is bounded above by $\fC G_{n_G}(f)$ according to Lemma \ref{Gnormlem}.  The upper bound on the error of the sequence of algorithms $\{A_n\}_{n \in \ci}$ in \eqref{Anerrbound} then implies that 
\[
\norm[\ch]{S(f) -  A_n(f)} \le \min(h(n),\tau \tildeh(n)) \fC G_{n_G}(f) \qquad \forall f \in \cc_{\tau}.
\]
This error upper bound may be made no greater than the error tolerance, $\varepsilon$, by choosing the algorithm cost, $n$, to satisfy the condition in Stage 2. of Algorithm \ref{twostagedetalgo}, provided that this can be done within the maximum cost budget.  In this case, the algorithm is successful, as claimed in the theorem.

To ensure that the algorithm does not attempt to overrun the cost budget, one must limit the $\cg$-semi-norm of the input function.  The definition of  $\fc$ in \eqref{normdeflate} implies that $G_{n_G}(f) \le \fc\norm[\cg]{f}$ according to Lemma \ref{Gnormlem}. This means that for any function, $f$, with actual $\cg$-semi-norm $\sigma=\norm[\cg]{f}$, the upper bound on its $\cg$-semi-norm computed via Lemma \ref{Gnormlem} is no greater than $\fC \fc \sigma$.  Thus, after using $N_G$ samples to estimate $\norm[\cg]{f}$, functions in $\cn$ as defined in \eqref{nicefdef} never need more than $N_{\max} - n_G$ additional samples to estimate $S(f)$ with the desired accuracy.  This establishes that Algorithm \ref{twostagedetalgo} must be successful for all $f \in \cn$.  It furthermore establishes an upper bound on the cost of the algorithm as given in \eqref{auto2stagedetcost}.

Now consider the penalty for not knowing $\norm[\cg]{f}$ in advance.  If the sequence of nonadaptive algorithms, $\{A_n\}_{n \in \ci}$, used to construct Algorithm \ref{twostagedetalgo} are nearly optimal for solving the problem on both $\cf$ and $\cg$, as defined in \eqref{nearoptdef}, then it follows that for $\cj \in \{\cf,\cg\}$,
\begin{multline*}
\sup_{0 < \varepsilon/\sigma \le 1} \frac{\cost(A,\cn,\varepsilon,\infty,\sigma)} {\comp(\varepsilon/\sigma,\ca_{\fix}(\cj,\ch,S,\Lambda))} \left(\frac{\varepsilon}{\sigma}\right)^{p} \\
= \sup_{0 < \varepsilon/\sigma \le 1} \frac{\cost(A,\cn,\varepsilon,\infty,\sigma)}{\min\{n \in \ci : \err(A_n,\cj,\ch,S) \le \varepsilon/\sigma\}} \left(\frac{\varepsilon}{\sigma}\right)^{p/2}  \\
 \times \sup_{0 < \varepsilon/\sigma \le 1} \frac{\min\{n \in \ci : \err(A_n,\cj,\ch,S) \le \varepsilon/\sigma\}} {\comp(\varepsilon/\sigma,\ca_{\fix}(\cj,\ch,S,\Lambda))} \left(\frac{\varepsilon}{\sigma}\right)^{p/2}
\end{multline*} 
The first of these suprema is finite by comparing the convergence rates of the sequence algorithms, $\{A_n\}_{n \in \ci}$, in \eqref{algseqerrbd} with the cost of the automatic algorithm given by \eqref{auto2stagedetcost}. The second of these suprema is finite for all $p>0$ by the near optimality of $\{A_n\}_{n \in \ci}$.  
\end{proof}

There are a several remarks that may facilitate understanding of this result.

\begin{rem} There are three main conditions to be checked for this theorem to hold.
\begin{enumerate}
\renewcommand{\labelenumi}{\roman{enumi}.}
\item An algorithm, $G$, to approximate the semi-norm in the larger space,  $\norm[\cg]{\cdot}$,  must be identified and its error must be bounded.
\item Both the error functions $h$ and $\tildeh$, for the sequence of nonadaptive algorithms, $\{A_n\}_{n \in \ci}$, must be computed explicitly for the the automatic algorithm to be defined.  
\item The near optimality of this sequence of nonadaptive algorithms must be verified to ensure that there is no significant penalty for not having an a priori upper bound on $\norm[\cg]{f}$.
\end{enumerate}
Sections \ref{integsec} and \ref{approxsec} provide concrete examples where these conditions are checked.
\end{rem}

\begin{rem} If $\tildeh$ is unknown, then one may take $\tildeh=\infty$, and the algorithm still satisfies the error tolerance with a cost upper bound given in \eqref{auto2stagedetcost}.  The optimality result in \eqref{penalty} then only holds for $\cg$, and not $\cf$.  The analogy holds if $h$ is unknown.  However, at least one of these two functions $h$ or $\tildeh$, must be known for this theorem to have a meaningful result.
\end{rem}

\begin{rem} The cost of Algorithm \ref{twostagedetalgo}, as given by \eqref{auto2stagedetcost}, depends on $\sigma$, which is essentially the $\cg$-semi-norm of the input function, $f$.  Thus, if $\sigma$ is smaller, the cost will correspondingly be smaller.  Moreover, $\sigma$ is not an input parameter for the algorithm.  Rather the algorithm reliably estimates $\norm[\cg]{f}$, and then adjusts the number of samples used (the cost) accordingly.
\end{rem}

\begin{rem}
The definition of the set of algorithms for which the Algorithm \ref{twostagedetalgo} is guaranteed to work, $\cn$, depends somewhat on $\norm[\cg]{f}$, but only because of the practical constraint of a cost budget of $N_{\max}$.  This dependence disappears if one lifts this constraint by taking $N_{\max} \to \infty$.  The primary constraint determining the success of the algorithm is that $f \in \cc_{\tau}$.
\end{rem}

\begin{rem} Instead of choosing $\tau$ as an input parameter for Algorithm \ref{twostagedetalgo}, one may alternatively choose the inflation factor $\fC >1$.  This then implies that 
\begin{equation} \label{taufromnC}
\tau = \left(1 - \frac{1}{\fC}\right)\frac{1}{\err_{+}(G_{n_G},\cf,\reals_+,\norm[\cg]{\cdot})},
\end{equation}
which is equivalent to \eqref{norminflate}.
\end{rem}

\begin{rem} It is observed in the examples of Sections \ref{integsec} and \ref{approxsec} that for the sequence of algorithms $\{A_n\}_{n \in \ci}$
\begin{equation}
\tau \tildeh(n) \le h(n) \quad \forall n \in \ci.
\end{equation}
or equivalently, $\min(h(n),\tau \tildeh(n))=\tau\tildeh(n)$.  This then simplifies Algorithm \ref{twostagedetalgo} in the computation of the sample size for $A$ in \eqref{Nmindef} and also in Theorem \ref{TwoStageDetermThm} in the definition of $\cn$ in \eqref{nicefdef} and the upper bound on the cost in \eqref{auto2stagedetcost}. 
\end{rem}


\subsection{Embedded Nonadaptive Algorithms $\{A_n\}_{n \in \ci}$}

Suppose that the sequence of nonadaptive algorithms, 
\[
\{A_n\}_{n \in \ci} = \{A_{n_1}, A_{n_2}, \ldots \}, 
\]
are \emph{embedded}, i.e., $A_{n_{i+1}}$ uses all of the data used by $A_{n_{i}}$ for $i=1, 2, \ldots$.  An example would be a sequence of composite trapezoidal rules for integration that uses a number of trapezoids that is an increasing power of two. Furthermore, it is supposed that the data used by $G$, the algorithm used to estimate the $\cg$-semi-norm of $f$, is the same data used by $A_{n_1}$, and so $n_1=n_G$.  Then the total cost of Algorithm \ref{twostagedetalgo} can be reduced; it is simply $n_A$, as given in Stage 2, instead of $n_G+n_A$.  Moreover, $\tN_{\max}$ may then be taken to be $N_{\max}$, and the cost bound of the automatic algorithm in \eqref{auto2stagedetcost} does not need the term $n_G$.

Again suppose that $\{A_n\}_{n \in \ci}$, $A_n  \in \ca_{\fix}(\cg,\ch,S,\Lambda)$, consists of algorithms as described in  \eqref{algseqdef} and \eqref{algseqerrbd}, but some of which are embedded in others.  Specifically, suppose that for some fixed $r > 1$, for all $n \in \ci$, there exists at least one $\tn \in \ci$ with $n < \tn \le rn$, such that the data for $A_n$ is embedded in the data for $A_{\tn}$.  An example would be all possible composite trapezoidal rules for integration that use trapezoids of equal widths. Suppose also that there exists a sequence of algorithms for approximating the $\cg$-sem-inorm, $\{G_n\}_{n \in \ci}$, $G_n  \in \ca_{\fix}(\cg,\reals_+,\norm[\cg]{\cdot},\Lambda)$, such that for each $n \in \ci$, $A_n$ and $G_n$ use exactly the same data. Moreover, define $\fc_n$ and $\fC_n$ in terms of $G_n$ as in \eqref{normdeflate} and \eqref{norminflate} for all $n \in \ci$.
It is assumed that $\err_{\pm}(G_n,\cf,\reals_+,\norm[\cg]{\cdot})$ are non-increasing functions of $n$, which implies that $\fC_{n}$ and $\fc_n$ do not increase as $n$ increases. These embedded algorithms suggest the following iterative algorithm.

\begin{algo} \label{multistagealgo}  Let the Banach spaces $\cf$, $\cg$, and $\ch$, the solution operator $S$, and the error tolerance $\varepsilon$, the maximum cost budget $N_{\max}$, and the positive constant $\tau$ be as described in Algorithm \ref{twostagedetalgo}. Let the sequences of algorithms, $\{A_n\}_{n \in \ci}$ and  $\{G_n\}_{n \in \ci}$ be as described above.  Set $i=1$.  Let $n_1$ be the smallest number in $n \in \ci$ satisfying $\err_+(G_n,\cf,\norm[\cg]{\cdot},\reals_+) \le 1/\tau$. For any input function $f \in \cf$, do the following:
\begin{description}

\item [Stage 1. Estimate $\| f \| _{\cg}$.] Compute $G_{n_i}(f)$ and $\fC_{n_i} \le \infty$ as defined in \eqref{norminflate}.  

\item [Stage 2. Check for Convergence.] Check whether $n_i$ is large enough to satisfy the error tolerance, i.e., 
\begin{equation} \label{multistageconv}
\min(h(n_i),\tau \tildeh(n_i))\fC_{n_i} G_{n_i}(f) \le \varepsilon.
\end{equation}
If this is true, then set $W$ to be false, return $(A_{n_i}(f),W)$ and terminate the algorithm.

\item[Stage 3. Compute $n_{i+1}$.]  Otherwise, if the inequality above fails to hold, compute $\fc_{n_i}$ according to \eqref{normdeflate} using $G_{n_i}$. Choose $n_{i+1}$ as the smallest number exceeding $n_i$ and not less than $N_{A}(\varepsilon \fc_{n_i}/G_{n_i}(f))$ such that $A_{n_{i}}$ is embedded in $A_{n_{i+1}}$. If $n_{i+1} \le N_{\max}$, increment $i$ by $1$, and return to Stage 1.  

Otherwise, if $n_{i+1} > N_{\max}$, choose $n_{i+1}$ to be the largest number not exceeding $N_{\max}$ such that $A_{n_{i}}$ is embedded in $A_{n_{i+1}}$, and set $W$ to be true. Return $(A_{n_{i+1}}(f),W)$ and terminate the algorithm.
\end{description}  
\end{algo}

This iterative algorithm is guaranteed to converge also, and its cost can be bounded.  The following theorem is analogous to Theorem \ref{TwoStageDetermThm}.

\begin{theorem}  \label{MultiStageThm}  Let  $\cf$, $\cg$, $\ch$, $\varepsilon$, $N_{\max}$, $\tau$, and $n_1$ be given as described in Algorithm \ref{multistagealgo}. Assume that $n_1 \le N_{\max}$. Let $r$ be the number described in the paragraph preceding that algorithm.  Define 
\[
\tN_A(a) = \min\left\{ n \in \ci : \min(h(n),\tau \tildeh(n))\fC_n\fc_n \le a \right\}, \quad a \in (0,\infty).
\]
Let $\cc_\tau$ be the cone of functions defined in \eqref{conedef} whose $\cf$-semi-norms are no larger than $\tau$ times their $\cg$-semi-norms.  Let
\begin{align} 
\nonumber
\cn &= \left \{ f \in \cc_\tau :r\tN_{A}\left(\frac{\varepsilon}{\norm[\cg]{f}} \right) \le N_{\max} \right\} \\
\label{nicefdefmulti}
&= \left \{ f \in \cc_\tau : \norm[\cg]{f} \le \frac{\varepsilon}{\fC_{N_{\max}/r} \fc_{N_{\max}/r} \min(h(N_{\max}/r),\tau \tildeh(N_{\max}/r))} \right\}
\end{align}
be the nice subset of the cone $\cc_\tau$.  Then it follows that Algorithm \ref{multistagealgo} is successful for all functions in $\cn$,  i.e.,  $\success(A,W,\cn,\varepsilon,N_{\max}) = 1$.  Moreover, the cost of this algorithm is bounded above in terms of the $\cg$-semi-norm of the input function as follows:
\begin{equation} \label{automultistagedetcost}
\cost(A,\cn,\varepsilon,N_{\max},\sigma)
\le \max \left(n_1, r\tN_{A}\left(\frac{\varepsilon}{\sigma} \right) \right).
\end{equation}
The upper bound on the cost of this specific algorithm provides an upper bound on the complexity of the problem, $\comp(\varepsilon,\ca(\cn,\ch,S,\Lambda),N_{\max},\sigma)$.  

Consider the limit of infinite cost budget, i.e., $N_{\max} \to \infty$.  If the sequence of algorithms $\{A_n\}_{n \in \ci}$, $A_n \in\ca_{\fix}(\cg,\ch,S,\Lambda)$  is nearly optimal for the problems $(\cg,\ch,S,\Lambda)$ and $(\cf,\ch,S,\Lambda)$ as defined in \eqref{nearoptdef}, then Algorithm \ref{multistagealgo} does not incur a significant penalty for not knowing $\norm[\cg]{f}$ a priori, i.e., for all $p>0$,
\begin{equation*} \label{multistagepenalty}
\sup_{0 < \varepsilon/\sigma \le 1} \frac{\cost(A,\cc_{\tau},\varepsilon,\infty,\sigma)} {\comp(\varepsilon/\sigma,\ca_{\fix}(\cj,\ch,S,\Lambda))} \left(\frac{\varepsilon}{\sigma}\right)^p <\infty, \qquad \cj \in \{\cf,\cg\}.
\end{equation*}
\end{theorem}

\begin{proof} Let $n_1, \ldots, n_{j}$ be the sequence of $n_i$ generated by Algorithm \ref{multistagealgo}, $j$ being the number of the iterate where the algorithm either 
\begin{enumerate}
\renewcommand{\labelenumi}{\roman{enumi})}
\item terminates because the convergence criterion, \eqref{multistageconv}, is satisfied for $i=j$, or 

\item terminates with a warning because \eqref{multistageconv} is not satisfied for $i=j$, but the the proposed $n_{j+1}$ exceeds the cost budget, $N_{\max}$. 

\end{enumerate}
Here $j$ may be any positive integer.  The design of Algorithm \ref{multistagealgo} guarantees that $n_1 < \cdots < n_j$.  It is shown that under the hypotheses of this theorem, the algorithm does terminate without a warning and the error tolerance is satisfied.  

First, consider possibility i) and recall inequality \eqref{twosidedGineq}.
Since \eqref{multistageconv} is satisfied, it follows that $\norm[\ch]{S(f)-A_{n_j}(f)} \le \varepsilon$ by the same argument as given in the proof of Theorem \ref{TwoStageDetermThm}.  In this case the algorithm terminates without warning and the approximate value is within the required tolerance.

If $j=1$, then the cost of the algorithm is $n_1$.  If $j>1$, then the convergence criterion was not satisfied for $i=j-1$. Thus, it follows that $n_{j-1} < \tN_A(\varepsilon/\norm[\cg]{f})$, since $h(n), \tildeh(n), \fC_n$, and $\fc_n$ all do not decrease as $n$ increases. 
If $n_{j-1} \ge N_{A}(\varepsilon \fc_{n_{j-1}}/G_{n_{j-1}}(f))$, then Stage 3 chooses $n_{j}$ to be the smallest element of $\ci$ that exceeds $n_{j-1}$ and for which $A_{n_{j-1}}$ is embedded in $A_{n_j}$.  By the definition of $r$ it follows that
\begin{equation*}
n_{j} \le r  n_{j-1} < r \tN_{A}(\varepsilon/\norm[\cg]{f}).
\end{equation*}
If, on the other hand, $n_{j-1} < N_{A}(\varepsilon \fc_{n_{j-1}}/G_{n_{j-1}}(f))$, then Stage 3 chooses $n_{j}$ to be the smallest element of $\ci$ that is no less than $N_{A}(\varepsilon \fc_{n_{j-1}}/G_{n_{j-1}}(f))$ and for which $A_{n_{j-1}}$ is embedded in $A_{n_j}$.  By the definition of $r$, \eqref{twosidedGineq}, and the definition of $\tN_A$, it follows that
\begin{equation*}
n_{j} < r  N_{A}(\varepsilon \fc_{n_{j-1}}/G_{n_{j-1}}(f)) \le r  N_{A}(\varepsilon/\norm[\cg]{f}) \le r \tN_{A}(\varepsilon/\norm[\cg]{f}).
\end{equation*}
In both cases, the algorithm chooses $n_{j} < r \tN_{A}(\varepsilon/\norm[\cg]{f})$.  Thus, the cost of the algorithm is bounded as in \eqref{automultistagedetcost}.

Second, consider possibility ii), meaning that the convergence criterion, \eqref{multistageconv}, is not satisfied for $i=j$.  Then Stage 3 tries to choose $n_{j+1}$ to satisfy this criterion.  Using similar arguments as in the previous paragraph, it follows that $n_j < \tN_A(\varepsilon/\norm[\cg]{f})$. 
If $n_j \ge N_{A}(\varepsilon \fc_{n_j}/G_{n_j}(f))$, then the proposed $n_{j+1}$ satisfies
\begin{equation*}
n_{j+1} \le r  n_j < r \tN_{A}(\varepsilon/\norm[\cg]{f}).
\end{equation*}
If, on the other hand, $n_j < N_{A}(\varepsilon \fc_{n_j}/G_{n_j}(f))$, then the proposed $n_{j+1}$ satisfies
\begin{equation*}
n_{j+1} < r  N_{A}(\varepsilon \fc_{n_j}/G_{n_j}(f)) \le r  N_{A}(\varepsilon/\norm[\cg]{f}) \le r \tN_{A}(\varepsilon/\norm[\cg]{f}).
\end{equation*}
In both cases, the $n_{j+1}$ proposed by the algorithm satisfies $n_{j+1} < r \tN_{A}(\varepsilon/\norm[\cg]{f})$, which does not exceed the cost budget by the  by the definition of $\cn$.  Thus, possibility ii) cannot happen.

The proof of the optimality of the multistage algorithm follows the same line of argument used to prove the optimality of the two-stage algorithm in Theorem \ref{TwoStageDetermThm}.  This completes the proof.
\end{proof}

\subsection{Lower Complexity Bounds for the Algorithms}
Lower complexity bounds are typically proved by constructing fooling functions.  First, a lower bound is derived for the complexity of problems defined on $\cf$- and $\cg$-semi-norm balls of input functions.  This technique is generally known \cite{???}.  Then it is shown how to extend this idea for the cone $\cc_{\tau}$.  

Consider the Banach spaces $\cf$, $\cg$, $\ch$, and the \emph{linear} solution operator $S: \cg \to \ch$.  Let $\Lambda$ be the set of bounded linear functionals that can be used as data. Suppose that for any $n>0$, and for all $\vL \in \Lambda^m$, satisfying $\$(\vL)\le n$, there exists an $f_1 \in \cf$, depending on $n$ and the $L_i$, with solution norm one, zero data, and bounded $\cf$ and $\cg$ semi-norms, i.e.,
\begin{equation} \label{assumpfone}
\norm[\ch]{S(f_1)} = 1, \quad \vL(f_1)= \vzero, \quad
\norm[\cg]{f_1} \le g(n), \quad \norm[\cf]{f_1} \le \tg(n) \norm[\cg]{f_1}, 
\end{equation}
for some positive, non-decreasing functions $g$ and $\tg$ defined in $(0,\infty)$.  For example, one might have $g(n)=an^p$ and $\tg(n)=bn^q$ with positive $a, b, p$, and $q$.  Since the data for the function $f_1$ are all zero, it follows that $A(f_1)=A(-f_1)$ for any algorithm, $A$, adaptive or not, that is based on the information $\vL(f_1)$.  Then, by the triangle inequality and \eqref{assumpfone} the error for one of the fooling functions $\pm f_1$ must be at least one:
\begin{align*}
\MoveEqLeft{\max(\norm[\ch]{S(f_1)-A(f_1)},\norm[\ch]{S(-f_1)-A(-f_1)})} \\
& = \max(\norm[\ch]{S(f_1)-A(f_1)},\norm[\ch]{-S(f_1)-A(f_1)})\\
& \ge \frac{1}{2} \left[ \norm[\ch]{S(f_1)-A(f_1)}+ \norm[\ch]{S(f_1)+A(f_1)} \right] \\
& \ge \frac{1}{2} \norm[\ch]{[S(f_1)-A(f_1)]+[S(f_1)+A(f_1)]} = \norm[\ch]{S(f_1)}=1.
\end{align*}
Furthermore, applying \eqref{assumpfone}, for $\cj \in \{\cf,\cg\}$, it follows that any nonadaptive algorithm satisfying the error tolerance $\varepsilon$ must have a cost $n$ satisfying the following inequality:
\begin{align*}
\varepsilon & \ge \sup_{f \ne 0} \frac{\norm[\ch]{S(f)-A(f)}}{\norm[\cj]{f}} \\
& \ge \frac{\max(\norm[\ch]{S(f_1)-A(f_1)},\norm[\ch]{S(-f_1)-A(-f_1)})}{\norm[\cj]{f_1}} \\
& \ge \frac{1}{\norm[\cj]{f_1}} 
\ge \begin{cases} \displaystyle \frac{1}{g(n)}, & \cj=\cg,\\[2ex]
 \displaystyle  \frac{1}{g(n)\tg(n)}, & \cj=\cf.
\end{cases}
\end{align*}
This implies lower bounds on the complexity of nonadaptive algorithms, as defined in \eqref{fixcostcomplex}:
\[
\comp(\varepsilon,\ca_{\fix}(\cj,\ch,S,\Lambda)) \ge
\begin{cases} g^{-1}(\varepsilon^{-1}), & \cj=\cg,\\
(g\tg)^{-1}(\varepsilon^{-1}), & \cj=\cf,
\end{cases}
\]
where $g^{-1}$ and $(g\tg)^{-1}$ denote the inverse functions of $g$ and $g\tg$, respectively.
Thus, the cost of solving the problem within error tolerance $\varepsilon$ for input functions in a $\cg$-semi-norm ball of radius $\sigma$ is at least $g^{-1}(\sigma\varepsilon^{-1})$ and for input functions in a $\cf$-semi-norm ball of radius $\sigma$ is at least $(g\tg)^{-1}(\sigma\varepsilon^{-1})$

Turning to the problem of solving functions in the cone $\cc_{\tau}$, the lower bound on the complexity becomes a bit more difficult to derive.  Note that condition \eqref{assumpfone} allows the fooling function $f_1$ to lie \emph{outside} this cone for $bn^q > \tau$.  Thus, when considering the cone of input functions, the fooling function must be modified as described below.

It is assumed that there exists a function $f_0$ with non-zero $\cg$-semi-norm lying in the interior of the cone $\cc_{\tau}$, i.e.,
\begin{equation}
\label{assumpfzero}
\norm[\cg]{f_{0}} > 0, \qquad \norm[\cf]{f_{0}} \le \tau_0 \norm[\cg]{f_{0}}, \qquad \tau_0 < \tau.
\end{equation}
Furthermore, suppose that for each $n>0$, and for all $\vL \in \Lambda^m$, satisfying $\$(\vL)\le n$, there exists $f_1$ as described above in \eqref{assumpfone}. Under these assumptions, one may show the following lower bound on the complexity of solving the problem $S$ for functions in the cone $\cc_{\tau}$.

\begin{theorem} \label{complowbd} Suppose that functions $f_{0}$ and $f_1$ can be found that satisfy conditions \eqref{assumpfone} and \eqref{assumpfzero}.  It then follows that the complexity of the problem, defined by \eqref{complexdef}, assuming infinite cost budget, over the cone of functions $\cc_{\tau}$ is
\begin{multline*}
\comp(\varepsilon,\ca(\cc_{\tau},\ch,S,\Lambda),\infty,\sigma) \\
\ge \min\left(g^{-1}\left(\frac{\sigma(\tau-\tau_0)}{2(2\tau-\tau_0)\varepsilon}\right), (g\tg)^{-1}\left(\frac{\sigma\tau(\tau-\tau_0)}{2(2\tau-\tau_0)\varepsilon}\right) \right).
\end{multline*}
\end{theorem}

\begin{proof} Let $A$ be a successful, possibly adaptive, algorithm for all functions lying in the cone $\cc_{\tau}$.  Given an error tolerance, $\varepsilon$, and a positive $\sigma$, let $f_0$ be a function satisfying \eqref{assumpfzero} and choose 
\begin{subequations}\label{c0c1bumpdef}
\begin{equation} 
\label{c0bumpdef}
c_0 = \frac{\sigma\tau}{\norm[\cg]{f_0} (2\tau - \tau_0)}.
\end{equation} 
Provide the algorithm $A$ with the input function $c_0f_0$, and let $\vL(c_0f_0)=c_0\vL(f_0)$ be the data vector extracted by $A$ to obtain the estimate $A(c_0f_0)$. Let $n=\$(\vL)$ denote the cost of this algorithm for the function $c_0f_0$, and define two fooling functions, $f_{\pm}=c_0f_{0} \pm c_1 f_1$, in terms of the $f_{0}$ and $f_1$ satisfying conditions \eqref{assumpfone} with $c_1$ satisfying
\begin{equation} 
\label{c1bumpdef}
c_1= \frac{(\tau-\tau_0)c_0\norm[\cg]{f_0}}{\norm[\cg]{f_1} [\tg(n) + \tau]} = \frac{\sigma \tau (\tau-\tau_0)}{\norm[\cg]{f_1}(2\tau - \tau_0) [\tg(n) + \tau]}.
\end{equation}
\end{subequations}
These fooling functions must lie inside the cone $\cc_{\tau}$ because
\begin{align*}
\norm[\cf]{f_{\pm}} - \tau  \norm[\cg]{f_{\pm}} & \le  c_{0}\norm[\cf]{f_0} + c_1 \norm[\cf]{f_1} - \tau (c_{0}\norm[\cg]{f_0} - c_1 \norm[\cg]{f_1}) \\
&\qquad \qquad \qquad \qquad \qquad \qquad \text{by the triangle inequality} \\
& \le c_1 [\tg(n)+\tau] \norm[\cg]{f_1} - (\tau - \tau_0) c_{0} \norm[\cg]{f_0} \qquad \text{by \eqref{assumpfone}, \eqref{assumpfzero}} \\
& = 0 \qquad \qquad \text{by \eqref{c0c1bumpdef}}.
\end{align*}
Moreover, both fooling functions have $\cg$-semi-norms no greater than $\sigma$, since
\begin{align*}
\norm[\cg]{f_{\pm}} &\le c_{0} \norm[\cg]{f_0} + c_1 \norm[\cg]{f_1} \\
& = \frac{\sigma \tau}{2\tau - \tau_0}\left[1 + \frac{\tau-\tau_0}{\tg(n) + \tau} \right] \qquad \text{by \eqref{c0c1bumpdef}}\\
&\le \frac{\sigma\tau}{2\tau - \tau_0}\left[1 + \frac{\tau-\tau_0}{\tau} \right] = \sigma.
\end{align*}

Following the argument earlier in this section, it is noted that the data used by algorithm $A$ for both fooling functions is the same, i.e., $\vL(f_{\pm})=\vL(c_0f_0)$, and so $A(f_{\pm})=A(c_0f_0)$.  Consequently, by the same argument used above, 
\[
\varepsilon  \ge  \max(\norm[\ch]{S(f_+)-A(f_+)},\norm[\ch]{S(f_-)-A(f_-)}) \ge c_1 \norm[\ch]{S(f_1)}=c_1.
\]
Since $A$ is successful for these two fooling functions, $c_1$, as defined in \eqref{c0c1bumpdef}, must be no larger than the error tolerance, which implies by \eqref{assumpfone} that 
\begin{align*}
\frac{\sigma \tau(\tau-\tau_0)}{\varepsilon} & \le \frac{\sigma \tau(\tau-\tau_0)}{c_1}  = \norm[\cg]{f_1} (2\tau - \tau_0) [\tg(n) + \tau] \\
&\le (2\tau - \tau_0) g(n) [\tg(n) + \tau)] \\
&\le 2 (2\tau - \tau_0) g(n)\max( \tg(n), \tau).
\end{align*}
Since $A$ is an arbitrary successful algorithm, this inequality provides a lower bound on the cost, $n$, that any such algorithm requires.  This then implies the lower bound on the complexity of the problem.   
\end{proof}

\begin{rem} Now suppose that the sequence of nonadaptive algorithms used to construct the adaptive, automatic Algorithm \ref{twostagedetalgo}, and the fooling functions in Theorem \ref{complowbd} have comparable powers, namely $p_1=p$ and $p_2=p+q$.  It then follows by comparing the upper bound on the cost in Theorem \ref{TwoStageDetermThm} to the lower bound in Theorem \ref{complowbd} that Algorithm \ref{twostagedetalgo} is optimal. 
\end{rem}

\vspace{1cm}

The next two sections illustrate the theorems of Section \ref{genthmsec} by looking at the problems of integration and approximation.  Each section identifies 

\begin{itemize}

\item the Banach spaces of input functions, $\cf$ and $\cg$, and their semi-norms, 

\item the Banach space of outputs, $\ch$, and its norm,

\item the solution operator, $S$,

\item a set of non-adaptive algorithms, $\{A_n\}_{n \in \ci}$, which approximate $S$, are indexed by their cost, $n$, and have the property that some lower cost algorithms are embedded in higher cost algorithms,

\item the upper bound error functions, $h$ and $\tildeh$, defined in \eqref{algseqerrbd}, 

\item a set of non-adaptive algorithms, $\{G_n\}_{n \in \ci}$, which approximate $\norm[\cg]{\cdot}$, such that $G_n$ uses the same function data as $A_n$,

\item the deflation and inflation factors, $\fc_n$ and $\fC_n$, which are defined in \eqref{normdeflate} and \eqref{norminflate} respectively, and

\item the fooling functions $f_0$ and $f_1$, along with the associated parameter $\tau_0$ and the functions $g$ and $\tg$, all of which satisfy \eqref{assumpfone} and \eqref{assumpfzero}.

\end{itemize}
This allows one to use Algorithms \ref{twostagedetalgo} and \ref{multistagealgo} with the guarantees provided by Theorems \ref{TwoStageDetermThm} and \ref{MultiStageThm}, and the lower bound on complexity provided by Theorem \ref{complowbd}.


\section{Approximation of One-Dimensional Integrals} \label{integsec}

\input{univariate_integration.tex}

\section{$\cl_{\infty}$ Approximation of Univariate Functions} \label{approxsec}

\input{ApproxUnivariate.tex}

\section{Addressing Concerns About Automatic Algorithms} \label{overcomesec}

Automatic algorithms are popular, especially for univariate integration problems.  As mentioned in Section \ref{integsec}, MATLAB \cite{TrefEtal12,MAT7.12}, Mathematica \cite{Mat8a}, and the NAG \cite{NAG23} library all have one or more automatic integration routines.  In spite of this popularity certain concerns or even objections have been raised about automatic algorithms.  This section addresses those concerns by showing that the assumptions made in this article violate the assumptions on which those concerns are based.

\subsection{Concerns Raised by Lyness}

In his provocatively titled SIAM Review article, \emph{When Not to Use an Automatic Quadrature Routine} \cite[p.\ 69]{Lyn83}, James Lyness makes the following claim.
\begin{quote}
While prepared to take the risk of being misled by chance alignment of zeros in the integrand function, or by narrow peaks which are ``missed,'' the user may wish to be reassured that for ``reasonable'' integrand functions which do not have these characteristics all will be well. It is the purpose of the rest of this section to demonstrate by example that he cannot be reassured on this point. In fact the routine is likely to be unreliable in a significant proportion of the problems it faces (say $1$ to $5\%$) and there is no way of predicting in a straightforward way in which of any set of apparently reasonable problems this will happen.
\end{quote}

The following is a summary of Lyness's argument using the notation of the present article. Lyness's automatic algorithm consists of a sequence of non-adaptive algorithms $\{A_{n_i}\}_{i=1}^{\infty}$, and a stopping rule that returns $A_{n_{i}}(f)$ as the answer for the first $i$ where $A_{n_{i}}(f)-A_{n_{i-1}}(f)$ is small enough.  To fool this automatic algorithm one constructs an integrand $f_\lambda$, which is parameterized by $\lambda$, such that 
\begin{itemize}
\item $f_\lambda$ is ``reasonable'' for all $\lambda \in [0,1]$,
\item $A_n(f_\lambda)$ is continuous in $\lambda$, and 
\item for some $i$ with moderate $n_i$, $A_{n_{i}}(f_\lambda)-A_{n_{i-1}}(f_\lambda)$ has different signs for $\lambda=0,1$.  
\end{itemize}
It then follows that $A_{n_{i}}(f_{\lambda_*})-A_{n_{i-1}}(f_{\lambda_*})=0$ for some $\lambda_* \in [0,1]$.  Then this algorithm will likely fail for integrands $f_{\lambda}$ with $\lambda$ nearby $\lambda_*$ since the stopping criterion is satisfied, but $n_i$ is not large enough to satisfy the desired error tolerance.

Lyness's model of automatic quadrature algorithms describes many, perhaps most, existing  ones.  His argument that any such algorithm will fail for certain ``reasonable'' integrands is correct.  Algorithm \ref{??} presented in Section \ref{integsec}, however, never fails for ``reasonable'' integrands because the stopping criterion used here is not of the form that Lyness assumes.  The stopping criteria used in Algorithm \ref{??}, as well as for the general Algorithms \ref{twostagedetalgo} and \ref{multistagealgo} are based on estimates of the norms of the input functions, not on whether two different algorithms give the same answer. 

Although he means it as such, Lyness's warning in \cite{Lyn83} should not be interpreted as an objection to automatic algorithms.  It should be an objection to stopping criteria that are based on the value of a single functional, in this case, $A_{n_{i}}-A_{n_{i-1}}$.  What Lyness clearly demonstrates is that it one may easily make a functional vanish even though the error of the algorithm is significant.

\subsection{No Advantage in Adaption}

There are rigorous results from information based complexity theory dating to ???? stating that adaption does not help, namely adaptive algorithms have no significant advantage over non-adaptive algorithms. The automatic algorithms presented here are by definition adaptive in determining the total number of function data required based on an initial sample of the input function.  The reason that adaption can help in this context is that the cone, $\cc_{\tau}$, of input functions is not a convex set.  This violates one of the assumptions required to prove the negative result that adaption does not help.

To see why $\cc_{\tau}$ is not convex, let $f_{\text{in}}$ and $f_{\text{out}}$ be functions in $\cf$ with nonzero $\cg$-semi-norms, where $f_{\text{in}}$  lies in the interior of this cone, and $f_{\text{out}}$ lies outside the cone.  This means that 
\[
\frac{\norm[\cf]{f_{\text{in}}}} {\norm[\cg]{f_{\text{in}}}} = \tau_{\text{in}} < \tau < \tau_{\text{out}} =  \frac{\norm[\cf]{f_{\text{out}}}} {\norm[\cg]{f_{\text{out}}}}.
\]
Next define two functions in terms of $f_{\text{in}}$ and $f_{\text{out}}$ as follows:
\[
f_{\pm} = (\tau-\tau_{\text{in}}) \norm[\cg]{f_{\text{in}}} f_{\text{out}}  \pm (\tau + \tau_{\text{out}}) \norm[\cg]{f_{\text{out}}} f_{\text{in}},
\]
These functions must lie inside  $\cc_{\tau}$ because
\begin{align*}
\frac{\norm[\cf]{f_{\pm}}} {\norm[\cg]{f_{\pm}}} &= \frac{\norm[\cf]{(\tau-\tau_{\text{in}}) \norm[\cg]{f_{\text{in}}} f_{\text{out}}  \pm (\tau + \tau_{\text{out}}) \norm[\cg]{f_{\text{out}}} f_{\text{in}}}}
{\norm[\cg]{(\tau-\tau_{\text{in}}) \norm[\cg]{f_{\text{in}}} f_{\text{out}}  \pm (\tau + \tau_{\text{out}}) \norm[\cg]{f_{\text{out}}} f_{\text{in}}}}\\
& \le 
\frac{(\tau-\tau_{\text{in}}) \norm[\cg]{f_{\text{in}}} \norm[\cf]{f_{\text{out}}}  + (\tau + \tau_{\text{out}}) \norm[\cg]{f_{\text{out}}} \norm[\cf]{f_{\text{in}}}}
{-(\tau-\tau_{\text{in}}) \norm[\cg]{f_{\text{in}}} \norm[\cg]{f_{\text{out}}}  + (\tau + \tau_{\text{out}}) \norm[\cg]{f_{\text{out}}} \norm[\cg]{f_{\text{in}}}}\\
& =
\frac{(\tau-\tau_{\text{in}})\tau_{\text{out}}  + (\tau + \tau_{\text{out}}) \tau_{\text{in}} } {-(\tau-\tau_{\text{in}}) + (\tau_{\text{out}}-\tau)}
=
\frac{\tau (\tau_{\text{out}} +\tau_{\text{in}}) } {\tau_{\text{out}} + \tau_{\text{in}}} =  \tau.
\end{align*}
On the other hand, the average of $f_{\pm}$, which is also a convex combination is 
\[
\frac{1}{2} f_- + \frac{1}{2} f_+ = (\tau-\tau_{\text{in}}) \norm[\cg]{f_{\text{in}}} f_{\text{out}}.
\]
Since $\tau > \tau_{\text{in}}$, this is a nonzero multiple of $f_{\text{out}}$, and it lies outside $\cc_{\tau}$.  Thus, this cone is not a convex set.

\subsection{Why Cones?}

There are good reasons why the error analysis for automatic algorithms is performed for input functions lying in cones, $\cc_{\tau}$, of the form \eqref{conedef}.  Automatic algorithms proceed by estimating, perhaps conservatively, the approximation error and then increasing the number of function data until that error estimate is no larger than the prescribed tolerance.  These error estimates are constructed in such a way that if they are reliable for $f$, then they are also reliable for $cf$, where $c$ is any real number.  Thus, if an algorithm is successful for the input function $f$, then it is also successful for the input $cf$, where $c$ is any real number.  By definition a cone is a set that contains $cf$ for all numbers $c$ if it contains $f$.  

One might wonder whether the definition of $\cc_{\tau}$ in \eqref{conedef} is too strict.  Ignoring the cost budget, an alternative would be to define the cone of success, $\cc_{\text{success}}$, consisting of all input functions for which the automatic algorithm successfully approximates the solution within the error tolerance.  Clearly $\cc_{\text{success}}$ contains $\cc_{\tau}$, and quite likely $\cc_{\text{success}}$ also contains functions not in $\cc_{\tau}$.  However, the definition of $\cc_{\text{success}}$ does not provide any insight into the  kind of input functions that the automatic algorithm can successfully handle.  Moreover, the optimality results that one can often prove, including those in Sections \ref{integsec} and \ref{approxsec}, show that the cost of the automatic algorithm is within a constant multiple of the cost of the best algorithm for $\cc_{\tau}$.

The automatic algorithms presented here do rely on the parameter $\tau$, the width of the cone, to be specified.  This requires the user's judgement, and it's value cannot be determined from the function data.  The choice of $\tau$ reflects how cautious the user is willing to be, since a larger $\tau$ leads to an adaptive algorithm with a higher cost. 

\section{Further Work} \label{furthersec}

The results presented here suggest a number of other interesting open problems that need to be addressed.  Here is a summary.

\begin{itemize}

\item The univariate integration and function approximation algorithms in Sections \ref{integsec} and \ref{approxsec} have low order convergence.  Guaranteed automatic algorithms with higher ordre convergence rates for smoother input functions are needed.  These might be based on higher degree piecewise polynomial approximations.

\item There are other types of problems, e.g., differential equation initial value problems and nonlinear optimization, which fit the general framework presented here.  These problems also have automatic algorithms, but without guarantees.  It would be helpful to develop guaranteed automatic algorithms in these other areas.

\item The algorithms developed here are \emph{globally adaptive}, in the sense that the function data determines the sample size, but not the kinds of data collected or the locations of the sample points, which depend only on the sample size.  Some existing automatic algorithms are \emph{locally adaptive} in that they collect more data in regions of special interest, say where the function has a spike.  Such algorithms need guarantees lie the ones that are provided here for globally adaptive algorithms.  The right kinds of spaces $\cg$ and $\cf$, and their semi-norms, need to be identified that are appropriate for locally adaptive algorithms.

\item For some numerical problems the error bound of the non-adaptive algorithm involves a $\cg$- or $\cf$-semi-norm that is very hard to approximate because of its complexity.  An example is multivariate quadrature using quasi-Monte Carlo algorithms, where the error depends on the \emph{variation} of the integrand.  The definition of the variation may vary somewhat with the definition of $\cg$ or $\cf$, but it is essentially some norm of a mixed partial derivative of the integrand.  To obtain guaranteed automatic algorithms one must either find an efficient way to approximate the variation of the function or find other suitable conservative estimates for the error that can be reliably obtained from the function data.  

\item This article considers only the worst case error of deterministic algorithms.  There are many random algorithms, and they must be analyzed by somewhat different methods.  A guaranteed Monte Carlo algorithm for estimating the mean of a random variable, which includes multivariate integration as a special case, has been proposed by \cite{HicEtal14a}.

\end{itemize}


\bibliographystyle{model1b-num-names.bst}
\bibliography{FJH22,FJHown22}
\end{document}

