%\newcommand{\R}{\mathbb{R}}
The first example we consider is univariate integration on the unit interval, $S(f)=\int_{0}^{1}f(x) \, \dif x$.  The two spaces of input functions are Sobolev spaces of different degrees of smoothness:
\begin{equation*}
  \cf=\mathcal{W}^{2,1} \quad \text{and} \quad
  \cg=\mathcal{W}^{1,1},
\end{equation*}
where the general Sobolev spaces with smoothness of degree $k$ are defined as 
\begin{equation} \label{defSobolev}
  \mathcal{W}^{k,p}=\mathcal{W}^{k,p}[0,1]=\{f\in C[0,1]: \|f^{k}\|_{p}<\infty\}.
\end{equation}
The corresponding semi-norms are $\|f\|_{\cf}=\|f''\|_{1}$ and $\|f\|_{\cg}=\|f'\|_{1}$, respectively, and the cone of integrands is defined as  
\begin{equation}\label{coneinteg}
\cc_{\tau}=\{f\in \mathcal{W}^{2,1}:\|f''\|_1\leq\tau\|f'\|_1\}.
\end{equation}
The space of outputs $\ch$ is $\reals$.

The non-adaptive building block to construct the adaptive algorithm is the composite trapezoidal rule based on $n-1$ trapezoids:
\begin{equation*}
    A_{n}(f)
    =\frac{1}{2n}[f(x_1)+2f(x_2)+\cdots+2f(x_{n-1})+f(x_n)], \qquad x_i=\frac{i-1}{n-1},
\end{equation*}
defined for $n \in \mathcal{I}=\{2,3,\ldots\}$, and with $\cost(A_n)=n$.  The algorithm $A_n$ is imbedded in the algorithm $A_{2n-1}$, which uses $2n-2$ trapezoids.  
The algorithm for approximating $\|f'\|_{1}$ is the corresponding norm of the linear spline using the same $x_i$ as used in the trapezoidal rule:
\begin{equation}\label{1direst}
    G_n(f)=\sum_{i=0}^{n-1}\left|f(x_{i+1})-f(x_{i})\right|.
\end{equation} 

\subsection{Adaptive Algorithm and Upper Bound on the Cost}

Constructing the adaptive algorithm for integration requires upper bounds on the errors of the $A_n$ and $G_n$.  The errors of the trapezoidal rule for integrands in the spaces $\mathcal{W}^{2,1}$ and $\mathcal{W}^{1,1}$ are bounded in terms of $h(n)=1/[8(n-1)^2]$ and $\tilde{h}(n)=1/[2(n-1)]$, respectively, according to \cite[(7.14) and (7.15)]{BraPet11a}.  Since $G_{n}(f)$ never overestimates $\|f'\|_{1}$, it follows that $h_{\cg_{-}}(n)=0$ and $\mathfrak{c}_n=1$. 

To find an upper bound on $\|f'\|_{1}-G_{n}(f)$, note that for $x_i \le x_{i+1}$
\begin{subequations}
\begin{equation}
f'(x)=(n-1)\left\{[f(x_{x+1})-f(x_{i})]+\int_{x_i}^{x_{i+1}}(n-1)v(t,x)f''(t) \, \dif t \right\}
\end{equation}
where 
\begin{equation}
v(t,x)=\begin{cases}  x_i-t, & x_i\leq t\leq x,\\
t-x_{i+1}, & x< t \leq x_{i+1}.
\end{cases}
\end{equation}
\end{subequations}
This implies the following upper bound on a piece of $\|f'\|_{1}$
\begin{align*}
\MoveEqLeft{\int_{x_i}^{x_{i+1}}|f'(x)| \, \dif x - |f(x_{i+1})-f(x_i)|}\\
& \le (n-1)\int_{x_i}^{x_{i+1}}\int_{x_i}^{x_{i+1}}|v(t,x)||f''(t)| \dif t \, \dif x\\
& \le  (n-1)\int_{x_i}^{x_{i+1}}2(t-x_i)(x_{i+1}-t)|f''(t)| \, \dif t \\
& \le  (n-1) \max_{x_i \le t \le x_{i+1}} \abs{2(t-x_i)(x_{i+1}-t)} \int_{x_i}^{x_{i+1}} |f''(t)| \, \dif t \\
 &  \leq  \frac{1}{2(n-1)}\int_{x_i}^{x_{i+1}} |f''(t)| \, \dif t .
\end{align*}
Applying this inequality over the entire interval, $[0,1]$ leads to 
\begin{align*}
\norm[1]{f'} - G_n(f)  &  = \sum_{i=1}^{n-1} \left \{  \int_{x_i}^{x_{i+1}}|f'(x)|\, \dif x - |f(x_{i+1})-f(x_i)| \right \} \\
& \le \frac{1}{2(n-1)} \sum_{i=1}^{n-1} \int_{x_i}^{x_{i+1}} |f''(t)| \, \dif t = 
\frac{\|f''(t)\|_{1}}{2(n-1)}.
 \end{align*}
According to \eqref{Gerrbds} and , we have $h_{+}(n)=1/[2(n-1)]$ and an inflation factor of 
\begin{equation}\label{factor}
\mathfrak{C}_n =\frac{1}{1 - \tau/[2(n-1)]} \geq 1 \qquad \text{for } n>1+\tau/2.
\end{equation}

It is straightforward to verify condition \eqref{hbestverb}, namely that $h(n) \leq \tilde{h}(n) h_{+}(n)$ for all $n>1$.  Thus, in the algorithms and theorems of the the previous sections, it is always the case that $ \tau h(n) \le \tilde{h}(n)$.  The following is multi-stage Algorithm \ref{multistagealgo} for the integration problem.  

\begin{algo} \label{multistageintegalgo}
Let the error tolerance $\varepsilon$, the maximum cost budget $N_{\text{max}}$ and the cone constant $\tau$ be given inputs. Let the sequence of algorithms $\{A_n\}_{n\in \mathcal{I}}$, $\{G_n\}_{n\in \mathcal{I}}$ be described above. Set $i=1$. Let $n_1=\lceil(\tau+1)/2\rceil+1$. For any input function $f\in \mathcal{W}^{2,1}$, do the following:
\begin{description}
\item[Stage 1.\ Estimate {$\|f'\|_{1}$}.] Compute $G_{n_i}(f)$ in \eqref{1direst}.

\item[Stage 2. Check for convergence.] Check whether $n_i$ is large enough to satisfy the error tolerance, i.e.
    \begin{align*}
     G_{n_i}(f) \le \frac{4\varepsilon(n_i-1)(2n_i-2 - \tau)}{\tau}.
    \end{align*}
    If this is true, then set $W$ to be false, return $(A_{n_i}(f),W)$ and terminate the algorithm. If this is not true, go to Stage 3.

\item[Stage 3. Compute $n_{i+1}$.] Otherwise, if the inequality above fails to hold,
choose
$$
n_{i+1}=1+ (n_i-1)\max\left\{\left\lceil\frac{1}{(n_i-1)}\sqrt{\frac{\tau G_{n_i}(f)}{8\varepsilon}}\right\rceil,2\right\}.
$$
If $n_{i+1} \le N_{\max}$, increment $i$ by $1$, and return to Stage 1.

Otherwise, if $n_{i+1} > N_{\max}$, choose $n_{i+1}$ to be the largest number not exceeding $N_{\max}$ such that $A_{n_{i}}$ is embedded in $A_{n_{i+1}}$, and set $W$ to be true. Return $(A_{n_{i+1}}(f),W)$ and terminate the algorithm.
\end{description}
\end{algo}

 \begin{theorem}

Let  $\varepsilon$, $N_{\max}$, $\tau$ and $n_1$ be given as described in Algorithm \ref{multistageintegalgo}.
Assume that $n_1 \le N_{\max}$. Let $r$ be the number described in the paragraph preceding that algorithm.
Let $\cc_\tau$ be the cone of functions defined in \eqref{coneinteg}.
Define
\[
\tN_A(a) = \min\left\{ n \in \ci : \frac{\tau \fC_n }{8(n-1)^2} \le a \right\}, \quad a \in (0,\infty).
\]
and
$$
\cn = \left \{ f \in \cc_\tau :r\tN_{A}\left(\frac{\varepsilon}{\sigma} \right) \le N_{\max} \right\}
= \left \{ f \in \cc_\tau : \sigma \le \frac{8(N_{\max}/r-1)^2 \varepsilon }{\tau \fC_{N_{\max}/r}  } \right\}
$$
be the nice subset of the cone $\cc_\tau$.  Then it follows that Algorithm \ref{multistageintegalgo} is successful for all functions in $\cn$,  i.e.,  $\success(A,W,\cn,\varepsilon,N_{\max}) = 1$.  Moreover, the cost of this algorithm is bounded above in terms of the $\|f'\|_{1}$ of the input function as follows:
\begin{multline}
\cost(A,\cn,\varepsilon,N_{\max},\sigma) \\
\le  \min\left\{ n \in \ci : n \ge \left(\frac{\sigma\tau}{8\varepsilon(1 - \tau/(2(n_{1}-1)))}\right)^{\frac{1}{2}} +1\right\}
\end{multline} where $\sigma=\|f'\|_{1}.$
 \end{theorem}

\begin{proof}
Applied Theorem \ref{MultiStageThm}, then we can have the above result.
\end{proof}




\subsection{Lower Bounds}
Now we consider the lower bound. We define the fooling function as $f_{\pm}=c_0f_0\pm c_1f_1$. The first piece of the fooling function is defined by $f_0=x$. In this case,
\begin{align*}
    \|f_0\|_{W^{1,1}}&=\|f'_0\|_1=1,\\
    \|f_0\|_{W^{2,1}}&=\|f''_0\|_1=0=\tau_0.
\end{align*}

The second part depends on the sample points. Given any $n$ sample points $\{\xi_i\}_{i=1}^{n}$ between $0$ and $1$ such that $0=\xi_0\leq \xi_1<\cdots<\xi_n\leq \xi_{n+1}=1$, let $l$ and $u$ be the two consecutive points with maximum distance between them, i.e. $\exists j\in \{1,2,\cdots,n-1\}$ such that $l=\xi_j, u=\xi_{j+1}$ and $u-l=\max_{1\leq i\leq n}(\xi_{i+1}-\xi_i)$. Then let
$$f_1(x)=\begin{cases}  \displaystyle  \frac{30(x-l^2)(x-u)^2}{(u-l)^5}, & l\leq x\leq u,\\[2ex]
 \displaystyle  0, & \text{otherwise}.
\end{cases}.$$
Note that
\begin{align*}
    \|S(f_1)\|_{W^{1,1}}&=\int_{0}^{1}f_1(x)dx=1,\\
    \|f_1\|_{W^{1,1}}&=\|f'_{1}\|_1=\frac{15}{4(u-l)}\leq \frac{15}{4}(n+1)=g(n),\\
    \|f_1\|_{W^{2,1}}&=\|f''_{1}\|_1=\frac{40}{\sqrt{3}(u-l)^2}=\frac{32}{3\sqrt{3}(u-l)}\|f_1'\|_1\leq \frac{32}{3\sqrt{3}}(n+1)\|f_1'\|_1%=\tilde{g}(n)g(n).
\end{align*}
We can easily get that $g^{-1}(n)=\frac{4}{15}x-1$ and $(\tilde{g}g)^{-1}(x)=\sqrt{\frac{\sqrt{3}}{40}x}-1$. So according to Theorem \ref{MultiStageThm} in the paper, $$\text{comp}\geq \min\left(\frac{4}{15}(\frac{\sigma}{4\varepsilon})+1,\sqrt{\frac{\sqrt{3}}{40}\frac{\sigma\tau}{4\varepsilon}}+1\right)=\min\left(\frac{\sigma}{15\varepsilon},\sqrt{\frac{\sqrt{3}\sigma\tau}{160\varepsilon}}\right)+1$$


\begin{theorem} \label{complowbdinteg} Suppose that functions $f_{0}$ and $f_1$ can be found that satisfy conditions \eqref{assumpfone} and \eqref{assumpfzero}.  It then follows that the complexity of the problem, defined by \eqref{complexdef}, assuming infinite cost budget, over the cone of functions $\cc_{\tau}$ is
$$
\comp(\varepsilon,\ca(\cc_{\tau},\ch,S,\Lambda),\infty,\sigma)
\ge \min\left(\frac{\sigma}{15\varepsilon},\sqrt{\frac{\sqrt{3}\sigma\tau}{160\varepsilon}}\right)+1.
$$
\end{theorem}

\begin{proof}
$g(n), \ \tg(n)$ and $\tau_0$ is obtained above, then by Theorem \ref{complowbd}
we can have the above results.
\end{proof}

\subsection{Numerical Example}
Consider the test function $f(x)=e^{-(a(x-\mu))^2},$
where $\mu$'s are uniformly distributed on $[-1/\sqrt{2}a,1-1/\sqrt{2}a],$ and $\log_{10} a \sim U[0,4].$
If we have large $a$, we can hardly approximate the function due to the spiky shape of the function. From the numerical results, we can find the success rate for our algorithm. Since $\norm[1]{f'}=-e^{-(a\mu)^2}-e^{-(a(1-\mu))^2}, \ \norm[1]{f''}=-2a(\mu e^{-(a\mu)^2}+(1-\mu)e^{-(a(1-\mu))^2})$, if $f \in \cc_{\tau}$ then we should have
$\norm[1]{f''}/\norm[1]{f'} \leq \tau$. We use Monte Carlo simulation with 1000000 samples to estimate this probability. To obtain the observed success rate, here we use $\tau = 10, 25 , 100$ separately to test $10000$ times with $\varepsilon = 1e-7$.
\begin{table}[h]
\centering
\begin{tabular}{cccc}
$\tau$ &  10 & 25 & 100\\
\toprule
Probability of $f \in \mathfrak{C}_{\tau}$ &  $\leq 45 \%$ &  $\leq 50 \%$  & $\leq 54 \%$ \\
Observed Success Rate & $52\%$ &  $60\%$  & $74\%$ \\
\end{tabular}
\caption{ Comparison between probability of the input function lying in the cone and the empirical success rate of the algorithm}
\end{table}

From the above table, we can find our algorithm works very well, as the observed
success rate is always larger than the expected success rate.
