The first example we consider is univariate integration on the unit interval, $S(f)=\int_{0}^{1}f(x) \, \dif x$.  The two spaces of input functions are Sobolev spaces of different degrees of smoothness:
\begin{equation*}
  \cf=\mathcal{W}^{2,1} \quad \text{and} \quad
  \cg=\mathcal{W}^{1,1},
\end{equation*}
where the general Sobolev spaces with smoothness of degree $k$ are defined as 
\begin{equation} \label{defSobolev}
  \mathcal{W}^{k,p}=\mathcal{W}^{k,p}[0,1]=\{f\in C[0,1]: \|f^{k}\|_{p}<\infty\}.
\end{equation}
The corresponding semi-norms are $\|f\|_{\cf}=\|f''\|_{1}$ and $\|f\|_{\cg}=\|f'\|_{1}$, respectively, and the cone of integrands is defined as  
\begin{equation}\label{coneinteg}
\cc_{\tau}=\{f\in \mathcal{W}^{2,1}:\|f''\|_1\leq\tau\|f'\|_1\}.
\end{equation}
The space of outputs $\ch$ is $\reals$.

The non-adaptive building block to construct the adaptive algorithm is the composite trapezoidal rule based on $n-1$ trapezoids:
\begin{equation*}
    A_{n}(f)
    =\frac{1}{2n}[f(x_1)+2f(x_2)+\cdots+2f(x_{n-1})+f(x_n)], \qquad x_i=\frac{i-1}{n-1},
\end{equation*}
defined for $n \in \mathcal{I}=\{2,3,\ldots\}$, and with $\cost(A_n)=n$.  The algorithm $A_n$ is imbedded in the algorithm $A_{2n-1}$, which uses $2n-2$ trapezoids, so the minimum cost multiple is $r=2$.  
The algorithm for approximating $\|f'\|_{1}$ is the corresponding norm of the linear spline using the same $x_i$ as used in the trapezoidal rule:
\begin{equation}\label{1direst}
    G_n(f)=\sum_{i=0}^{n-1}\left|f(x_{i+1})-f(x_{i})\right|.
\end{equation} 

\subsection{Adaptive Algorithm and Upper Bound on the Cost}

Constructing the adaptive algorithm for integration requires upper bounds on the errors of the $A_n$ and $G_n$.  The errors of the trapezoidal rule for integrands in the spaces $\mathcal{W}^{2,1}$ and $\mathcal{W}^{1,1}$ are bounded in terms of $h(n)=1/[8(n-1)^2]$ and $\tilde{h}(n)=1/[2(n-1)]$, respectively, according to \cite[(7.14) and (7.15)]{BraPet11a}.  Since $G_{n}(f)$ never overestimates $\|f'\|_{1}$, it follows that $h_{\cg_{-}}(n)=0$ and $\mathfrak{c}_n=1$. 

To find an upper bound on $\|f'\|_{1}-G_{n}(f)$, note that for $x_i \le x_{i+1}$
\begin{subequations}
\begin{equation}
f'(x)=(n-1)\left\{[f(x_{x+1})-f(x_{i})]+\int_{x_i}^{x_{i+1}}(n-1)v(t,x)f''(t) \, \dif t \right\}
\end{equation}
where 
\begin{equation}
v(t,x)=\begin{cases}  x_i-t, & x_i\leq t\leq x,\\
t-x_{i+1}, & x< t \leq x_{i+1}.
\end{cases}
\end{equation}
\end{subequations}
This implies the following upper bound on a piece of $\|f'\|_{1}$
\begin{align*}
\MoveEqLeft{\int_{x_i}^{x_{i+1}}|f'(x)| \, \dif x - |f(x_{i+1})-f(x_i)|}\\
& \le (n-1)\int_{x_i}^{x_{i+1}}\int_{x_i}^{x_{i+1}}|v(t,x)||f''(t)| \dif t \, \dif x\\
& \le  (n-1)\int_{x_i}^{x_{i+1}}2(t-x_i)(x_{i+1}-t)|f''(t)| \, \dif t \\
& \le  (n-1) \max_{x_i \le t \le x_{i+1}} \abs{2(t-x_i)(x_{i+1}-t)} \int_{x_i}^{x_{i+1}} |f''(t)| \, \dif t \\
 &  \leq  \frac{1}{2(n-1)}\int_{x_i}^{x_{i+1}} |f''(t)| \, \dif t .
\end{align*}
Applying this inequality over the entire interval, $[0,1]$ leads to 
\begin{align*}
\norm[1]{f'} - G_n(f)  &  = \sum_{i=1}^{n-1} \left \{  \int_{x_i}^{x_{i+1}}|f'(x)|\, \dif x - |f(x_{i+1})-f(x_i)| \right \} \\
& \le \frac{1}{2(n-1)} \sum_{i=1}^{n-1} \int_{x_i}^{x_{i+1}} |f''(t)| \, \dif t = 
\frac{\|f''(t)\|_{1}}{2(n-1)}.
 \end{align*}
According to \eqref{Gerrbds} and , we have $h_{+}(n)=1/[2(n-1)]$ and an inflation factor of 
\begin{equation}\label{factor}
\mathfrak{C}_n =\frac{1}{1 - \tau/(2n-2)} \geq 1 \qquad \text{for } n>1+\tau/2.
\end{equation}

By condition \eqref{hbestverb}, it follows that $\min(\tildeh(n),\tau h(n))=\tau h(n)$, which then simplifies some of the expressions in Algorithm \ref{multistagealgo} and Theorem \ref{MultiStageThm}.  Specifically, the left side of the inequality in \eqref{multistageconv} becomes
\[
\min(\tildeh(n_i),\tau h(n_i))\fC_{n_i} G_{n_i}(f) = \frac{\tau  G_{n_i}(f) } {4(n_i-1)(2n_i-2 -\tau)}.
\]
The function $N_A$ defined in \eqref{Nmindef} is
\[
N_{A}(a)= \min\left\{ n \in \ci : \tau h(n) \le a \right\} = 1+ \left \lceil \sqrt{a/(8\tau)}\right \rceil.
\]
The denominator in the definition of the set of integrands $\cn$ in \eqref{nicefdefmulti} is
\[
\fC_{N_{\max}/r} \fc_{N_{\max}/r} \min(\tildeh(N_{\max}/r),\tau h(N_{\max}/r)) =
\frac{\tau}{2 (N_{\max}-2)(N_{\max}-2 -\tau)}.
\]
The function $\tN_A$ defined in \eqref{tNmindef} is
\begin{align*} \label{tNmindef}
\tN_A(a) &= \min\left\{ n \in \ci : \min(\tildeh(n),\tau h(n))\fC_n\fc_n \le a \right\} \\
&= 1+ \left \lceil \sqrt{\frac{\tau}{8a} + \frac{\tau^2}{16}} +\frac{\tau}{4} \right \rceil \le 1+ \left \lceil\sqrt{\frac{\tau}{8a}} + \frac{\tau}{2} \right \rceil.
\end{align*}
With these preliminaries, Algorithm \ref{multistagealgo} and Theorem \ref{MultiStageThm} may be applied directly to  yield the following automatic, adaptive integration algorithm and its guarantee.

\begin{algo} \label{multistageintegalgo}
Let the error tolerance $\varepsilon$, the maximum cost budget $N_{\text{max}}$ and the cone constant $\tau$ be given inputs. Let the sequence of algorithms $\{A_n\}_{n\in \mathcal{I}}$, $\{G_n\}_{n\in \mathcal{I}}$ be described above. Set $i=1$. Let $n_1=\lceil(\tau+1)/2\rceil+1$. For any input function $f\in \mathcal{W}^{2,1}$, do the following:
\begin{description}
\item[Stage 1.\ Estimate {$\|f'\|_{1}$}.] Compute $G_{n_i}(f)$ in \eqref{1direst}.

\item[Stage 2. Check for convergence.] Check whether $n_i$ is large enough to satisfy the error tolerance, i.e.
    \begin{align*}
     G_{n_i}(f) \le \frac{4\varepsilon(n_i-1)(2n_i-2 - \tau)}{\tau}.
    \end{align*}
    If this is true, then set $W$ to be false, return $(A_{n_i}(f),W)$ and terminate the algorithm. If this is not true, go to Stage 3.

\item[Stage 3. Compute $n_{i+1}$.] Otherwise, if the inequality above fails to hold,
choose
$$
n_{i+1}=1+ (n_i-1)\max\left\{\left\lceil\frac{1}{(n_i-1)}\sqrt{\frac{\tau G_{n_i}(f)}{8\varepsilon}}\right\rceil,2\right\}.
$$
If $n_{i+1} \le N_{\max}$, increment $i$ by $1$, and return to Stage 1.

Otherwise, if $n_{i+1} > N_{\max}$, choose $n_{i+1}$ to be the largest number not exceeding $N_{\max}$ such that $A_{n_{i}}$ is embedded in $A_{n_{i+1}}$, and set $W$ to be true. Return $(A_{n_{i+1}}(f),W)$ and terminate the algorithm.
\end{description}
\end{algo}

\begin{theorem} Let  $\varepsilon$, $N_{\max}$, $\tau$ and $n_1$ be given as described in Algorithm \ref{multistageintegalgo}.  Assume that $n_1 \le N_{\max}$.  Let $\cc_\tau$ be the cone of functions defined in \eqref{coneinteg}.  Let
$$
\cn
= \left \{ f \in \cc_\tau : \norm[1]{f'} \le \frac{2\varepsilon (N_{\max}-2)(N_{\max}-2 -\tau)}{\tau} \right\}
$$
be the nice subset of the cone $\cc_\tau$.  Then it follows that Algorithm \ref{multistageintegalgo} is successful for all functions in $\cn$,  i.e.,  $\success(A,W,\cn,\varepsilon,N_{\max}) = 1$.  Moreover, the cost of this algorithm is bounded above as follows:
\begin{align*}
\cost(A,\cn,\varepsilon,N_{\max},\norm[\infty]{f'})
&\le 2+2 \left \lceil \sqrt{\frac{\tau \norm[1]{f'}}{8\varepsilon} + \frac{\tau^2 }{16}} + \frac{\tau}{4} \right\rceil \\
&\le 2+2 \left \lceil\sqrt{\frac{\tau \norm[1]{f'}}{8\varepsilon}} + \frac{\tau}{2} \right \rceil.
\end{align*}
\end{theorem}

\subsection{Lower Bound on the Computational Cost}
Next, we derive a lower bound on the cost of approximating functions in the cone $\cc_{\tau}$ by constructing fooling functions. Following the arguments of Section \ref{LowBoundSec}, we choose  $f_0(x)=x.$ Then
\[
\norm[1]{f'_0}=1, \qquad \norm[1]{f''_0}=0, \qquad \tau_0=0.
\]
For any $n =2, 3, \ldots$, suppose that the one has the data $L_i(f)=f(\xi_i)$, $i=1, \ldots, n$ for arbitrary $\xi_i$, where $0=\xi_0 \le \xi_1 < \cdots < \xi_n \le \xi_{n+1} = 1$.  There must be some $j=0, \ldots, n$ such that $\xi_{j+1} - \xi_j \ge 1/(n+1)$.  The function $f_{1}$, defined as
$$
f_{1}(x):=\begin{cases} \displaystyle
\frac{30(x-\xi_{j})^{2}(\xi_{j+1}-x)^{2}}{(\xi_{j+1}-\xi_{j})^5} & \xi_{j} \le x \leq \xi_{j+1},\\
0 & \text{otherwise},
\end{cases}
$$
has $\int_0^1 f(x) \, \dif x=1$ and $f_1(\xi_i)=0$ for $i=0, \ldots, n+1$.  Moreover, the norms of the first and second derivatives of $f_1$ are
\begin{align*}
\norm[1]{f'_1}&=\frac{15}{4(\xi_{j+1}-\xi_{j})}\leq \tg(n),\\
\norm[1]{f''_1}&=\frac{20}{\sqrt{3}(\xi_{j+1}-\xi_{j})^2}
=\frac{16\norm[1]{f'_1}}{3\sqrt{3}(\xi_{j+1}-\xi_{j})}
 \leq g(n) \norm[1]{f'_1},
\end{align*}
where the inequality $\xi_{j+1} - \xi_j \ge 1/(n+1)$ is used to define $\tg$ and $g$ as
\[
\tg(n) = \frac{15(n+1)}{4}, \qquad g(n) = \frac{16(n+1)}{3\sqrt{3}}, \qquad (g\tg)(n) = \frac{20(n+1)^2}{\sqrt{3}}.
\]
Using these choices of $f_0$ and $f_1$, along with the corresponding $\tg$ and $g$ above, one may invoke Proposition \ref{optimalprop}, Theorem \ref{complowbd}, and Corollary \ref{optimcor} to obtain the following theorem.

\begin{theorem} \label{complowbdappr} The adaptive trapezoidal algorithm is optimal for integration of functions in $\mathcal{W}^{1,1}$ and $\mathcal{W}^{2,1}$. Assuming an infinite cost budget, the complexity of the function recovery problem problem, over the cone of functions $\cc_{\tau}$ defined in \eqref{coneinteg} is
\begin{align*}
\comp(\varepsilon,\ca(\cc_{\tau},\ch,S,\Lambda),\infty,\norm[1]{f'})
&\ge \min\left(\frac{\norm[1]{f'}}{15 \varepsilon}, \sqrt{\frac{\sqrt{3}\tau\norm[1]{f'}}{80\varepsilon}} \right)-1\\
& \sim \sqrt{\frac{\sqrt{3}\tau\norm[1]{f'}}{80\varepsilon}}  \quad\text{as } \frac{\norm[\infty]{f'}}{\varepsilon} \to \infty.
\end{align*}
Moreover, Algorithm \ref{multistageapproalgo} is optimal for approximating functions in $\cc_{\tau}$.
\end{theorem}

\subsection{Numerical Example}
Consider the test function $f(x)=e^{-(a(x-\mu))^2},$
where $\mu$'s are uniformly distributed on $[-1/\sqrt{2}a,1-1/\sqrt{2}a],$ and $\log_{10} a \sim U[0,4].$
If we have large $a$, we can hardly approximate the function due to the spiky shape of the function. From the numerical results, we can find the success rate for our algorithm. Since $\norm[1]{f'}=-e^{-(a\mu)^2}-e^{-(a(1-\mu))^2}, \ \norm[1]{f''}=-2a(\mu e^{-(a\mu)^2}+(1-\mu)e^{-(a(1-\mu))^2})$, if $f \in \cc_{\tau}$ then we should have
$\norm[1]{f''}/\norm[1]{f'} \leq \tau$. We use Monte Carlo simulation with 1000000 samples to estimate this probability. To obtain the observed success rate, here we use $\tau = 10, 25 , 100$ separately to test $10000$ times with $\varepsilon = 1e-7$.
\begin{table}[h]
\centering
\begin{tabular}{cccc}
$\tau$ &  10 & 25 & 100\\
\toprule
Probability of $f \in \mathfrak{C}_{\tau}$ &  $\leq 45 \%$ &  $\leq 50 \%$  & $\leq 54 \%$ \\
Observed Success Rate & $52\%$ &  $60\%$  & $74\%$ \\
\end{tabular}
\caption{ Comparison between probability of the input function lying in the cone and the empirical success rate of the algorithm}
\end{table}

From the above table, we can find our algorithm works very well, as the observed
success rate is always larger than the expected success rate.
