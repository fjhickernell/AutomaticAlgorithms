Yuhan needs to fix this section.

Let $\ch$ be the space of continuous functions on $[0,1]$ with $\cl_{\infty}$ first derivatives.  Suppose one wants to approximate functions in $\ch$ based on function values.  The goal is to make the $\cl_{\infty}$ error small, i.e., to find an algorithm, $A$ for which $\norm[\infty]{f - A(f)} \le \varepsilon$.  A non-adaptive algorithm is described and analyzed first, however, as described in the introduction it is successful only for functions whose size is known.  Next an adaptive algorithm is constructed from the non-adaptive one.  A measure of guile is defined, and functions with moderate guile are guaranteed to be well-approximated by the non-adpative algorithm.

\subsection{Non-Adaptive Algorithms} As a start, consider the algorithm that produces a piecwise constant approximation using function values at $n+1$ equally spaced data sites, $\left\{x_i = \frac{i}{n}\right\}_{i=0}^n$:
\begin{equation} \label{appxbuildblock}
\tA_n(f) = 1_{\left[0,\frac{1}{2n}\right)}(x) \, f(0) + \sum_{i=1}^{n-1} 1_{\left[\frac{2i-1}{2n},\frac{2i+1}{2n}\right)}(x) \, f\left(x_i\right) + 1_{\left[\frac{2n-1}{2n},1\right]}(x) \, f(1).
\end{equation}
This algorithm assigns as the value $\tA_n(f)(x)$ the value of $f\left(x_i\right)$ corresponding to the data site, $x_i$, nearest $x$.  The algorithm $\tA_n$ is used to construct an adaptive algorithm to be described later.  Note that the cost of this algorithm is $n+1$, not $n$.  Although this algorithm is slightly sub-optimal for fixed sample size, 

Define the size of a function as the $\cl_{\infty}$ norm of its first derivative, i.e., $\size(f) = \norm[\infty]{f'}$. Furthermore, as in the previous section, define the ball of functions $\cb_{\sigma} :=\{f \in \ch : \norm[\infty]{f'} \le \sigma\}$.

\begin{theorem} \label{stdappxthm}  The problem of approximating functions of bounded size has complexity
\[
\comp(\varepsilon,\cb_{\sigma},\Lambda^{\std}) = \left \lceil \frac{\sigma}{2\varepsilon} \right \rceil.
\]
The algorithm defined in \eqref{appxbuildblock} is nearly optimal, in the sense that it satisfies the error tolerance with only one additional function evaluation than is optimal:
\[
\sup_{f \in \cb_{\sigma}} \norm[\infty]{f - \tA_{\lceil\sigma/(2\varepsilon)\rceil}(f)} \le \varepsilon, \qquad \cost(\tA_{\lceil\sigma/(2\varepsilon)\rceil},\cb_{\sigma}) = \left \lceil \frac{\sigma}{2\varepsilon} \right \rceil + 1.
\]
\end{theorem}
\begin{proof}  For any function $f$ lying in the ball $\cb_\sigma$, and any $x \in [0,1)$, let $x_i$ be the data site nearest $x$, and so $\abs{x-x_i} \le 1/(2\lceil\sigma/(2\varepsilon)\rceil) \le \varepsilon/\sigma$ and $\tA_{\lceil\sigma/(2\varepsilon)\rceil}(f)(x)=f(x_i)$. It then follows that
\begin{equation*}
\abs{f(x) - \tA_{\lceil\sigma/(2\varepsilon)\rceil}(f)(x)}=\abs{f(x) - f(x_i)} = \abs{\int_{x_i}^x f'(t) \, \dif t} \le \norm[\infty]{f'} \abs{x-x_i} \le  \sigma \frac{\varepsilon}{\sigma} = \varepsilon.
\end{equation*}
This implies that $\tA_{\lceil\sigma/(2\varepsilon)\rceil}$ that attains the error tolerance with  $\left \lceil \frac{\sigma}{2\varepsilon} \right \rceil + 1$ function evaluations.  A similar argument using an algorithm based on function evaluations at $x_i=(2i-1)/n,\ i=1, \ldots, n$ can be used to show that $\comp(\varepsilon,\cb_{\sigma},\Lambda^{\std}) \le \left \lceil \frac{\sigma}{2\varepsilon} \right \rceil$.

To prove a lower bound, one considers \emph{any} (possibly adaptive) algorithm $A$ using $n\le \left \lceil \frac{\sigma}{2\varepsilon} \right \rceil - 1$ function evaluations.  The proof proceeds by constructing a fooling function, $f^* \in \cb_\sigma$ for which $\norm[\infty]{f^* - A(f^*)} > \varepsilon$.  Let $A$ be any such algorithm.  Since the algorithm may be adaptive, the choice of the point, $\xi_i$, where the fooling function, $f^*$, is evaluated, may depend on $(x_1,f^*(\xi_1)), \ldots, (\xi_{i-1},f^*(\xi_{i-1}))$, for $i=2, \ldots, n$. By fiat, the fooling function is constructed to vanish at all data sites, $\xi_i$.  The fooling function is a translated and scaled bump.  Define a standard triangular bump on $\reals$ as 
\begin{equation} \label{trianglebump}
\psi(x) : = \max(1-\abs{x},0) = \begin{cases}0, & \abs{x} > 1, \\
1-\abs{x}, & \abs{x} \le 1,
\end{cases}
\end{equation}
and note that $\norm[\infty]{\psi} = \norm[\infty]{\psi'}=1$.  Given the algorithm $A$ with data sites $\xi_1, \ldots, \xi_n$, there exists at least one point $\xi_0 \in [0,1)$ with $\min_{1 \le i \le n} \abs{\xi_i -\xi_0} \ge 1/(2n)$.  Then define the fooling function by 
\[
f^*: x \mapsto \frac{\sigma}{2n}\psi(2n(x-\xi_0)).
\]
By definition, $\pm f^*(\xi_i)=0$ for $i=1, \ldots, n$, and $\norm[\infty]{\pm{f^*}'} = \sigma$, so $\pm f^* \in \cb_\sigma$.   Moreover, $\pm f^*(\xi_i)= \pm \sigma/(2n)$.  Since the two functions $\pm f^*$ provide the same data to algorithm $A$, it follows that $A(f)(\xi_0)=A(-f)(\xi_0)$.  The triangle inequality and the strict upper bound on $n$ then imply that the approximation error must be greater than the desired tolerance.
\begin{align*}
\sup_{f \in \cb} \norm[\infty]{f - A(f)} 
&\ge \max\left(\norm[\infty]{f^* - A(f^*)},\norm[\infty]{-f^* - A(- f^*)}\right) \\ 
& \ge \max\left(\abs{f^*(\xi_0) - A(f)(\xi_0)},\abs{-f^*(\xi_0) - A(-f^*)(\xi_0)}\right) \\
& \ge \frac{1}{2} \left [ \abs{f(\xi_0) - A(f^*)(\xi_0)} + \abs{f^*(\xi_0)+ A(-f^*)(\xi_0)} \right ]\\
& \ge \frac{1}{2} \abs{f^*(\xi_0) - A(f^*)(\xi_0) + f^*(\xi_0) + A(-f^*)(\xi_0)}\\
& = \abs{f^*(\xi_0)} = \frac{\sigma}{2n} \ge \frac{\sigma}{2 \left(\left \lceil \frac{\sigma}{2\varepsilon} \right \rceil - 1\right)} > \varepsilon. 
\end{align*}
Thus, $\comp(\varepsilon,\cb_{\sigma},\Lambda^{\std}) > \left \lceil \frac{\sigma}{2\varepsilon} \right \rceil -1$, which completes the proof.
\end{proof}

\subsection{Adaptive Algorithms}

The complexity result in Theorem \ref{stdappxthm} requires knowledg of the size of the function to be approximated, i.e, $\size(f) = \norm[\infty]{f'}$.  This is typically not known in advance, however, it can be approximated from the data. Given the data sites $0, 1/n, \dots, 1$, and the corresponding function values, one may estimate $\size(f)$ in terms of divided differences:
\begin{equation}
\hsigma_n(f)= \max_{i=1, \ldots, n} n\abs{f(i/n)-f((i-1)/n)}.
\end{equation}
Note that $\hsigma_n(f)$ never overestimates $\size(f)$.  The quality of this estimate depends on how much $f'$ changes over a short distance.  This motivates the following definition of guile:
\begin{equation}
\guile(f):= \frac{\norm[\infty]{f''}}{\norm[\infty]{f'}} = \frac{\norm[\infty]{f''}}{\size(f)},
\end{equation}
with the convention that constant functions have zero guile.  The guile of a function is the size of its second derivative relative to the size of its first derivative.  This guile remains unchanged when the function is multiplied by any nonzero constant.

A function with small guile has a size that is well approximated by $\hsigma_n$.  Suppose that $\zeta$ is some point satisfying $\abs{f'(\zeta)} = \norm[\infty]{f'}$ and that $(i-1)/n \le \zeta < i/n$.  It then follows that $f(i/n)-f((i-1)/n) = f'(\eta)/n$ for some $\eta$ between $(i-1)/n$ and $i/n$.  Thus,
\begin{align*}
\norm[\infty]{f'} &= \abs{f'(\zeta)} = \abs{f'(\eta) + \int_{\eta}^{\zeta} f''(x) \, \dif x} \le \abs{f'(\eta)} + \abs{\int_{\eta}^{\zeta} f''(x) \, \dif x} \\
& \le \hsigma_n(f) + \abs{\zeta - \eta} \norm[\infty]{f''} \le \hsigma_n(f) + \frac{\guile(f) \norm[\infty]{f'}}{n}, \\
\norm[\infty]{f'} & \le \frac{\hsigma_n(f)}{1 - \guile(f)/n}.
\end{align*}
Letting $\cn_{\tau} = \{ f \in \ch : \guile(f) \le \tau\}$ now carves out a subset of $\ch$ which includes functions of arbitrarily size, but for which that size can be easily bounded empirically as
\begin{equation} \label{sizebd}
\size(f) \le \frac{\hsigma_n(f)}{1 - \tau/n} \qquad \forall f \in \cn_{\tau}.
\end{equation}
This then suggests the following adaptive algorithm that is guaranteed to approximate functions in $\cn_{\tau}$ within a tolerance of $\varepsilon$.


\begin{algo} \label{adapappxalgo} Given positive numbers $\varepsilon$ and $\tau$, let $n_1=\lceil \tau \rceil +1$, and $j=1$. 
\begin{enumerate}
\renewcommand{\labelenumi}{Step \arabic{enumi}.} 
\item Let $n=n_12^{j-1}$.  Evaluate $\hsigma_{n}(f)$.
\item If 
\[
\frac{\hsigma_n(f)}{2(n - \tau)} \le 
\varepsilon,
\]
then output $A_{\varepsilon}(f) = \tA_{n}(f)$ as the approximation.  Else, let $j=j+1$ and to to Step 1.
\end{enumerate}
\end{algo}

\begin{theorem} \label{adapappxthm}  \emph{Nice} functions may be well approximated at a reasonable cost by $A_{\varepsilon}$, the adaptive Algorithm \ref{adapappxalgo}.  In particular,
\begin{gather*}
\sup_{f \in \cn_{\tau}} \norm[\infty]{f - A_{\varepsilon}(f)} \le \varepsilon, \\
\left \lceil \frac{\sigma}{2\varepsilon} \right \rceil \le \comp(\varepsilon,\cn_{\tau},\sigma) \le \cost(A_{\varepsilon},\cn_{\tau},\sigma) \le 2 \lceil \tau \rceil + \max\left(2,\frac{\sigma}{\varepsilon} \right) + 1.
\end{gather*}
\end{theorem}
\begin{proof}  For any nice function $f$ lying in $\cn_\tau$, its size is bounded above by $\hsigma_n(f)/(1-\tau/n)$ according to \eqref{sizebd}.  Furthermore, by the proof of Theorem \ref{stdappxthm} it follows that 
\[
\norm[\infty]{f - \tA_n(f)} \le \frac{\size(f)}{2n} \le \frac{\hsigma_n(f)}{2(n-\tau)}.
\]
Thus, when the stopping criterion in Step 2 is satisfied, the resulting approximation is within the desired tolerance.

The algorithm terminates for the first positive integer $j$ satisfying the inequality in Step 2.  Since $\hsigma(f)$ never overestimates $\size(f)$ for all $f \in \ch$, this means that
\begin{align*}
n \ge \tau + \frac{\hsigma_n(f)}{2 \varepsilon} 
& \Longleftarrow (\lceil \tau \rceil + 1 ) 2^{j-1} = n_1 2^{j-1} = n \ge \lceil \tau \rceil + \frac{\size(f)}{2 \varepsilon} \\
& \Longleftarrow j \ge 2 +  \log_2\left(\frac{\lceil \tau \rceil + \frac{\size(f)}{2 \varepsilon}} {\lceil \tau \rceil + 1} \right) \\
& \Longleftarrow n \ge 2 \lceil \tau \rceil + \max\left(2,\frac{\size(f)}{\varepsilon} \right).
\end{align*}
This establishes an upper bound on the cost of the algorithm.

The proof of the lower bound is similar to that in Theorem \ref{stdappxthm}. Consider \emph{any} (possibly adaptive) algorithm $A$ using $n\le \left \lceil \frac{\sigma}{2\varepsilon} \right \rceil - 1$ function evaluations. 
A fooling function $f^* \in \cn_\tau$ is constructed to vanish at all data sites, $\xi_i$.  Unlike Theorem  \ref{stdappxthm}, the fooling function must be continuously differentiable.  The bump function in Theorem \ref{stdappxthm} is replaced by 
\begin{equation} \label{contbump}
\phi(x) : = [\max(0,1-x^2)]^2 = \begin{cases}(1-x^2)^2, & \abs{x} \le 1, \\
0, & \abs{x} > 1,
\end{cases}
\end{equation}
which has first and second derivatives
\begin{equation} \label{contbump}
\phi'(x) = \begin{cases} -4x(1-x^2), & \abs{x} \le 1, \\
0, & \abs{x} > 1,
\end{cases} \qquad 
\phi''(x) = \begin{cases} -4(1-3x^2), & \abs{x} \le 1, \\
0, & \abs{x} > 1,
\end{cases}
\end{equation}
and note that $\norm[\infty]{\phi} = 1$, $\norm[\infty]{\phi'}=8/(3 \sqrt{3})$, and $\norm[\infty]{\phi''}=8$.  Given the algorithm $A$ with data sites $\xi_1, \ldots, \xi_n$, there exists at least one point $\xi_0 \in [0,1)$ with $\min_{1 \le i \le n} \abs{\xi_i -\xi_0} \ge 1/(2n)$.  Then define the fooling function by 
\[
f^*: x \mapsto \frac{\sigma}{2n}\psi(2n(x-\xi_0)).
\]
By definition, $\pm f^*(\xi_i)=0$ for $i=1, \ldots, n$, and $\norm[\infty]{\pm{f^*}'} = \sigma$, so $\pm f^* \in \cb_\sigma$.   Moreover, $\pm f^*(\xi_i)= \pm \sigma/(2n)$.  Since the two functions $\pm f^*$ provide the same data to algorithm $A$, it follows that $A(f)(\xi_0)=A(-f)(\xi_0)$.  The triangle inequality and the strict upper bound on $n$ then imply that the approximation error must be greater than the desired tolerance.
\begin{align*}
\sup_{f \in \cb} \norm[\infty]{f - A(f)} 
&\ge \max\left(\norm[\infty]{f^* - A(f^*)},\norm[\infty]{-f^* - A(- f^*)}\right) \\ 
& \ge \max\left(\abs{f^*(\xi_0) - A(f)(\xi_0)},\abs{-f^*(\xi_0) - A(-f^*)(\xi_0)}\right) \\
& \ge \frac{1}{2} \left [ \abs{f(\xi_0) - A(f^*)(\xi_0)} + \abs{f^*(\xi_0)+ A(-f^*)(\xi_0)} \right ]\\
& \ge \frac{1}{2} \abs{f^*(\xi_0) - A(f^*)(\xi_0) + f^*(\xi_0) + A(-f^*)(\xi_0)}\\
& = \abs{f^*(\xi_0)} = \frac{\sigma}{2n} \ge \frac{\sigma}{2 \left(\left \lceil \frac{\sigma}{2\varepsilon} \right \rceil - 1\right)} > \varepsilon. 
\end{align*}
Thus, $\comp(\varepsilon,\cb_{\sigma},\Lambda^{\std}) > \left \lceil \frac{\sigma}{2\varepsilon} \right \rceil -1$, which completes the proof.
\end{proof}
