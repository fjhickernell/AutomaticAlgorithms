
In this part, we consider the function recovery $S(f)=f$ and $\ch=\cl_{\infty}.$
Then $\norm[\ch]{S(f)-A(f)}=\norm[\infty]{f-A(f)}.$
Let $\cg \subset \mathcal{W}^{1,\infty}[0,1]$ and $\cf \subset \mathcal{W}^{2,\infty}[0,1],$ where
$$\mathcal{W}^{k,\infty}[0,1]=\left\{u \in \cl_{\infty}[0,1]:D^\alpha u \in \cl_\infty[0,1], \forall |\alpha| \leq k \right\}.$$

Given $\tau>0$, then by the \eqref{conedef}, we have $\cc_{\tau} \subset \cf$ as
\begin{equation}\label{taudef}
\cc_{\tau}=\{f \in  \mathcal{W}^{2,\infty}[0,1] : \norm[\infty]{f''}\leq \tau\norm[\infty]{f'}\},
\end{equation}

Suppose one wants to approximate functions in $\cc_{\tau}$ based on function values.  The goal is to find an algorithm, $A$ for which $$\norm[\infty]{f-A(f)} \le \varepsilon.$$

Here we use piecewise linear interpolation to approximate. Thus $\forall f \in \cc_{\tau},$ consider the sequence of algorithms $\{A_n\}_{n \in \ci},$ where
$\ci=\{2,3,\dots\}$
 and given the data $f(x_i),$ where $ x_i=\frac{i}{n-1}, i=0, \ldots,n-1,$ and $\cost(A_{n})=n.$

For $x_{i-1} \leq x \leq x_i, \ i=1, \ldots, n-1,$ define $A_{n}(f)$
$$A_{n}(f)(x)=f(x_{i-1})+\frac{x-x_{i-1}}{x_i-x_{i-1}}\left(f(x_i)-f(x_{i-1})\right).$$


\subsection{Upper Bound}

Fix the algorithm $A_n,$ which $\cost(A_n)=n,$
suppose the data sites $x_0, x_1, \dots, x_{n-1}$ and the corresponding function values. One may estimate $\norm[\infty]{f'}$ in terms of divided differences:
\begin{equation} \label{estfirstderiv}
G_{n}(f)= \max_{i=1, \ldots, n-1} (n-1)\abs{f(x_i)-f(x_{i-1})}.
\end{equation}
Note that $G_n(f)$ never overestimates $\norm[\infty]{f'}$. Thus, we have
$$\err_{-}(G_n,\cf,\reals_{+},\norm[\cg]{\cdot})=0.$$
 Suppose that $\zeta$ is some point satisfying $\abs{f'(\zeta)} = \norm[\infty]{f'}$ and
that $x_{i-1}\le \zeta < x_i$.  It then follows that $f(x_i)-f(x_{i-1}) = f'(\eta)/(n-1)$ for some $\eta$ between $x_{i-1}$ and $x_i$.  Thus,
%\begin{align*}
%\norm[\infty]{f'} &= \abs{f'(\zeta)}\leq \abs{f'(\eta)} + \abs{f'(\eta)-f'(\zeta) }  \le G_{n}(f) + \abs{ \eta-\zeta}^{s}\sup\limits_{x,t \in [0,1]}\frac{|f'(x)-f'(t)|}{|x-t|^{s}}\nonumber\\
%& \leq  G_n(f) + \frac{1}{m^{s}} \sup\limits_{x,t \in [0,1]}\frac{|f'(x)-f'(t)|}{|x-t|^{s}}\nonumber\\
%&\Rightarrow \err_{+}(G_n,\cf,\reals_{+},\norm[\cg]{\cdot}) =\frac{1}{(n-1)^{s}}
%\end{align*}
\begin{align*}
\norm[\infty]{f'} &= \abs{f'(\zeta)}\leq \abs{f'(\eta)} + \abs{f'(\eta)-f'(\zeta) }  \le G_{n}(f) + \abs{\int_\zeta^\eta f''(x)dx }\nonumber\\
& \leq  G_n(f) + \norm[\infty]{f''}|\eta-\zeta| \leq G_n(f)+ \frac{1}{n-1}\norm[\infty]{f''}\nonumber\\
&\Rightarrow \err_{+}(G_n,\cf,\reals_{+},\norm[\cg]{\cdot}) =\frac{1}{n-1}.
\end{align*}
Hence, we have
%$$\err_{-}(G_n,\cf,\reals_{+},\norm[\cg]{\cdot})=0, \ \err_{+}(G_n,\cf,\reals_{+},\norm[\cg]{\cdot}) =\frac{1}{(n-1)^{s}}.
%$$
$$\err_{-}(G_n,\cf,\reals_{+},\norm[\cg]{\cdot})=0, \ \err_{+}(G_n,\cf,\reals_{+},\norm[\cg]{\cdot}) =\frac{1}{(n-1)}.
$$

Here we use same notation for divided difference. Denote
$$f[x,y]:=\frac{f(y)-f(x)}{y-x},f[x,y,z]:=\frac{f[y,z]-f[x,y]}{z-x}.$$
Note that for all $x,y \in [0,1]$ distinct, we have
 $$|f(y)-f(x)|=\left|\int_{x}^{y}f'(t)dt\right| \leq |y-x|\|f'\|_{\infty}.$$
 Therefore we have $|f[x,y]|\leq \|f'\|_{\infty}.$\\
For $x_{i-1} \leq x \leq x_i, \ i=1, \ldots, n-1,$ we  can rewrite
$$A_{n}(f)(x)=f\left(x_{i-1}\right)+f\left[x_{i-1},x_{i}\right]\left(x-x_{i-1}\right).$$
Then by Eq(2.67) in \cite{walter}, we have
$$
f(x)-A_{n}(f)(x)
=f\left[x_{i-1},x_{i},x\right]\cdot (x-x_{i-1})(x-x_i).
$$
As we can rewrite
$$|f\left[x_{i-1},x_{i},x\right]|=\left|\frac{f[x_{i},x]-f[x_{i-1},x]}{x_i-x_{i-1}}\right|\leq2(n-1)\norm[\infty]{f'}.$$
And as $x \in \left[x_{i-1},x_{i} \right],$ thus we have
$\left|(x-x_{i-1})(x-x_{i})\right| \leq \frac{1}{4(n-1)^2}.$
Then,
$$
|f(x)-A_{n}(f)(x)|\leq \frac{1}{4(n-1)^2}\cdot 2(n-1)\norm[\infty]{f'}=\frac{1}{2(n-1)}\norm[\infty]{f'}.
$$
Similarly, we can obtain that for $x \in [0,1]$
$$\norm[\infty]{f-A_{n}(f)} \leq \frac{\|f'\|_{\infty}}{2(n-1)} \Rightarrow \err(A_n,\cg,\ch,S) \leq \frac{1}{2}(n-1)^{-1} \Rightarrow h(n)=\frac{1}{2}(n-1)^{-1}.$$

On the other hand, by Eq(2.68) in \cite{walter},
we know
$f\left[x_{i-1},x_{i},x\right]=\frac{1}{2}f''(\xi),$ where $\xi \in [x_{i-1},x_{i}]$
Then we know
$$
|f\left[x_{i-1},x_{i},x\right] |\leq \frac{1}{2}\|f''\|_{\infty}.$$

And as $x \in \left[x_{i-1},x_{i} \right],$ thus we have
$\left|(x-x_{i-1})(x-x_{i})\right| \leq \frac{1}{4(n-1)^2}.$

Then,
$$
|f(x)-A_{n}(f)(x)|\leq \frac{1}{4(n-1)^2}\frac{\norm[\infty]{f''}}{2}=\frac{1}{8(n-1)^2}\norm[\infty]{f''}.
$$

Hence, we can obtain that for $\forall x \in [0,1]$
$$\norm[\infty]{f-A_{n}(f)} \leq  \frac{1}{8(n-1)^{2}}\norm[\infty]{f''} \Rightarrow \err(A_n,\cf,\ch,S) \leq \frac{1}{8}(n-1)^{-2}
\Rightarrow \tilde{h}(n)=\frac{1}{8}(n-1)^{-2}.$$


%Combine eq.(\ref{errorg}) and eq.(\ref{errorf}), we have the same result as in eq. (\ref{Anerrbound}).
%$$\norm[\ch]{S(f) -  A_n(f)} \le \frac{\min(C_1,C_2\tau n^{p_1-p_2})\norm[\cg]{f}}{n^{p_1}} \qquad \forall f \in \cc_{\tau}.$$

%
%Then by Algorithm \ref{twostagedetalgo}, we have the following algorithm
%\begin{algo} \label{adpatalgo}
%{\bf (Automatic, Adaptive, Two-Stage, Deterministic).}
% Let $\varepsilon$ be a positive error tolerance, and let $N_{\max}$ be the maximum cost allowed.  Let $\tau$ be a fixed positive number and
% let $A_{n}$ is defined as above. Let $n_{0}$ be a number larger than $\tau+2.$ Given an input function $f$, do the following:
%
%\begin{description}
%\item[Stage 1.\ Estimate {$\norm[\infty]{f'}$}.] Compute $G_{n_{0}}(f)$ at a cost of $n_{0} = \cost(A_{n_{0}})$.   Define the inflation factor
%$$
%\fC =\frac{1}{1 - \tau (n_{0}-1)^{-s}} \ge 1.
%$$
%Then $\fC G_{n_{0}}(f)$ provides a reliable upper bound on $\norm[\infty]{f'}$.
%
%\item [Stage 2.\ Estimate {$f$}.] Choose the sample size need to approximate $f$, namely, check whether $N$ is large enough to satisfy the error tolerance, i.e.,
%\[
%\frac{\min(1,\tau (N-1)^{-s})\fC G_{n_{0}}(f)}{2N} \le \varepsilon.
%\]
%If $N \le N_{\max}-n_0$, then $f$ may be approximated within the desired error tolerance and within the cost budget.  Set the warning flag to false, $W=0$. Otherwise, recompute $N$ to be within budget, $N = \tN_{\max} := \max\{N \in \ci : N\le N_{\max} -  N_G\}$, and set the warning flag to true, $W=1$.  Compute $A_N(f)$ as the approximation to $f$.
%
%\end{description}
%
%\end{algo}

However, we can find if $\fC =\frac{1}{1 - \tau (n-1)^{-1}} \ge 1,$ then $\tau (n-1)^{-1} <1,$ which means $ \tau\tilde{h}(n)< h(n).$
Thus we only need to find $N$ such that  \[
\frac{\tau \fC G_{n_{0}}(f)}{8(N-1)^2} \le \varepsilon.
\]

Then we can have the complexity of the upper bound.
\begin{theorem}   Let  $\varepsilon$, $N_{\max}$, $\tN_{\max}$, $\fC$, and $\tau$ be given as described in Algorithm \ref{twostagedetalgo},  Let
$$
\fc =1 + \tau \err_{-}(G,\cf,\reals_+,\norm[\cg]{\cdot}) =1.
$$
Let $\cc_\tau$ be the cone of functions defined in \eqref{taudef}.
Then it follows that Algorithm \ref{twostagedetalgo} is successful for all functions in this set of \emph{nice} functions $\cn$,  i.e.,  $\success(A,W,\cn,\varepsilon,N_{\max}) = 1$.  Moreover, the cost of this algorithm is bounded above in terms of the $\norm[\infty]{f'}$ of the input function as follows:
\begin{multline}
\cost(A,\cn,\varepsilon,N_{\max},\sigma) \\
\le  \min\left\{ n \in \ci : n \ge \left(\frac{\sigma\tau}{8\varepsilon(1 - \tau/(n_{0}-1))}\right)^{\frac{1}{2}} +1\right\}
\end{multline} where $\sigma=\norm[\infty]{f'}.$
%The upper bound on the cost of this specific algorithm provides an upper bound on the complexity of the problem, $\comp(\varepsilon,\ca(\cn,\ch,S,\Lambda),N_{\max},\sigma)$.  If the sequence of algorithms $\{A_n\}_{n \in \ci}$, $A_n \in\ca_{\fix}(\cg,\ch,S,\Lambda)$  is nearly optimal for the problems $(\cg,\ch,S,\Lambda)$ and $(\cf,\ch,S,\Lambda)$ as defined in \eqref{nearoptdef}, then Algorithm \ref{twostagedetalgo} does not incur a significant penalty for not knowing $\norm[\cg]{f}$ a priori, i.e., for all $p>0$,
%\begin{equation} \label{penalty}
%\sup_{0 < \varepsilon/\sigma \le 1} \frac{\cost(A,\cn,\varepsilon,\infty,\sigma)} {\comp(\varepsilon/\sigma,\ca_{\fix}(\cj,\ch,S,\Lambda))} \left(\frac{\varepsilon}{\sigma}\right)^p <\infty, \qquad \cj \in \{\cf,\cg\}.
%\end{equation}
\end{theorem}

\begin{proof}
Applied Theorem \ref{TwoStageDetermThm}, then we can have the above result.
\end{proof}

If we consider about the embedded algorithms, by Algorithm $\ref{multistagealgo},$ we obtain
\begin{algo}
Let the error tolerance $\varepsilon$, the maximum cost budget $N_{\max}$, and the positive constant $\tau$ be as described in  \eqref{taudef}. Let the sequences of algorithms, $\{A_n\}_{n \in \ci}$ and  $\{G_n\}_{n \in \ci}$ be as described above.  Set $i=1$.  Let $n_1$ be the smallest number in $n \in \ci$ satisfying $\frac{1}{n-1} \le 1/\tau$. Then do the following:
\begin{description}

\item [Stage 1. Estimate {$\norm[\infty]{f'} $}.]
Compute $G_{n_i}(f)$ in \eqref{estfirstderiv}.
\item [Stage 2. Check for Convergence.]
 Check whether $n_i$ is large enough to satisfy the error tolerance, i.e.,
$$
\frac{1}{8}\frac{\tau}{(n_i-1)^{2} - \tau (n_i-1)} G_{n_i}(f) \le \varepsilon.
$$
If this is true, then set $W=0$, return $(A_{n_i}(f),W)$ and terminate the algorithm.


\item[Stage 3. Compute $n_{i+1}$.]  Otherwise, if the inequality above fails to hold,
choose $n_{i+1}$ as the smallest number exceeding $n_i$ and not less than $N_{A}(\varepsilon /G_{n_i}(f))$ such that $A_{n_{i}}$ is embedded in $A_{n_{i+1}}$. That is
$$n_{i+1}-1=(n_i-1)\max\left\{\left\lceil\frac{1}{(n_i-1)}\left(\frac{\tau G_{n_i}(f)}{8\varepsilon}\right)^{\frac{1}{2}}\right\rceil,2\right\}.$$
 If $n_{i+1} \le N_{\max}$, increment $i$ by $1$, and return to Stage 1.
Otherwise, if $n_{i+1} > N_{\max}$, choose $n_{i+1}$ to be the largest number not exceeding $N_{\max}$ such that $A_{n_{i}}$ is embedded in $A_{n_{i+1}}$, and set the $W=1$. Return $(A_{n_{i+1}}(f),W)$ and terminate the algorithm.
\end{description}
\end{algo}

%\begin{theorem} \label{adapappxthm}  \emph{Nice} functions may be well approximated at a reasonable cost by $A_{\varepsilon}$, the adaptive Algorithm \ref{adapappxalgo}.  In particular,
%\begin{align*}
%\sup_{f \in \cn_{\tau}} \norm[\infty]{f - A_{\varepsilon}(f)} & \le \varepsilon, \\
%\min\left\{\left\lceil  \left(\frac{(3-s)^{\frac{1-s}{2}}}{32(1-s)^{\frac{1-s}{2}}}\frac{\sigma\tau}{\varepsilon}\right)^{\frac{1}{1+s}}  \right\rceil-1, \left\lceil \frac{3\sqrt{3}\sigma}{16\varepsilon}\right\rceil-1 \right\} & \leq \comp(\varepsilon,\mathcal{N}_{s,\tau},\sigma)  \\
%\le \cost(A_{\varepsilon},\cn_{\tau},\sigma) & \le  \left\lceil \min\left( \frac{\sigma}{2\varepsilon(1 - \tau/n_{0}^{s})} , \left(\frac{\sigma\tau}{2\varepsilon(1 - \tau/n_{0}^{s})}\right)^{\frac{1}{1+s}}\right) \right\rceil.
%\end{align*}
%\end{theorem}
%\begin{proof}
%For any nice function $f$ lying in $\mathcal{N}_{s,\tau},$ we can have
%$$\|f-A_{\varepsilon}(f)\|_{\infty}\leq \min\left\{ \frac{\hsigma_{n_{0}}(f)}{2n(1 - \tau/n_{0}^{s})} , \frac{\tau\hsigma_{n_{0}}(f)}{2n^{1+s}(1 - \tau/n_{0}^{s})} \right\}.$$
%Thus, when the stopping criterion in Step 2 is satisfied, the resulting approximation is within
%the desired tolerance.
%
%The algorithm terminates for the first positive integer $j$ satisfying the inequality in Step
%2. Since $\hat{\sigma}(f)$ never overestimates size$(f)$ for all $f \in H,$ this means that
%\begin{align*}
% & n \ge \left\lceil \min\left( \frac{\hsigma}{2\varepsilon(1 - \tau/n_{0}^{s})} , \left(\frac{\tau\hsigma}{2\varepsilon(1 - \tau/n_{0}^{s})}\right)^{\frac{1}{1+s}}\right) \right\rceil\\
%   \Rightarrow & n \ge  \left\lceil \min\left( \frac{\hsigma_{n_{0}}(f)}{2\varepsilon(1 - \tau/n_{0}^{s})} , \left(\frac{\tau\hsigma_{n_{0}}(f)}{2\varepsilon(1 - \tau/n_{0}^{s})}\right)^{\frac{1}{1+s}}\right) \right\rceil\\
%\Rightarrow &  \min\left\{ \frac{\hsigma_{n_{0}}(f)}{2n(1 - \tau/n_{0}^{s})} , \frac{\tau\hsigma_{n_{0}}(f)}{2n^{1+s}(1 - \tau/n_{0}^{s})} \right\}   \leq \varepsilon.
%\end{align*}
%This establishes an upper bound on the cost of the algorithm.\\


\subsection{Lower Bound}
Next, we will derive the lower bound.

Suppose that for any $n>0,$ for any algorithm which cost is less than $n.$
We want to build fooling function at all data sites $\xi_{i}, \ i=0,\ldots,n-1.$
There exists at least one $i$ such that $\xi_{i+1}-\xi_{i}\ge\frac{1}{n-1}.$
Then we can define $f_{1}$
$$f_{1}(x):=\left\{\begin{matrix}
A_{1}(x-\xi_{j})^{2}(x-\xi_{j+1})^{2} & \xi_{j}< x \leq \xi_{j+1},\\
0 & \text{otherwise},
\end{matrix}\right.,$$
where $A_{1} \ge 0.$

As we need $\norm[\infty]{f_1}=1,$ we can have $A_{1}=\frac{16}{(\xi_{i+1}-\xi_{i})^4}.$
Then we can obtain
$$\norm[\infty]{f'_1}=\frac{16}{3\sqrt{3}(\xi_{j+1}-\xi_{j})}\leq \frac{16}{3\sqrt{3}}(n-1).$$
$$\norm[\infty]{f''_1}=\frac{16}{(\xi_{j+1}-\xi_{j})^2}
=\frac{3\sqrt{3}}{(\xi_{j+1}-\xi_{j})}\norm[\infty]{f'_1}
 \leq 3\sqrt{3}(n-1)\norm[\infty]{f'_1}.
$$
Thus, we have
$$g(n)=\frac{16}{3\sqrt{3}}(n-1), \ \tg(n)=3\sqrt{3}(n-1).$$

Let $f_0=x.$ Then we have
$$\norm[\infty]{f'_0}=1, \norm[\infty]{f''_0}=0 \Rightarrow \tau_0=0.$$

\begin{theorem} \label{complowbdappr} Suppose that functions $f_{0}$ and $f_1$ can be found that satisfy conditions \eqref{assumpfone} and \eqref{assumpfzero}.  It then follows that the complexity of the problem, defined by \eqref{complexdef}, assuming infinite cost budget, over the cone of functions $\cc_{\tau}$ is
$$
\comp(\varepsilon,\ca(\cc_{\tau},\ch,S,\Lambda),\infty,\sigma)
\ge \min\left(\frac{3\sqrt{3}\sigma}{64 \varepsilon}+1, \left(\frac{\sigma\tau}{64\varepsilon}\right)^{2}+1 \right).
$$
\end{theorem}

\begin{proof}
$g(n), \ \tg(n)$ and $\tau_0$ is obtained above, then by Theorem \ref{complowbd}
we can have the above results.
\end{proof}

\subsection{Numerical Example}

We want to testify the success rate for our algorithm. So we
choose test function $$f(x)=e^{-(a(x-\mu))^2},$$
where $\mu \sim U[0,1]$ and $\log_{10} a \sim U[0,4].$
And $\mu \sim U[0,1]$ means $\mu$ is uniform on [0,1].

Also for this kind of function, if we have large $a$, we can approximate the function hardly
because the function has a sharper bump.

And we know
$$\norm[\infty]{f'}=\sqrt{\frac{2}{e}} a, \ \norm[\infty]{f''}=2a^2.$$

If $f \in \cc_{\tau}$ then we should have
$$a \leq \frac{\tau}{\sqrt{2e}} \Rightarrow \text{Expected Success Rate }= \frac{\ln \left(\frac{\tau}{\sqrt{2e}}\right)}{4}.$$

We use $\tau = 10, 25 , 100$ separately to test $10000$ times with
$\varepsilon = 1e-7$ to obtain the observed success rate.
\begin{table}[H]
\begin{tabular}{|c |c |c| c |}
\hline
$\tau$ &  10 & 25 & 100\\
\hline
Expected Success Rate &  $\leq 16 \%$ &  $\leq 26 \%$  & $\leq 41 \%$ \\
  \hline
Observed Success Rate & 29$\%$ &  38$\%$  & 53$\%$ \\
\hline
\end{tabular}
\end{table}
From the above table, we can find our algorithm works very well, as the observed
success rate is always larger than the expected success rate.\\




