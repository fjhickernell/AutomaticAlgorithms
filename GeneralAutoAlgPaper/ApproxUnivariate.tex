
In this part, we consider the function recovery $S(f)=f$ and $\ch=\cl_{\infty}.$
Then $\norm[\ch]{S(f)-A(f)}=\norm[\infty]{f-A(f)}.$
Let $\cg \subset \mathcal{W}^{1,\infty}[0,1]$ and $\cf \subset \mathcal{W}^{2,\infty}[0,1],$ where
$\mathcal{W}^{k,\infty}[0,1]=\left\{u \in \cl_{\infty}[0,1]:D^\alpha u \in \cl_\infty[0,1], \forall |\alpha| \leq k \right\}.$
Given $\tau>0$, then by \eqref{conedef}, we have $\cc_{\tau} \subset \cf$ as
\begin{equation}\label{taudef}
\cc_{\tau}=\{f \in  \mathcal{W}^{2,\infty}[0,1] : \norm[\infty]{f''}\leq \tau\norm[\infty]{f'}\},
\end{equation}

Suppose one wants to approximate functions in $\cc_{\tau}$ based on function values.  The goal is to find an algorithm, $A$ for which $\norm[\infty]{f-A(f)} \le \varepsilon.$ Here we use piecewise linear interpolation to approximate. Thus $\forall f \in \cc_{\tau},$ consider the sequence of algorithms $\{A_n\}_{n \in \ci},$ where
$\ci=\{2,3,\dots\}$
 and given the data $f(x_i),$ where $ x_i=\frac{i}{n-1}, i=0, \ldots,n-1,$ and $\cost(A_{n})=n.$
For $x_{i-1} \leq x \leq x_i, \ i=1, \ldots, n-1,$ define $A_{n}(f)$
$$A_{n}(f)(x)=f(x_{i-1})+\frac{x-x_{i-1}}{x_i-x_{i-1}}\left(f(x_i)-f(x_{i-1})\right).$$

\subsection{Upper Bound}

%Fix the algorithm $A_n,$ which $\cost(A_n)=n,$
%suppose the data sites $x_0, x_1, \dots, x_{n-1}$ and the corresponding function values. One may estimate $\norm[\infty]{f'}$ in terms of divided differences:
%\begin{equation} \label{estfirstderiv}
%G_{n}(f)= \max_{i=1, \ldots, n-1} (n-1)\abs{f(x_i)-f(x_{i-1})}.
%\end{equation}

% Suppose that $\zeta$ is some point satisfying $\abs{f'(\zeta)} = \norm[\infty]{f'}$ and
%that $x_{i-1}\le \zeta < x_i$.  It then follows that $f(x_i)-f(x_{i-1}) = f'(\eta)/(n-1)$ for some $\eta$ between $x_{i-1}$ and $x_i$.  Thus,
%%\begin{align*}
%%\norm[\infty]{f'} &= \abs{f'(\zeta)}\leq \abs{f'(\eta)} + \abs{f'(\eta)-f'(\zeta) }  \le G_{n}(f) + \abs{ \eta-\zeta}^{s}\sup\limits_{x,t \in [0,1]}\frac{|f'(x)-f'(t)|}{|x-t|^{s}}\nonumber\\
%%& \leq  G_n(f) + \frac{1}{m^{s}} \sup\limits_{x,t \in [0,1]}\frac{|f'(x)-f'(t)|}{|x-t|^{s}}\nonumber\\
%%&\Rightarrow \err_{+}(G_n,\cf,\reals_{+},\norm[\cg]{\cdot}) =\frac{1}{(n-1)^{s}}
%%\end{align*}
%\begin{align*}
%\norm[\infty]{f'} &= \abs{f'(\zeta)}\leq \abs{f'(\eta)} + \abs{f'(\eta)-f'(\zeta) }  \le G_{n}(f) + \abs{\int_\zeta^\eta f''(x)dx }\nonumber\\
%& \leq  G_n(f) + \norm[\infty]{f''}|\eta-\zeta| \leq G_n(f)+ \frac{1}{n-1}\norm[\infty]{f''}\nonumber\\
%&\Rightarrow \err_{+}(G_n,\cf,\reals_{+},\norm[\cg]{\cdot}) =\frac{1}{n-1}.
%\end{align*}


Given the data sites $x_i=(i-1)/(n-1)$, $i=0, \ldots, n$, let the $\cl_\infty$ norm of the $f'$ be approximated by
\begin{equation}\label{estfirstderiv}
G_n(f) = (n-1) \sup_{i=1, \ldots, n-1} \abs{f(x_{i+1})-f(x_i)}
\end{equation}

Note that $G_n(f)$ never overestimates $\norm[\infty]{f'}$. Thus, we have
$h_{-}(n)=0.$
For any $x \in [x_{i}, x_{i+1}]$, note that
\begin{align*}
\MoveEqLeft{\abs{f'(x)} - (n-1) \abs{f(x_{i+1})-f(x_i)}} \\
 & \le \abs{f'(x) - (n-1)[f(x_{i+1})-f(x_i)]} \\
&=\abs{ \int_{x_i}^{x_{i+1}} f''(t) [(n-1)(t-x_i) -  1_{[x,x_{i+1}]}(t)] \, \dif t} \\
& \le \sup_{x_i \le t \le x_{i+1}} \abs{f''(t)} \int_{x_i}^{x_{i+1}} \abs{  (n-1)(t-x_i) -  1_{[x,x_{i+1}]}(t)} \, \dif t \\
&=\sup_{x_i \le t \le x_{i+1}} \abs{f''(t)} \left \{\frac{1}{2(n-1)} - (x-x_i)[1-(n-1)(x-x_i)] \right\} \\
&\le \frac{1}{2(n-1)}\sup_{x_i \le t \le x_{i+1}} \abs{f''(t)}
\end{align*}
Furthermore, this inequality is tight if $f''$ is constant second derivative and $f'$ does not change sign over $[x_{i}, x_{i+1}]$, and $x=x_i$ or $x_{i+1}$.  Applying the above argument for $i=0, \ldots, n-1$ implies that
\[
\norm[\infty]{f'} - G_n(f) \le \frac{\norm[\infty]{f''}}{2(n-1)} \Rightarrow h_{+}(n)=\frac{1}{2(n-1)}
\]
with equality holding when has a constant second derivative and its first derivative does not change sign over $[0,1]$.
Then by \eqref{normdeflate} and \eqref{norminflate}, we can obtain
\begin{equation}\label{inflaapprox}
\fc_n=1, \ \   \fC_n =\frac{1}{1 - \tau /(2(n-1))}.
\end{equation}

Here we use divided difference. Denote
$$f[x,y]:=\frac{f(y)-f(x)}{y-x},f[x,y,z]:=\frac{f[y,z]-f[x,y]}{z-x}.$$
Note that for all $x,y \in [0,1]$ distinct, we have
 $$|f(y)-f(x)|=\left|\int_{x}^{y}f'(t)dt\right| \leq |y-x|\|f'\|_{\infty}.$$
 Therefore we have $|f[x,y]|\leq \|f'\|_{\infty}.$
For $x_{i-1} \leq x \leq x_i, \ i=1, \ldots, n-1,$ we  can rewrite
$A_{n}(f)(x)=f\left(x_{i-1}\right)+f\left[x_{i-1},x_{i}\right]\left(x-x_{i-1}\right).$
Then by Eq(2.67) in \cite{walter}, we have
$
f(x)-A_{n}(f)(x)
=f\left[x_{i-1},x_{i},x\right]\cdot (x-x_{i-1})(x-x_i).
$
And we have
$\left|(x-x_{i-1})(x-x_{i})\right| \leq \frac{1}{4(n-1)^2}.$
On the one hand, we can rewrite
\begin{align*}&|f\left[x_{i-1},x_{i},x\right]|=\left|\frac{f[x_{i},x]-f[x_{i-1},x]}{x_i-x_{i-1}}\right|\leq2(n-1)\norm[\infty]{f'}.\\
\Rightarrow & |f(x)-A_{n}(f)(x)|\leq \frac{1}{4(n-1)^2}\cdot 2(n-1)\norm[\infty]{f'}=\frac{\norm[\infty]{f'}}{2(n-1)}.
\end{align*}
Similarly, we can obtain that for $x \in [0,1]$
$$\norm[\infty]{f-A_{n}(f)} \leq \frac{\|f'\|_{\infty}}{2(n-1)} \Rightarrow \err(A_n,\cg,\ch,S) \leq \frac{1}{2}(n-1)^{-1} \Rightarrow \tilde{h}(n)=\frac{1}{2}(n-1)^{-1}.$$

On the other hand, by Eq(2.68) in \cite{walter},
we know
$f\left[x_{i-1},x_{i},x\right]=\frac{1}{2}f''(\xi),$ where $\xi \in [x_{i-1},x_{i}].$
Then we know
$
|f\left[x_{i-1},x_{i},x\right] |\leq \frac{\norm[\infty]{f''}}{2}.$
Thus,
$$
|f(x)-A_{n}(f)(x)|\leq \frac{1}{4(n-1)^2}\frac{\norm[\infty]{f''}}{2}=\frac{\norm[\infty]{f''}}{8(n-1)^2}.
$$
Hence, we can obtain that for $\forall x \in [0,1]$
$$\norm[\infty]{f-A_{n}(f)} \leq  \frac{\norm[\infty]{f''}}{8(n-1)^{2}} \Rightarrow \err(A_n,\cf,\ch,S) \leq \frac{1}{8}(n-1)^{-2}
\Rightarrow h(n)=\frac{1}{8}(n-1)^{-2}.$$


%Combine eq.(\ref{errorg}) and eq.(\ref{errorf}), we have the same result as in eq. (\ref{Anerrbound}).
%$$\norm[\ch]{S(f) -  A_n(f)} \le \frac{\min(C_1,C_2\tau n^{p_1-p_2})\norm[\cg]{f}}{n^{p_1}} \qquad \forall f \in \cc_{\tau}.$$

%
%Then by Algorithm \ref{twostagedetalgo}, we have the following algorithm
%\begin{algo} \label{adpatalgo}
%{\bf (Automatic, Adaptive, Two-Stage, Deterministic).}
% Let $\varepsilon$ be a positive error tolerance, and let $N_{\max}$ be the maximum cost allowed.  Let $\tau$ be a fixed positive number and
% let $A_{n}$ is defined as above. Let $n_{0}$ be a number larger than $\tau+2.$ Given an input function $f$, do the following:
%
%\begin{description}
%\item[Stage 1.\ Estimate {$\norm[\infty]{f'}$}.] Compute $G_{n_{0}}(f)$ at a cost of $n_{0} = \cost(A_{n_{0}})$.   Define the inflation factor
%$$
%\fC =\frac{1}{1 - \tau (n_{0}-1)^{-s}} \ge 1.
%$$
%Then $\fC G_{n_{0}}(f)$ provides a reliable upper bound on $\norm[\infty]{f'}$.
%
%\item [Stage 2.\ Estimate {$f$}.] Choose the sample size need to approximate $f$, namely, check whether $N$ is large enough to satisfy the error tolerance, i.e.,
%\[
%\frac{\min(1,\tau (N-1)^{-s})\fC G_{n_{0}}(f)}{2N} \le \varepsilon.
%\]
%If $N \le N_{\max}-n_0$, then $f$ may be approximated within the desired error tolerance and within the cost budget.  Set the warning flag to false, $W=0$. Otherwise, recompute $N$ to be within budget, $N = \tN_{\max} := \max\{N \in \ci : N\le N_{\max} -  N_G\}$, and set the warning flag to true, $W=1$.  Compute $A_N(f)$ as the approximation to $f$.
%
%\end{description}
%
%\end{algo}

And we can find  $h(n) \leq \tilde{h}(n) h_{+}(n), \forall n \in \mathcal{I},$ then $ \tau h(n)< \tilde{h}(n) \ \forall n \in \mathcal{I}.$
Thus we only need to find $n$ such that  \[
\frac{\tau \fC_n G_{n}(f)}{8(n-1)^2} \le \varepsilon.
\]

If we consider about the embedded algorithms, by Algorithm $\ref{multistagealgo},$ we obtain
\begin{algo} \label{multistageapproalgo}
Let the error tolerance $\varepsilon$, the maximum cost budget $N_{\max}$, and the positive constant $\tau$ be as described in  \eqref{taudef}. Let the sequences of algorithms, $\{A_n\}_{n \in \ci}$ and  $\{G_n\}_{n \in \ci}$ be as described above.  Set $i=1$.  Let $n_1$ be the smallest number in $n \in \ci$ satisfying $\err_+(G_n,\cf,\norm[\cg]{\cdot},\reals_+) \le 1/\tau$. For any input function $f \in \cf$, do the following:
\begin{description}

\item [Stage 1. Estimate {$\norm[\infty]{f'} $}.]
Compute $G_{n_i}(f)$ in \eqref{estfirstderiv} and $\fC_n$ in \eqref{inflaapprox}.
\item [Stage 2. Check for Convergence.]
 Check whether $n_i$ is large enough to satisfy the error tolerance, i.e.,
$$
\frac{\tau \fC_{n_i} G_{n_i}(f)}{8(n_i-1)^2} \le \varepsilon.
$$
If this is true, then set $W$ to be false, return $(A_{n_i}(f),W)$ and terminate the algorithm.


\item[Stage 3. Compute $n_{i+1}$.]  Otherwise, if the inequality above fails to hold,
choose $n_{i+1}$ as the smallest number exceeding $n_i$ and not less than $N_{A}(\varepsilon /G_{n_i}(f))$ such that $A_{n_{i}}$ is embedded in $A_{n_{i+1}}$. That is
$$n_{i+1}-1=(n_i-1)\max\left\{\left\lceil\frac{1}{(n_i-1)}\left(\frac{\tau G_{n_i}(f)}{8\varepsilon}\right)^{\frac{1}{2}}\right\rceil,2\right\}.$$
 If $n_{i+1} \le N_{\max}$, increment $i$ by $1$, and return to Stage 1.\\
Otherwise, if $n_{i+1} > N_{\max}$, choose $n_{i+1}$ to be the largest number not exceeding $N_{\max}$ such that $A_{n_{i}}$ is embedded in $A_{n_{i+1}}$, and set $W$ to be true. Return $(A_{n_{i+1}}(f),W)$ and terminate the algorithm.
\end{description}
\end{algo}


Then we can have the complexity of the upper bound.
\begin{theorem}   Let  $\varepsilon$, $N_{\max}$, $\tau$ and $n_1$ be given as described in Algorithm \ref{multistageapproalgo}.
Assume that $n_1 \le N_{\max}$. Let $r$ be the number described in the paragraph preceding that algorithm.
Let $\cc_\tau$ be the cone of functions defined in \eqref{taudef}.
Define
\[
\tN_A(a) = \min\left\{ n \in \ci : \frac{\tau \fC_n }{8(n-1)^2} \le a \right\}, \quad a \in (0,\infty).
\]
Let $\cc_\tau$ be the cone of functions defined in \eqref{taudef}.  Let
$$
\cn = \left \{ f \in \cc_\tau :r\tN_{A}\left(\frac{\varepsilon}{\sigma} \right) \le N_{\max} \right\}
= \left \{ f \in \cc_\tau : \sigma \le \frac{8(N_{\max}/r-1)^2 \varepsilon }{\tau \fC_{N_{\max}/r}  } \right\}
$$
be the nice subset of the cone $\cc_\tau$.  Then it follows that Algorithm \ref{multistageapproalgo} is successful for all functions in $\cn$,  i.e.,  $\success(A,W,\cn,\varepsilon,N_{\max}) = 1$.  Moreover, the cost of this algorithm is bounded above in terms of the $\norm[\infty]{f'}$ of the input function as follows:
\begin{multline}
\cost(A,\cn,\varepsilon,N_{\max},\sigma) \\
\le  \min\left\{ n \in \ci : n \ge \left(\frac{\sigma\tau}{8\varepsilon(1 - \tau/(2(n_{1}-1)))}\right)^{\frac{1}{2}} +1\right\}
\end{multline} where $\sigma=\norm[\infty]{f'}.$
%The upper bound on the cost of this specific algorithm provides an upper bound on the complexity of the problem, $\comp(\varepsilon,\ca(\cn,\ch,S,\Lambda),N_{\max},\sigma)$.  If the sequence of algorithms $\{A_n\}_{n \in \ci}$, $A_n \in\ca_{\fix}(\cg,\ch,S,\Lambda)$  is nearly optimal for the problems $(\cg,\ch,S,\Lambda)$ and $(\cf,\ch,S,\Lambda)$ as defined in \eqref{nearoptdef}, then Algorithm \ref{twostagedetalgo} does not incur a significant penalty for not knowing $\norm[\cg]{f}$ a priori, i.e., for all $p>0$,
%\begin{equation} \label{penalty}
%\sup_{0 < \varepsilon/\sigma \le 1} \frac{\cost(A,\cn,\varepsilon,\infty,\sigma)} {\comp(\varepsilon/\sigma,\ca_{\fix}(\cj,\ch,S,\Lambda))} \left(\frac{\varepsilon}{\sigma}\right)^p <\infty, \qquad \cj \in \{\cf,\cg\}.
%\end{equation}
\end{theorem}

\begin{proof}
Applied Theorem \ref{MultiStageThm}, then we can have the above result.
\end{proof}




\subsection{Lower Bound}
Next, we will derive the lower bound. Suppose that for any $n>0,$ for any algorithm which cost is less than $n.$
We want to build fooling function at all data sites $\xi_{i}, \ i=0,\ldots,n-1.$
There exists at least one $i$ such that $\xi_{i+1}-\xi_{i}\ge\frac{1}{n-1}.$
Then we can define $f_{1}$
$$f_{1}(x):=\left\{\begin{matrix}
A_{1}(x-\xi_{j})^{2}(x-\xi_{j+1})^{2} & \xi_{j}< x \leq \xi_{j+1},\\
0 & \text{otherwise},
\end{matrix}\right.,$$
where $A_{1} \ge 0.$ As we need $\norm[\infty]{f_1}=1,$ we can have $A_{1}=\frac{16}{(\xi_{i+1}-\xi_{i})^4}.$
Then we can obtain
\begin{align*}
\norm[\infty]{f'_1}&=\frac{16}{3\sqrt{3}(\xi_{j+1}-\xi_{j})}\leq \frac{16}{3\sqrt{3}}(n-1) \Rightarrow g(n)=\frac{16}{3\sqrt{3}}(n-1).\\
\norm[\infty]{f''_1}&=\frac{16}{(\xi_{j+1}-\xi_{j})^2}
=\frac{3\sqrt{3}}{(\xi_{j+1}-\xi_{j})}\norm[\infty]{f'_1}
 \leq 3\sqrt{3}(n-1)\norm[\infty]{f'_1}\Rightarrow \tg(n)=3\sqrt{3}(n-1).
\end{align*}

Let $f_0=x.$ Then we have $\norm[\infty]{f'_0}=1, \norm[\infty]{f''_0}=0 \Rightarrow \tau_0=0.$

\begin{theorem} \label{complowbdappr} Suppose that functions $f_{0}$ and $f_1$ can be found that satisfy conditions \eqref{assumpfone} and \eqref{assumpfzero}.  It then follows that the complexity of the problem, defined by \eqref{complexdef}, assuming infinite cost budget, over the cone of functions $\cc_{\tau}$ is
$$
\comp(\varepsilon,\ca(\cc_{\tau},\ch,S,\Lambda),\infty,\sigma)
\ge \min\left(\frac{3\sqrt{3}\sigma}{64 \varepsilon}+1, \left(\frac{\sigma\tau}{64\varepsilon}\right)^{2}+1 \right).
$$
\end{theorem}

\begin{proof}
$g(n), \ \tg(n)$ and $\tau_0$ is obtained above, then by Theorem \ref{complowbd}
we can have the above results.
\end{proof}

\subsection{Numerical Example}

We want to testify the success rate for our algorithm. So we
choose test function $f(x)=e^{-(a(x-\mu))^2},$
where $\mu \sim U[0,1]$ and $\log_{10} a \sim U[0,4].$
And $\mu \sim U[0,1]$ means $\mu$ is uniform on [0,1].
Also for this kind of function, if we have large $a$, we can approximate the function hardly
because the function has a sharper bump. And we know
$\norm[\infty]{f'}=a\sqrt{2/e}, \ \norm[\infty]{f''}=2a^2.$
If $f \in \cc_{\tau}$ then we should have
$a \leq \tau/\sqrt{2e}.$ Which means Expected Success Rate should be $\frac{1}{4}\log_{10}(\tau/\sqrt{2e}).$
Here we use $\tau = 10, 25 , 100$ separately to test $10000$ times with
$\varepsilon = 1e-7$ to obtain the observed success rate.
\begin{table}[h]
\centering
\begin{tabular}{cccc}
$\tau$ &  10 & 25 & 100\\
\toprule
Expected Success Rate &  $\leq 16 \%$ &  $\leq 26 \%$  & $\leq 41 \%$ \\
Observed Success Rate & 25$\%$ &  34$\%$  & 49$\%$ \\
\end{tabular}
\caption{ Comparison between expected and observed success rate}
\end{table}

From the above table, we can find our algorithm works very well, as the observed
success rate is always larger than the expected success rate.\\




