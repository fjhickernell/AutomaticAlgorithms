\documentclass[]{article}
\setlength{\marginparwidth}{0.5in}
\usepackage{amsmath,amssymb,amsthm,mathtools,booktabs,alltt,xspace,array,tikz,pifont,comment,graphicx}
\usepackage[author-year]{amsrefs}
\input FJHDef.tex


\newcommand{\integ}{\texttt{integral}\xspace}
\DeclareMathOperator{\Var}{Var}
\newcommand{\tVar}{\widetilde{\Var}}
\DeclareMathOperator{\err}{err}
\DeclareMathOperator{\maxcost}{maxcost}
\DeclareMathOperator{\mincost}{mincost}
\newcommand{\oerr}{\overline{\err}}
\newcommand{\herr}{\widehat{\err}}
\newcommand{\terr}{\widetilde{\err}}

\newtheorem{theorem}{Theorem}
\newtheorem*{ballthm}{Ball Algorithm Theorem}
\newtheorem*{guessthm}{Heuristic Algorithm Theorem}
\newtheorem*{conethm}{Cone Algorithm Theorem}
\newtheorem{prop}[theorem]{Proposition}
\newtheorem{lem}{Lemma}
\newtheorem{cor}{Corollary}
\theoremstyle{definition}
\newtheorem{algo}{Algorithm}
\newtheorem*{ballalgo}{Ball Algorithm}
\newtheorem*{guessalgo}{Heuristic Algorithm}
\newtheorem*{conealgo}{Cone Algorithm}
\newtheorem{condit}{Condition}
%\newtheorem{assump}{Assumption}
\theoremstyle{remark}
\newtheorem{rem}{Remark}
\newcommand{\Fnorm}[1]{\abs{#1}_{\cf}}
\newcommand{\Gnorm}[1]{\abs{#1}_{\cg}}
\newcommand{\flin}{f_{\text{\rm{lin}}}}

\begin{document}

\title{Reliable Adaptive Numerical Integration for Cones of Integrands}
\author{Fred J. Hickernell, Martha Razo, and Sunny Yun}
\maketitle 


\begin{abstract} Hi
\end{abstract}


\section{Introduction} 

While some mathematical problems can be solved using pencil and paper, many others must be solved by numerical algorithms.  An adaptive algorithm is convenient for the user because it aims to provide an approximate solution with an error no greater than the tolerance, $\varepsilon$, and it automatically determines how much computational effort is needed to do so.  At the core of every adaptive algorithm is a method for estimating the error of the numerical approximation.  Unfortunately, these error estimates are typically based on heuristics and so do not come with guarantees. 

This article constructs an adaptive numerical integration (quadrature) algorithm that is \emph{guaranteed} to provide the value of the integral to within the desired absolute error tolerance, $\varepsilon$.  Our construction succeeds by focusing on a \emph{cone} of integrands.  We also show that our algorithm has an asymptotically optimal computational cost among all possible successful algorithms.  In view of the known flaws in existing adaptive numerical integration rules, we advocate changing how error estimation for numerical integration is taught in numerical analysis courses.  We also explain how our new paradigm for adaptive numerical integration can be extended to adaptive numerical algorithms for other problems.

\subsection{When \texttt{cos} Fails}

Before addressing the more difficult problem of numerical integration, let's consider the simpler problem of producing values of $\cos(x)$ on a computer or calculator with a double precision number system.  Double precision means that calculations are done using about $15$ decimal digits of accuracy, so we expect \texttt{cos($x$)}\footnote{We use the teletype font to denote a numerical algorithm here and elsewhere in this article.} on a computer to satisfy
\begin{equation} \label{coserrexpect}
\abs{\cos(x)-\texttt{cos($x$)}} \le 10^{-15}.
\end{equation}

The numerical calculation \texttt{cos($x$)} first identifies $y=x+n\pi$ near zero, where $n$ is an integer. Noting that $\cos(x)=(-1)^n\cos(y)$, the algorithm approximates $\cos(y)$ by a high degree polynomial or rational function.  This approach achieves the desired accuracy in many cases, but \texttt{cos($x$)} has poor accuracy for very large $x$.  For example, in MATLAB one obtains
\begin{alltt}
>> cos(1e17*pi)
ans =
    -5.300447339957113e-01
\end{alltt}
whereas one might hope for an answer of $1$\footnote{While our numerical examples in this article are coded in MATLAB \cite{MAT8.1}, the principles illustrated apply universally to numerical algorithms implemented in other programming languages or software packages.}.   The reason for the large error here is that the computer number system with $15$ decimal digits cannot distinguish between $10^{17}\pi$ and $(10^{17}+1)\pi$, whose cosine values are quite different. 

This example illustrates that virtually all numerical algorithms can be inaccurate.  The user should know for what inputs an algorithm might not give the desired accuracy.  For the \texttt{cos} algorithm, \eqref{coserrexpect} is wrong and should be replaced by 
\[
\abs{\cos(x)-\texttt{cos($x$)}} \le \min(2,10^{-15}(1+\abs{x})).
\]

\subsection{Why Numerical Integration Is Needed?}
Various analytic methods for evaluating integrals are taught in calculus courses, e.g., substitution, integration by parts, and partial fractions.  Unfortunately, many elementary functions have integrals that \emph{cannot} be expressed as elementary functions, no matter how clever one is.  One example is the standard normal probability density, $\phi: x \mapsto \me^{-x^2/2}/\sqrt{2 \pi}$, whose integral is the standard normal cumulative distribution, $\Phi : x \mapsto \int_{-\infty}^x \phi(t) \, \dif t$.  Since the $\Phi$ is not an elementary function, values of $\Phi$ must be provided via tables in statistics textbooks and special functions in calculators. By contrast, the derivatives of elementary functions are always elementary functions.  

The problem of numerical integration is much harder than the problem of evaluating the cosine function.  For \texttt{cos($x$)} the input $x$ is a number, which is known to $15$ significant digits.  For integration the key input is an imperfectly known function.  One would like to construct an algorithm \integ, such that
\begin{equation*}
\abs{\int_a^b f(x) \, \dif x - \texttt{integral($f$,$a$,$b$,$\varepsilon$)}} \le \varepsilon.
\end{equation*}
However, what is meant by the input $f$ is only a black-box algorithm that produces function values $f(x)$ for any given $x \in [a,b]$.  Although the algorithm \integ may make assumptions about the smoothness of $f$, the user is not expected to provide detailed information about $f$.

The next section reviews what is known about the composite trapezoidal rule and its error.  This is the basic method that we used to build our adaptive numerical integration algorithm.  Section \ref{traprulesec} describes an automatic algorithm based on knowing an upper bound on the variation of the first derivative of the integrand.  Unfortunately, this is not adaptive and rather impractical.  Adaptive algorithms estimate the error based on the function values sampled and then apply a stopping criterion to determine how many function values are required.  Section \ref{flawstopsec} describes the stopping criterion commonly taught in numerical analysis courses, but which has significant flaws, as pointed out by James Lyness \ycite{Lyn83}.  An improved stopping criterion for integrands lying in cones is described in Section \ref{newalgosec}.  This section builds on \ocite{HicEtal14b}, which presents a general framework for guaranteed adaptive algorithms.  In Section \ref{discusssec} we discuss the ramifications of these developments for for how we teach numerical integration in calculus courses and later on.  We also discuss the how we should go about developing new numerical algorithms.

\section{The Composite Trapezoidal Rule and Its Error Bound} \label{traprulesec}
The composite trapezoidal rule, $T_n$, which is often introduced in calculus courses, approximates an integral by the sum of the areas of $n$ trapezoids whose heights are function values (see Figure \ref{Gausstrapfig}):
\begin{multline}
\int_a^b f(x) \, \dif x \approx T_n(f) := \frac{b-a}{2n} \left [ f(a) + 2 f\biggl(\frac{(n-1)a+b}{n}\biggr) + \cdots  \right .\\
 \left . + 2 f\biggl(\frac{a+(n-1)b}{n}\biggr) + f(b) \right],
\end{multline}
where $n \in \naturals$ denotes the number of trapezoids, and $a$ and $b$ are finite with $a<b$.  The trapezoidal rule can also be thought of as the integral of the piecewise linear spline approximation to the integrand. Under mild smoothness conditions on $f$ one observes that $T_n(f)$ approaches $\int_a^b f(x) \, \dif x$ as $n$ increases (see \eqref{traperrbd} below). 

We would like to turn $T_n$ into an \emph{automatic} quadrature algorithm --- one that automatically chooses $n$ to ensure that the trapezoidal rule error is small enough.  Given the user's tolerance for error, $\varepsilon$, an automatic trapezoidal algorithm should choose $n$ such that 
\begin{equation} \label{errdef}
\err(f,n) := \abs{\int_0^1 f(x) \, \dif x - T_n(f)} \le \varepsilon.
\end{equation}

Upper bounds on the trapezoidal rule error assumed that the integrand possesses some smoothness.  For any function $f:[a,b]\to \reals$, let $f'$ denote the one-sided derivative:
\[
f'(x)=\lim_{\delta \downarrow 0} \frac{f(x+\delta)-f(x)}{\delta}, \quad a \le x < b, \qquad f'(b)=\lim_{\delta \downarrow 0} \frac{f(b)-f(b-\delta)}{\delta}.
\]
This makes $f'$ well-defined even when it has jump discontinuities.  All integrands considered in this article have derivatives with bounded variation and lie in the linear space
\[
\cv:=\{f : \Var(f')<\infty \}.
\]
Here $\Var(\cdot)$ represents the (total) variation of a function:
\begin{multline} \label{vardef}
\Var(f) \\
:= \sup \left \{ \sum_{i=1}^n \abs{f(x_i)-f(x_{i-1})} : n \in \naturals, \ a \le x_0 < x_1 < \cdots < x_{n} \le b \right\}.
\end{multline}
Intuitively, $\Var(f)$ is the total vertical distance up and down that a roller coaster travels on a track described by the graph of $f$. The function $f \mapsto \Var(f')$ is a semi-norm. 

Although the true error, $\err(f,n)$, is rarely known in practice, there exist rigorous upper bounds on the error that are proportional to $\Var(f')/n^2$, e.g., see \ocite{BraPet11a}*{Sect.\ 7.2, (7.15)}: 
\begin{equation} \label{traperrbd}
\err(f,n) \le \oerr(f,n):=\frac{(b-a)^3\Var(f')}{8n^2}, \qquad n \in \naturals := \{1, 2, \ldots \}. 
\end{equation}
The trapezoidal rule gives the exact answer for linear integrands.  Error bound \eqref{traperrbd} reflects this fact since if $f(x)=\alpha x+\beta$, then $f'(x)=\alpha$, and so $\Var(f')=0$.

To illustrate this error bound, consider the normal probability example mentioned above.  The trapezoidal rule approximation using $n=4$ trapezoids, the actual error, and the error bound are as follows:
\begin{subequations} \label{GaussExample}
\begin{gather}
f(x) = 2\phi(2x) = \sqrt{\frac{2}{\pi}} \me^{-2x^2}, \qquad \Var(f') = 1.5038, \\
\int_0^1 f(x)  \, \dif x = 0.4772, \qquad T_{4}(f)= 0.4750, \\
\err(f,4)=0.0022 \le 0.0117 = \oerr(f,4).
\end{gather}
\end{subequations}
(The value of the integral to four significant digits may be found by a variety of quadrature algorithms.)  Figure \ref{Gausstrapfig} depicts the integrand and the trapezoidal rule approximation. As expected, the actual error is no greater than the error bound in \eqref{traperrbd}.

\begin{figure}
\centering 
\includegraphics[width=8cm]{ProgramsImages/GaussInteg4TrapFigcolor.eps}
\caption{The integrand in \eqref{GaussExample} and the trapezoidal rule approximation for the integral. \label{Gausstrapfig}}
\end{figure}

Error bound \eqref{traperrbd} can be used to determine how large $n$ must be to meet the error tolerance if an upper bound on $\Var(f')$ is available. Below is a simple algorithm designed for balls of integrands and its justification.

\begin{ballalgo} Given a $\sigma>0$ and an error tolerance, $\varepsilon$, set 
\begin{equation}\label{algo1n}
n = \left \lceil \sqrt{\frac{(b-a)^3 \sigma}{8\varepsilon}} \right \rceil,
\end{equation}
and return the trapezoidal rule approximation $T_n(f)$ as the answer.
\end{ballalgo}
\begin{ballthm} If an integrand $f$ lies in the ball $\cb_{\sigma}=\{f : \Var(f') \le \sigma\}$, then the Ball Algorithm is successful is successful for $f$, i.e., 
\[
\abs{\int_a^b f(x) \, \dif x - T_n(f)}\le \varepsilon.
\]
\end{ballthm}

The Ball Algorithm is automatic because the user need not specify $n$.  Rather the required number of trapezoids is determined automatically in terms of the error tolerance, $\varepsilon$, and the radius of the ball, $\sigma$.  However, this is not an adaptive algorithm because the computational cost does not depend on function values obtained by the algorithm.

Calculus texts often teach students to use the Ball Algorithm, see e.g., \cite{???}.  For example, for the integrand in \eqref{GaussExample}, one may choose $\sigma=\Var(f')=1.5038$, and then the Ball Algorithm uses $n = \lceil 0.4336/\sqrt{\varepsilon}\, \rceil$ trapezoids.  However, in practice it is often infeasible to derive an upper bound on the variation of the first derivative of the integrand.  Thus, general purpose mathematical software does not rely on algorithms guaranteed for balls of integrands.

\section{An Adaptive Trapezoidal Algorithm with a Flawed Stopping Criterion} \label{flawstopsec}

Adaptive numerical integration algorithms bound or estimate the error using only function values and then determine the sample size accordingly.  MATLAB \cite{MAT8.3}, Mathematica \cite{Mat9a}, NAG \cite{NAG23}, and R \cite{R2512} all contain automatic numerical integration algorithms.  Unfortunately, their stopping criteria are unreliable, as pointed out by James Lyness \ycite{Lyn83}.  To illustrate the flawed logic that leads to unreliable error estimation in these sophisticated algorithms, we first focus on the simpler case of an adaptive trapezoidal algorithm.

\subsection{A Popular Error Estimate}
Textbooks such as \ocite{BurFai10}*{p.\ 223--224}, \ocite{CheKin12a}*{p.\ 233}, and  \ocite{Sau12a}*{p.\ 270}, advise readers to estimate the error of $T_n(f)$ by comparing it to $T_{n/2}(f)$, specifically,
\begin{equation}\label{baderr}
\herr(f,n) = \frac{\abs{T_n(f) - T_{n/2}(f)}}{3}, \qquad \frac n2 \in \naturals.
\end{equation}
This error estimate leads to the following adaptive, automatic  quadrature algorithm. For each iteration the number of trapezoids is doubled so that the data from the previous iteration can be reused. 

\begin{guessalgo} Given an inflation factor, $\fC\ge 1$, and an error tolerance, $\varepsilon$, let $j=1$ and $n_1=2$.

\begin{description} 

\item[Step 1] Compute the error estimate $\herr(f,n_j)$ according to \eqref{baderr}.

\item [Step 2] If $\fC\,\herr(f,n_j) \le \varepsilon$, then return the trapezoidal rule approximation $T_{n_j}(f)$ as the answer.  

\item [Step 3] Otherwise let $n_{j+1}=2 n_j$, increase $j$ by one, and go to Step 1.

\end{description}

\end{guessalgo}

The inflation factor $\fC$ is a measure of this algorithm's caution.  The Heuristic Algorithm is not typically accompanied by a guarantee of success, although one is given below in Section \ref{???}.

The Heuristic Algorithm algorithm works often in practice.  For the integrand in \eqref{GaussExample}, $\fC=1.2$, and $\varepsilon=10^{-4}$, the Heuristic Algorithm returns $0.4772$, which is indeed correct to the tolerance required.

\subsection{Spiky Integrands} 

Unfortunately, it is easy to find integrands for which the Heuristic Algorithm fails.  Figure \ref{spikeflukefig} (left) depicts the following spiky integrand:
\begin{subequations} \label{spiky}
\begin{gather} \label{spiky}
f_{\text{spiky}}(x;n) = 16n^2[\bbl nx \bbr(1-\bbl nx \bbr)]^2, \qquad \frac n2 \in \naturals, \quad \bbl x \bbr := x \bmod 1,\\
\int_0^1 f(x) \, \dif x = \frac{8}{15}, \qquad 
\Var(f'_{\text{spiky}}(\cdot;n))= \frac{64n^2}{3\sqrt{3}}, \\
T_n(f)=0, \qquad \err(f,n)= \frac{8}{15}\ne 0=\herr(f,n).
\end{gather}
\end{subequations}
The spiky integrand vanishes the data sites and has spikes of unit height in between.

The problem of spiky integrands is not unique to the Heuristic Algorithm.  \emph{Any} numerical integration algorithm that depends only on function values to estimate error will be fooled by sufficiently spiky integrands.  Suppose that some clever numerical integration algorithm samples the integrand at a seqeunce of points, $\xi_1, \xi_2, \ldots$, and suppose that $f(\xi_i)=0$ for all $\xi_i$.  Here the algorithm may even be adaptive, meaning that $\xi_{i+1}$ may depend on $(\xi_1, f(\xi_1)), \ldots, (\xi_i, f(\xi_i))$.  Eventually the algorithm stops.  If the algorithm is to be accurate for the zero integrand, then the estimate of the integral should be $0$ (or very close).  However, as illustrated in Figure \ref{spikeflukefig} (left) there exists a non-constant function that shares the same data --- $f(\xi_i)=0$ for all $\xi_i$ --- while having any value of the integral one wants. Such functions completely fool the algorithm.

The spiky integrands that fool the Ball Algorithm lie outside $\cb_{\sigma}$ according to  the Ball Algorithm Theorem.  The parameter $\sigma$ determines how spiky an integrand can be and still be integrated correctly by the Ball Algorithm.  Increasing $\sigma$ accommodates spikier integrands.  

The Heuristic Algorithm can be made more accommodating of spiky functions by increasing the initial number of trapezoids, $n_1$, to prevent the algorithm from stopping too soon.  However, this does not resolve the main flaw in the Heuristic Algorithm for spiky functions.  Namely, there is no mathematically precise condition that determines for which spiky integrands the Heuristic Algorithm might fail.


\subsection{Fluky Integrands} \label{flukysubsec}

Thirty years ago James Lyness wrote a SIAM Review article, \emph{When Not to Use an Automatic Quadrature Routine}.  While recognizing the problem of spiky integrands, he claims that there is even a more serious problem with automatic numerical integration methods   \cite{Lyn83}*{p.\ 69}:
\begin{quote}
While prepared to take the risk of being misled by chance alignment of zeros in the integrand function, or by narrow peaks which are ``missed,'' the user may wish to be reassured that for ``reasonable'' integrand functions which do not have these characteristics all will be well. It is the purpose of the rest of this section to demonstrate by example that he cannot be reassured on this point. In fact the routine is likely to be unreliable in a significant proportion of the problems it faces (say $1$ to $5\%$) and there is no way of predicting in a straightforward way in which of any set of apparently reasonable problems this will happen.
\end{quote}
Lyness went on to describe how to construct a ``reasonable'' integrand that fools an automatic quadrature algorithm.  We would call this a \emph{fluky} integrand.  

Figure \ref{spikeflukefig} (right) shows the following fluky integrand:
\begin{subequations} \label{fluky}
\begin{gather} \label{fluky}
f_{\text{fluky}}(x;n) = \frac{4 - 5 n^2 + n^4}{4}+ \frac{15n^2}{2} x(1-x)\{1 - n^2 x(1-x)\}, \quad \frac n2 \in \naturals, \\
\int_0^1 f(x) \, \dif x = 1, \qquad \Var(f'_{\text{fluky}}(\cdot;n))= \frac{5 n}{3}  \Bigl[9 n + 2 \sqrt{3(-2 + n^2)^3} \Bigr ],
\end{gather}
\end{subequations}
The integrand $f_{\text{fluky}}(\cdot;n)$ satisfies \eqref{failcond} for even $n$.   In contrast to the spiky integrand, this fluky integrand has only one local minimum of size $\Order(n^4)$ located at $1/2$ and two local maxima of size at $\Order(n^4)$ located at $\Order(n^{-2})$ and $1-\Order(n^{-2})$.  The variation of the first and third derivatives of $f_{\text{fluky}}(\cdot;n)$ increase with $n$, specifically,
\begin{equation*}
\Var(f'_{\text{fluky}}(\cdot;n))= \frac{10 n}{3}  \Bigl[9 n + 2 \sqrt{3(-2 + n^2)^3} \Bigr ], \qquad
\Var(f'''_{\text{fluky}}(\cdot;n))= 360n^4.
\end{equation*}
Disregarding its large magnitude, $f_{\text{fluky}}$ appears to quite ``reasonable''.  We describe integrands like this one as fluky because their construction is rather delicate, and they satisfy conditions \eqref{failcond} with a small number of local optima.  In this case $f_{\text{fluky}}$ was chosen to be a fourth degree polynomial --- to ensure a small number of local optima --- with symmetry about $1/2$.  The remaining coefficients were then determined uniquely by the constraints in \eqref{failcond}.  

Nearly all existing adaptive quadrature algorithms can be fooled by both spiky and fluky integrands.  Figure \ref{fig:foolquad} displays two integrands that fool MATLAB's {\tt quad} routine by satisfying \eqref{failcond}.

\begin{figure}
\centering 
\begin{tabular}{cc}
\includegraphics[width=5.5cm]{ExpositoryPaperSpikyquad.eps}
&
\includegraphics[width=5.5cm]{ExpositoryPaperFlukyquad.eps} \\
a) & b)
\end{tabular}
\caption{a) A spiky integrand designed to fool MATLAB's {\tt quad} and the data sampled by {\tt quad}; b) A fluky integrand designed to fool {\tt quad}. \label{fig:foolquad}}
\end{figure}

The failure of the error estimate in \eqref{baderr} for $f_{\text{fluky}}$ seems avoidable. Although the values of the trapezoidal rule are moderate, the sampled function values, $\bigl(f_{\text{fluky}}(i/n;n)\bigr)_{i=1}^{n}$ vary widely, which implies that $\Var(f_{\text{fluky}})$ must be large.  In the next section we transform this intuition into a better method for bounding the error of the trapezoidal rule.




Error estimate \eqref{baderr} may be justified by noting that Simpson's rule may be written as 
\begin{align*}
S_n(f) &:= \frac{4T_n(f) - T_{n/2}(f)}{3} =  T_n(f) + \frac{T_n(f) - T_{n/2}(f)}{3}= \\
& = \frac{1}{3n} \left [ f(0) + 4 f(1/n) + 2 f(2/n) + 4 f(3/n) + \cdots + 4 f(1-1/n) + f(1) \right].
\end{align*}
The error bound for Simpson's rule then serves as an bound on the error of $\herr(f,n)$ \cite{BraPet11a}*{Sect.\ 7.3, p.\ 231}:
\begin{align} \label{Simperrbd}
\abs{\err(f,n) - \herr(f,n)} & \le \abs{\int_0^1 f(x) \, \dif x - T_n(f) - \frac{T_n(f) - T_{n/2}(f)}{3}} \\
\nonumber
& = \abs{\int_0^1 f(x) \, \dif x - S_n(f)}  \le \frac{\Var(f''')}{36n^4}, \qquad \frac{n}2 \in \naturals. 
\end{align}
Comparing the orders of convergence in \eqref{traperrbd} and \eqref{Simperrbd}, it follows that $\herr(f,n)$ does an excellent job in approximating $\err(f,n)$ \emph{for $n$ large enough}, assuming finite $\Var(f''')$.

Unfortunately, without knowing an upper bound on $\Var(f''')$, one cannot know how large $n$ must be for $\herr(f,n)$ to be reliable.  If one really does have an upper bound on $\Var(f''')$, then it makes more sense to apply Simpson's rule and obtain a higher order convergence rate than that of Algorithm \ref{baderralgo}.  Since there is no really rigorous and compelling argument to use Algorithm \ref{baderralgo}, we do not formulate any theorem guaranteeing it.

One may identify integrands, $f$, that fool the error estimate completely, e.g.,
\begin{equation} \label{failcond}
\int_0^1 f(x) \, \dif x =  1, \quad T_{n}(f)=T_{n/2}(f) = -1, \quad \err(f,n)=2 \ne 0 = \herr(f,n).
\end{equation}
Equation \eqref{Simperrbd} implies that $\Var(f''') \ge 36 n^4 \abs{\err(f,n) - \herr(f,n)}$.  So, unless $n$ is small, any integrand $f$ satisfying \eqref{failcond} must have large  $\Var(f''')$.  Figure \ref{spikeflukefig} shows two such integrands for $n=16$. These two integrands illustrate two different ways in which the error estimate $\herr(\cdot)$ fails, and hence, Algorithm \ref{baderralgo} also fails.  One way is ultimately unavoidable but should be quantifiable.  The other way is inexcusable, but is prevalent in widely used automatic numerical quadrature.

\begin{figure}
\centering 
\includegraphics[width=5.5cm]{ProgramsImages/SpikyInteg16TrapFigcolor.eps} \qquad
\includegraphics[width=5.5cm]{FlukyInteg16TrapFigcolor.eps}
\caption{Examples of a spiky integrand (left) and a fluky integrand (right), which satisfy the failure conditions \eqref{failcond} for $n=16$. \label{spikeflukefig}}
\end{figure}

The failure of Algorithm \ref{baderralgo} cannot be solved by using a more conservative stopping criterion of the form $C\herr(f,n_j) \le \varepsilon$ in Step 2 where $C>1$.  Integrands satisfying \eqref{failcond} have an estimated error of $C\herr(f,n)=0$ regardless of how large $C$ is.



\subsection{Fluky Integrands} \label{flukysubsec}

Thirty years ago James Lyness wrote a SIAM Review article, \emph{When Not to Use an Automatic Quadrature Routine}.  While recognizing the problem of spiky integrands, he claims that there is even a more serious problem with automatic quadrature methods   \cite{Lyn83}*{p.\ 69}:
\begin{quote}
While prepared to take the risk of being misled by chance alignment of zeros in the integrand function, or by narrow peaks which are ``missed,'' the user may wish to be reassured that for ``reasonable'' integrand functions which do not have these characteristics all will be well. It is the purpose of the rest of this section to demonstrate by example that he cannot be reassured on this point. In fact the routine is likely to be unreliable in a significant proportion of the problems it faces (say $1$ to $5\%$) and there is no way of predicting in a straightforward way in which of any set of apparently reasonable problems this will happen.
\end{quote}
Lyness went on to describe how to construct a ``reasonable'' integrand that fools an automatic quadrature algorithm.  We would call this a \emph{fluky} integrand.  

Figure \ref{spikeflukefig} (right) plots a fluky integrand for Algorithm \ref{baderralgo}:
\begin{equation} \label{fluky}
f_{\text{fluky}}(x;n) = \frac{2 - 5 n^2 + n^4}{2}+ 15n^2 x(1-x)\{1 - n^2 x(1-x)\}, \qquad n \in \naturals.
\end{equation}
The integrand $f_{\text{fluky}}(\cdot;n)$ satisfies \eqref{failcond} for even $n$.   In contrast to the spiky integrand, this fluky integrand has only one local minimum of size $\Order(n^4)$ located at $1/2$ and two local maxima of size at $\Order(n^4)$ located at $\Order(n^{-2})$ and $1-\Order(n^{-2})$.  The variation of the first and third derivatives of $f_{\text{fluky}}(\cdot;n)$ increase with $n$, specifically,
\begin{equation*}
\Var(f'_{\text{fluky}}(\cdot;n))= \frac{10 n}{3}  \Bigl[9 n + 2 \sqrt{3(-2 + n^2)^3} \Bigr ], \qquad
\Var(f'''_{\text{fluky}}(\cdot;n))= 360n^4.
\end{equation*}
Disregarding its large magnitude, $f_{\text{fluky}}$ appears to quite ``reasonable''.  We describe integrands like this one as fluky because their construction is rather delicate, and they satisfy conditions \eqref{failcond} with a small number of local optima.  In this case $f_{\text{fluky}}$ was chosen to be a fourth degree polynomial --- to ensure a small number of local optima --- with symmetry about $1/2$.  The remaining coefficients were then determined uniquely by the constraints in \eqref{failcond}.  

Nearly all existing adaptive quadrature algorithms can be fooled by both spiky and fluky integrands.  Figure \ref{fig:foolquad} displays two integrands that fool MATLAB's {\tt quad} routine by satisfying \eqref{failcond}.

\begin{figure}
\centering 
\begin{tabular}{cc}
\includegraphics[width=5.5cm]{ExpositoryPaperSpikyquad.eps}
&
\includegraphics[width=5.5cm]{ExpositoryPaperFlukyquad.eps} \\
a) & b)
\end{tabular}
\caption{a) A spiky integrand designed to fool MATLAB's {\tt quad} and the data sampled by {\tt quad}; b) A fluky integrand designed to fool {\tt quad}. \label{fig:foolquad}}
\end{figure}

The failure of the error estimate in \eqref{baderr} for $f_{\text{fluky}}$ seems avoidable. Although the values of the trapezoidal rule are moderate, the sampled function values, $\bigl(f_{\text{fluky}}(i/n;n)\bigr)_{i=1}^{n}$ vary widely, which implies that $\Var(f_{\text{fluky}})$ must be large.  In the next section we transform this intuition into a better method for bounding the error of the trapezoidal rule.


\section{A Guaranteed, Adaptive, Automatic Trapezoidal Algorithm} \label{newalgosec}

Algorithm \ref{ballalgo} does not use any values of $f$ to determine how many trapezoids are needed to approximate the integral of $f$ because an upper bound on $\Var(f')$ is assumed.  Algorithm \ref{baderralgo} uses values of $f$ determine how how many trapezoids are needed, but in a way that allows it to detect an occurrence of large $\Var(f')$.  In this section we present a way for reliably estimating $f'$, which then can be used to construct a reliable stopping criterion for an adaptive trapezoidal algorithm.

Estimating $\Var(f')$ reliably and directly requires additional smoothness assumptions on $f$, but if we assume more smoothness, then we ought to use a different rule, like Simpson's rule.  On the other hand, knowing that $\Var(f')$ is finite does allow us to reliably estimate, a weaker semi-norm of $f$, such as $\Var(f)$.  As observed in Section \ref{flukysubsec} above, the fluky algorithms that fool Algorithm \ref{baderralgo} not only have large $\Var(f')$ but also large $\Var(f)$.  The approach taken in this section is to explore what can be accomplished by estimating a weaker semi-norm.

Let $A_n$ denote the linear spline approximation using $n+1$ equally spaced points, i.e., 
\begin{equation}
\label{Andef}
A_n(f)(x) = \begin{cases} f(0)(1-nx) + f(1/n)nx, & 0 \le x < 1/n, \\
f(1/n)(2-nx) + f(2/n)(nx-1), & 1/n \le x < 2/n, \\
\qquad \qquad \qquad \qquad \vdots & \qquad \quad \vdots \\
f(1-1/n)(n-nx) + f(1)(nx-n + 1), & 1-1/n \le x \le 1.
\end{cases}
\end{equation}   
The trapezoidal rule can be written as $T_n(f) = \int_0^1 A_n(f)(x) \, \dif x$.  Using this definition of linear spline we focus on the semi-norm defined by $f \mapsto \Var(f-A_1(f))$.  Like $f \mapsto \Var(f')$ this weaker semi-norm also vanishes for functions that are linear in $x$.  If $\Var(f')$ is finite, then it follows that $\Var(f) = \norm[1]{f'}:=\int_0^1 \abs{f(x)} \, \dif x$, so the weaker semi-norm may be written as $f \mapsto \norm[1]{f'-f(1)+f(0)}$.  It is shown in \ocite{HicEtal14b}*{Section 5.1} that 
\begin{equation} \label{weaknorm}
2 \norm[1]{f'-f(1)+f(0)} \le \Var(f') \qquad \forall f \text{ with } \Var(f')<\infty.
\end{equation}

The data-driven quantity 
\begin{equation} \label{weaknormalgo}
\norm[1]{A_n(f)'-f(1)+f(0)}= \sum_{i=1}^n \abs{f\left(\frac in \right)-f\left(\frac{i-1}n \right) - \frac{f(1)-f(0)}{n}}
\end{equation}
always underestimates $\norm[1]{f'-f(1)+f(0)}$, but not by much provided that $\Var(f')$ is not too large. Again in \ocite{HicEtal14b}*{Section 5.1} it is shown that 
\begin{equation} \label{errweaknorm}
0 \le \norm[1]{f'-f(1)+f(0)} - \norm[1]{A_n(f)'-f(1)+f(0)} \le \frac{\Var(f')}{2n}.
\end{equation}

At this point it seems that we have reached a dead end because we have an error bound in terms of $\Var(f')$, just like the trapezoidal rule error bound \eqref{traperrbd}.  However, we make a further assumption that $f$ lies in the cone defined as 
\begin{equation} \label{conedef}
\cc_{\tau} := \{f : \Var(f') \le \tau \norm[1]{f'-f(1)+f(0)} \}.
\end{equation}
This cone consists of functions whose stronger semi-norms are not too much greater than their weaker semi-norms.  The set $\cc_{\tau}$ is called a cone because $f \in \cc_{\tau}$ implies that $cf \in \cc_{\tau}$ for all $c \in \reals$.  Substituting this upper bound on $\Var(f')$ into \eqref{errweaknorm} and re-arranging the terms implies a data-driven upper bound on  $\Var(f')$:
\begin{multline} \label{varfprimeless}
\Var(f') \le \tau \norm[1]{f'-f(1)+f(0)}  \\
\le \frac{\tau \norm[1]{A_n(f)'-f(1)+f(0)}}{1-\tau/(2n)} \qquad \forall f \in \cc_{\tau},
\end{multline}
provided that $n>\tau/2$.  There is also a companion bound in the other direction based on \eqref{weaknorm} and \eqref{errweaknorm}:
\begin{equation} \label{varfprimegreat}
\norm[1]{A_n(f)'-f(1)+f(0)} \le \norm[1]{f'-f(1)+f(0)} \le \frac{\Var(f')}{2}.
\end{equation}

Plugging \eqref{varfprimeless} into \eqref{traperrbd} yields the following guaranteed error bound for the trapezoidal rule:
\begin{equation} \label{guarerr}
\err(f,n) \le \terr(f,n) := \frac{\tau \norm[1]{A_n(f)'-f(1)+f(0)}} {4n(2n-\tau)} \qquad \forall f \in \cc_{\tau}.
\end{equation}
This new error bound can then be used to construct the following adaptive, automatic quadrature algorithm, which is a simplification of \ocite{HicEtal14b}*{Algorithm 4}:

\begin{algo}[Adaptive, Automatic, for Cones] \label{conealgo} Given $\tau>0$, and an error tolerance, $\varepsilon$, let $j=1$ and $n_1=\lceil (\tau+1)/2 \rceil$.

\begin{description} 

\item[Step 1] Compute the error estimate $\terr(f,n_j)$ according to \eqref{guarerr}.

\item [Step 2] If $\terr(f,n_j) \le \varepsilon$, then return the trapezoidal rule approximation $T_{n_j}(f)$ as the answer.  

\item [Step 3] Otherwise let $n_{j+1}=2 n_j$, increase $j$ by one, and go to Step 1.

\end{description}
\end{algo}

Error bound \eqref{guarerr} is used to prove that Algorithm \ref{conealgo} is successful for integrands in $\cc_{\tau}$.  Let $N(f,\varepsilon,\tau)$ denote the eventual number of trapezoids required by this algorithm.  Because the algorithm is adaptive, the number of trapezoids depends on the particulars of $f$ as well as the error tolerance.  The two bounds \eqref{varfprimeless} and \eqref{varfprimegreat} may be used to derive both lower and upper bounds on $N(f,\varepsilon,\tau)$:

\begin{theorem}\cite{HicEtal14b}*{Theorem 7} \label{conealgothm}
Let $N(f,\varepsilon,\tau)$ denote the final $n_j$ in Algorithm \ref{conealgo}.  Then this algorithm is successful for all functions in $\cc_{\tau}$,  i.e.,  $\abs{\int_0^1 f(x) \, \dif x - T_{N(f,\varepsilon,\tau)}(f)} \le \varepsilon$.  Moreover, $N(f,\varepsilon,\tau)$ is bounded below and above as follows:
\begin{multline}
\max \left(\left \lceil\frac{\tau+1}{2} \right \rceil, \left \lceil \sqrt{\frac{ \Var(f')}{8\varepsilon}} \right \rceil \right) \\
\le \max \left(\left \lceil\frac{\tau+1}{2} \right \rceil, \left \lceil \sqrt{\frac{\tau \norm[1]{f'-f(1)+f(0)}}{8\varepsilon}} \right \rceil \right) \\
\le
N(f,\varepsilon,\tau) \\
\le \sqrt{\frac{\tau \norm[1]{f'-f(1)+f(0)}}{2\varepsilon}} + \tau + 3
\le \sqrt{\frac{\tau \Var(f') }{4\varepsilon}} + \tau + 3.
\end{multline}
\end{theorem}

Note that the cost of the algorithm depends on $\Var(f')$, but without the algorithm needing to know $\Var(f')$ in advance.   Comparing the two error bounds on which Algorithms \ref{ballalgo} and \ref{conealgo} are based and applying the , we note that 
\begin{equation*}
\frac{\Var(f')}{\sigma} \le
\frac{\terr(f,n)}{\oerr(f,n)}
\le \frac{\tau\Var(f')}{\sigma (2-\tau/n)}.
\end{equation*}



For an integrand like \eqref{GaussExample} where $\Var(f')$ is known, the number of trapezoids required by Algorithm \ref{conealgo} is no greater than  $\sqrt{\tau \Var(f') /(4\varepsilon)} + \tau + 3$, whereas for Algorithm \ref{ballalgo} with $\sigma=\Var(f')$ the number of trapezoids is no more than $\bigl \lceil \sqrt{\Var(f') /(8\varepsilon)} \, \bigr\rceil$.  The extra cost for Algorithm \ref{newalgo} comes in the form of the factor $\sqrt{2\tau}$ and the additional additive term of $\tau+1$.  On the other hand, if $\Var(f')$ is not known in advance, then the disadvantage of Algorithm  \ref{ballalgo} is that if $\Var(f')$ is not known in advance, and $\sigma$ is chosen conservatively, then Algorithm \ref{newalgo} could cost arbitrarily more than $\sqrt{\Var(f') /(8\varepsilon)}+1$ in the limit of , 

\section{Acknowledgements}  The authors are grateful for discussions with a number of colleagues. This research is supported in part by grant NSF-DMS-1115392.

\bibliography{FJH22,FJHown22}
\end{document}

The Bernoulli polynomials are described by \ocite{AbrSte64}*{Chap.\ 23, \url{http://dlmf.nist.gov/24}}.  The first six are defined as follows 
\begin{subequations} \label{Bernoulli}
\begin{gather} 
B_0(x) = 1, \qquad B_1(x) = x-1/2,  \qquad 
B_2(x)=1/6 - x(1-x), \\ 
B_3(x) = \frac{1}{2} x(1-x)(1-2x), \qquad B_4(x) = -1/30 + [x(1-x)]^2, \\
B_5(x) = \frac{1}{6} x(1-x)(1-2x)(1 + 3 x - 3 x^2).
\end{gather}
\end{subequations}

\subsection{Necessary Conditions for Failure of the Error Estimate} 
Integrands that satisfy \eqref{failcond} and fool the error estimate in \eqref{baderr} must satisfy certain conditions.  First note that the error bound in \eqref{traperrbd} implies that
\[
\Var(f') \ge 8n^2 \err(f,n) =  16n^2 \asymp n^2 \qquad \text{as } n \to \infty,
\]
so the total variation of the first derivative must be increasingly large as the number of trapezoids increases.  Moreover, the trapezoidal rule added to the error estimate in \eqref{baderr} is actually Simpson's rule:
\begin{align*}
S_n(f) &:= T_n(f) + \herr(f,n) = \frac{4T_n(f) - T_{n/2}(f)}{3} \\
& = \frac{1}{3n} \left [ f(0) + 4 f(1/n) + 2 f(2/n) + 4 f(3/n) + \cdots + 4 f(1-1/n) + f(1) \right].
\end{align*}
The error bound of Simpson's rule then serves as an bound on the error of the error estimate \cite{BraPet11a}*{Sect.\ 7.3, p.\ 231}:
\begin{equation} \label{Simperrbd}
\abs{\err(f,n) - \herr(f,n)} = \abs{\int_0^1 f(x) \, \dif x - S_n(f)} \le \frac{\Var(f''')}{36n^4}, \qquad \frac{n}2 \in \naturals. 
\end{equation}
So, integrands satisfying \eqref{failcond} must also satisfy
\[
\Var(f''') \ge 36n^4 \abs{\err(f,n) - \herr(f,n)} =  36n^4 \asymp n^4 \qquad \text{as } n \to \infty.
\]
Thus, the total variation of the third derivative must increase even faster than the minimum increase of $\Var(f')$ as the number of trapezoids increases.

The derivatives of Bernoulli polynomials are multiples of lower degree Bernoulli polynomials and the integrals of nonconstant Bernoulli polynomials vanish (see \ocite{AbrSte64}*{Chap.\ 23, \url{http://dlmf.nist.gov/24}}):
\[
B'_n(x) = n B_{n-1}(x), \quad \int_0^1 B_{n+1}(x) \, \dif x, \qquad n \in \naturals.
\]
The formula for the derivatives facilitates the computation of the total variation of the spiky and fluky examples above, confirming that the variations of their first and third derivatives satisfy the above inequalities.  For the spiky integrand defined in \eqref{spiky},
\begin{subequations} \label{spikyvariation}
\begin{gather}
%f'_{\text{spiky}}(x;n) = 240 n B_3(\bbl nx \bbr), \qquad f''_{\text{spiky}}(x;n) = 720 n^2 B_2(\bbl nx \bbr), \\ f'''_{\text{spiky}}(x;n) = 1440 n^3 B_1(\bbl nx \bbr), \\
\Var(f_{\text{spiky}}(\cdot;n))= \frac{15n}{2}, \qquad  \Var(f'_{\text{spiky}}(\cdot;n))= \frac{80n^2}{\sqrt{3}} > 16 n^2, \\
\Var(f''_{\text{spiky}}(\cdot;n))= 360n^3, \qquad \Var(f'''_{\text{spiky}}(\cdot;n))= 1440n^4 > 36n^4.
\end{gather}
\end{subequations} 
For the fluky integrand defined in \eqref{fluky},
\begin{subequations} \label{spikyvariation}
\begin{gather}
%f'_{\text{fluky}}(x;n) = - 30 n^2 [B_1(x) + 2 n^2 B_3(x)], \\ f''_{\text{fluky}}(x;n) = -30 n^2 [1 + 6 n^2 B_2(x)], \qquad f'''_{\text{fluky}}(x;n) = -360 n^4 B_1(x), \\
\Var(f_{\text{fluky}}(\cdot;n))= \frac{15}{8} (8 - 4 n^2 + n^4),  \\
\Var(f'_{\text{fluky}}(\cdot;n))= \frac{10 n}{3}  \Bigl[9 n + 2 \sqrt{3(-2 + n^2)^3} \Bigr ]  > 16 n^2, \\
\Var(f''_{\text{fluky}}(\cdot;n))= 90n^4, \qquad \Var(f'''_{\text{fluky}}(\cdot;n))= 360n^4 > 36n^4.
\end{gather}
\end{subequations}  

For the spiky integrand the asymptotic order of the total variation for large $n$ increases with the order of the derivative while for the fluky integrand it stays the same.  Specifically,
\[
\Var(f^{(j)}_{\text{spiky}}(\cdot;n)) \asymp n^{j+1}, \quad \Var(f^{(j)}_{\text{fluky}}(\cdot;n)) \asymp n^{4}, \qquad j=0, \ldots, 3.
\]
This difference in asymptotic behaviors distinguishes whether a better error bound developed in the next section will be successful:  no for the spiky integrand , but yes for the fluky integrand.
