\documentclass[]{elsarticle}
\setlength{\marginparwidth}{0.5in}
\usepackage{amsmath,amssymb,amsthm,natbib,mathtools,graphicx}
\input FJHDef.tex

\DeclareMathOperator{\lin}{lin}
\DeclareMathOperator{\up}{up}
\DeclareMathOperator{\lo}{lo}
\DeclareMathOperator{\fix}{non}
\DeclareMathOperator{\err}{err}
\newcommand{\fudge}{\mathfrak{C}}

\newtheorem{theorem}{Theorem}
\newtheorem{prop}[theorem]{Proposition}
\newtheorem{lem}{Lemma}
\theoremstyle{definition}
\newtheorem{algo}{Algorithm}
\newtheorem{condit}{Condition}
%\newtheorem{assump}{Assumption}
\theoremstyle{remark}
\newtheorem{rem}{Remark}


\journal{Journal of Complexity}

\begin{document}

\begin{frontmatter}

\title{Guaranteed Automatic Algorithms with Relative Error}
\author{Yuhan Ding}
\author{Fred J. Hickernell}
\author{Yizhi Zhang}
\address{Room E1-208, Department of Applied Mathematics, Illinois Institute of Technology,\\ 10 W.\ 32$^{\text{nd}}$ St., Chicago, IL 60616}
\begin{abstract}
\end{abstract}

\begin{keyword}
adaptive \sep cones \sep function recovery \sep integration \sep quadrature
%% keywords here, in the form: keyword \sep keyword

\MSC[2010] 65D05 \sep 65D30 \sep 65G20
%% MSC codes here, in the form: \MSC code \sep code
%% or \MSC[2008] code \sep code (2000 is the default)

\end{keyword}
\end{frontmatter}


\section{A General Error Criterion} \label{relerrsec}

The criterion used for the automatica algorithms in \cite{HicEtal14b} is an \emph{absolute} error criterion. Given an error tolerance, $\varepsilon_a$, one seeks an algorithm, $A$, such that
\begin{equation} \label{abserrcrit}
\norm[\ch]{S(f)-A(f)} \le \varepsilon_a.
\end{equation}
This is done through a sequence of non-adaptive algorithms, $A_n$, with cost $n$.  For each $n$ one can compute from only data the quantity $\hvareps_n$, a reliable upper bound on $\norm[\ch]{S(f)-A(f)}$, i.e.,
\begin{equation} \label{hatvareps}
\norm[\ch]{S(f)-A(f)} \le \hvareps_n.
\end{equation}
The automatic algorithms in \cite{HicEtal14b} uses a sequence of $A_n$ with $n$ increasing until $\hvareps_n \le \varepsilon_a$.

In many practical situations, one needs to approximate the answer with a certain \emph{relative} accuracy, e.g., correct to three significant digits.  In this case, given a tolerance, $\varepsilon_r$, one seeks an algorithm, $A$, such that
\begin{equation} \label{globalrelerrcrit}
\norm[\ch]{S(f)-A(f)} \le \varepsilon_r \norm[\ch]{S(f)}.
\end{equation}
This is a global relative error criterion, rather than a point-wise relative error criterion.  One may generalize the pure absolute and pure relative error criteria as follows:
\begin{equation} \label{globalgenerrcrit}
\norm[\ch]{S(f)-A(f)} \le (1-\theta)\varepsilon_a + \theta \varepsilon_r \norm[\ch]{S(f)}, \qquad 0 \le \theta \le 1,
\end{equation}
where $\theta=0$ denotes the absolute error case, and $\theta=1$ denotes the relative error case.  

Since $(1-\theta)\varepsilon_a + \theta \varepsilon_r \norm[\ch]{S(f)} \le \max( \varepsilon_a, \varepsilon_r \norm[\ch]{S(f)})$,
the generalized criterion, \eqref{globalgenerrcrit}, implies that either the absolute error criterion, \eqref{abserrcrit} or the relative error criterion, \eqref{globalrelerrcrit}, is satisfied.  One can imagine a situation where one really wants the relative error criterion to be satisfied, but since $\norm[\ch]{S(f)}$ may be tiny in some situations, choosing $(1-\theta)\varepsilon_a$ small but nonzero allows the algorithm to have a chance of success.  In fact, one might choose 
\[
\theta=\frac{\varepsilon_r}{\varepsilon_a + \varepsilon_r}, \quad \varepsilon_a \ge 0, \ \varepsilon_r \ge 0, \  \min(\varepsilon_a, \varepsilon_r) \ne 0
\]
In this case, choosing $\varepsilon_a=0$ implies $\theta=1$ and automatically means that one is interested purely in relative error, while choosing $\varepsilon_r=0$ implies $\theta=0$ and automatically means that one is interested purely in absolute error.

Using the $\hvareps_n$, the aforementioned reliable upper bounds on $\norm[\ch]{S(f)-A(f)}$, the aim is to take enough samples so that the generalized error criterion can be satisfied, but not too many.  The triangle inequality implies that
\[
\norm[\ch]{A_n(f)} - \norm[\ch]{S(f) - A_n(f)} \le \norm[\ch]{S(f)} \le \norm[\ch]{A_n(f)} + \norm[\ch]{S(f) - A_n(f)}.
\]
Supposing that one can evaluate $\norm[\ch]{A_n(f)}$ strictly from the data, this implies that any algorithm satisfying the data-dependent criterion
\begin{equation} \label{globalgenerrcritsuff}
\hvareps_n \le \min\left((1-\theta)\varepsilon_a,  \frac{(1-\theta)\varepsilon_a + \theta \varepsilon_r \norm[\ch]{A_n(f)}}{1 +  \theta \varepsilon_r} \right)
\end{equation}
must also satisfy \eqref{globalgenerrcrit}.  This criterion is becomes the stopping criterion for the automatic Algorithm \ref{??} below.  Using the triangle inequality again implies that if
\begin{equation} \label{globalgenerrcritsuffagain}
\hvareps_n \le \min\left((1-\theta)\varepsilon_a,  \frac{(1-\theta)\varepsilon_a + \theta \varepsilon_r \norm[\ch]{S(f)}}{1 +  2\theta \varepsilon_r} \right),
\end{equation}
then \eqref{globalgenerrcritsuff} must also be satisfied.  This criterion is used to construct an upper bound on the cost of automatic Algorithm \ref{??} in Theorem \ref{??}.  

In many cases it is possible to work with a point-wise generalized error criterion.  Suppose that the space of solutions, $\ch$, is a vector space of real-valued functions on $\cy$, and that the $\ch$-norm is a sup norm:
\begin{equation}
\norm[\ch]{h} = \sup_{\vy \in \cy} \abs{h(\vy)}.
\end{equation}
Then a point-wise generalized error criterion would take the form:
\begin{equation} \label{localgenerrcrit}
\abs{(S-\tA_n)(f)(\vy)} \le (1-\theta)\varepsilon_a + \theta \varepsilon_r \abs{S(f)(\vy)} \quad \forall \vy \in \cy,
\end{equation}
where again $0 \le \theta \le 1$.  Here $\tA_n$ may not be the same as $A_n$, but as shall be seen below is defined in terms of $A_n$.
Suppose one has a reliable pointwise upper bound on the error of a non-adaptive algorithm, $A_n$, with cost $n$:
\begin{equation} \label{hatvareps}
\hvareps_n(\vy) \ge \abs{(S-A_n)(f) (\vy)}.
\end{equation}
Here, $\hvareps_n(\vy)$ might be independent of $\vy$. Furthermore, suppose that $A_n(f)(\vy)$ can be evaluated from the data.

\begin{prop} Suppose that $\theta_r \varepsilon_r \le 1$.  If error bound \eqref{hatvareps} holds, then the point-wise generalized error criterion \eqref{localgenerrcrit} also holds, provided that
\begin{gather*}
\hvareps_n(\vy) \le (1-\theta) \varepsilon_a + \theta \varepsilon_r \max(\hvareps_n(\vy), \abs{A_n(f)(\vy)}), \\
\tA_n(f)(\vy) = A_n(f)(\vy) - \theta \varepsilon_r \sign(A_n(f)(\vy)) \min(\hvareps_n(\vy), \abs{A_n(f)(\vy)}).
\end{gather*}
\end{prop}
\begin{proof}  The cases of $\hvareps_n(\vy) \le \abs{A_n(f)(\vy)}$ and $\hvareps_n(\vy) > \abs{A_n(f)(\vy)}$ are treated separately.  
In the former case the definition of $\tA_n(f)(\vy)$ and the inequality constraint on $\hvareps_n(\vy)$ imply that
\begin{multline*}
\abs{\tA_n(f)(\vy) - A_n(f)(\vy) + \theta \varepsilon_r \sign(A_n(f)(\vy)) \hvareps_n(\vy)}= 0  \\ \le (1-\theta)\varepsilon_a  + \theta \varepsilon_r\abs{A_n(f)(\vy)} - \hvareps_n(\vy)
\end{multline*}
\begin{multline*}
\implies -(1-\theta)\varepsilon_a + [1 - \theta \varepsilon_r\sign(A_n(f)(\vy))] [A_n(f)(\vy) + \hvareps_n(\vy)] \\
\le  \tA_n(f)(\vy) \le (1-\theta)\varepsilon_a + [1 + \theta \varepsilon_r\sign(A_n(f)(\vy))] [A_n(f)(\vy) - \hvareps_n(\vy)]
\end{multline*}
\begin{multline*}
\implies -(1-\theta)\varepsilon_a + \max\{S(f)(\vy) - \theta \varepsilon_r\abs{S(f)(\vy)} : \abs{(S-A_n)(f) (\vy)} \le \hvareps_n\} \\
\le  \tA_n(f)(\vy) \\
 \le (1-\theta)\varepsilon_a + \min\{S(f)(\vy) + \theta \varepsilon_r\abs{S(f)(\vy)} : \abs{(S-A_n)(f) (\vy)} \le \hvareps_n\}
\end{multline*}
\begin{multline*}
\implies -(1-\theta)\varepsilon_a + S(f)(\vy) - \theta \varepsilon_r\abs{S(f)(\vy)} \\
\le  \tA_n(f)(\vy)
 \le (1-\theta)\varepsilon_a + S(f)(\vy) + \theta \varepsilon_r\abs{S(f)(\vy)}  \qquad \forall \vy \in \cy,
\end{multline*}
\begin{multline*}
\implies -(1-\theta)\varepsilon_a - \theta \varepsilon_r\abs{S(f)(\vy)}
\le  \tA_n(f)(\vy) - S(f)(\vy) \\
 \le (1-\theta)\varepsilon_a + \theta \varepsilon_r\abs{S(f)(\vy)}  \qquad \forall \vy \in \cy,
\end{multline*}
which implies \eqref{localgenerrcrit}.

In the case of $\hvareps_n(\vy) > \abs{A_n(f)(\vy)}$, the definition of $\tA_n(f)(\vy)$ and the inequality constraint on $\hvareps_n(\vy)$ imply that
\begin{equation*}
\abs{\tA_n(f)(\vy) - (1 - \theta \varepsilon_r) A_n(f)(\vy)}= 0 \le (1-\theta)\varepsilon_a - (1-\theta \varepsilon_r) \hvareps_n(\vy)
\end{equation*}
\begin{multline*}
\implies -(1-\theta)\varepsilon_a + (1 - \theta \varepsilon_r) [A_n(f)(\vy) + \hvareps_n(\vy)] \\
\le  \tA_n(f)(\vy) \le (1-\theta)\varepsilon_a + (1 - \theta \varepsilon_r) [A_n(f)(\vy) - \hvareps_n(\vy)]
\end{multline*}
\begin{multline*}
\implies -(1-\theta)\varepsilon_a + \max\{S(f)(\vy) - \theta \varepsilon_r\abs{S(f)(\vy)} : \abs{(S-A_n)(f) (\vy)} \le \hvareps_n\} \\
\le  \tA_n(f)(\vy) \\
 \le (1-\theta)\varepsilon_a + \min\{S(f)(\vy) + \theta \varepsilon_r\abs{S(f)(\vy)} : \abs{(S-A_n)(f) (\vy)} \le \hvareps_n\}
\end{multline*}
From this point the argument showing that \eqref{localgenerrcrit} is satisfied proceeds exactly as in the previous case.
\end{proof}




\section*{Acknowledgements} 

\bibliographystyle{spbasic}
\bibliography{FJH22,FJHown22}
\end{document}
